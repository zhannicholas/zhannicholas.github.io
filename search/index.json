[{"content":"Stay hungry, Stay foolish\n","href":"/","title":"主页"},{"content":"","href":"/notebook/","title":"笔记本"},{"content":"","href":"/posts/","title":"博客"},{"content":"","href":"/rust/","title":"Rust"},{"content":"","href":"/java/","title":"Java"},{"content":"","href":"/java/concurrency/","title":"并发"},{"content":"分布式计算（Distributed Computing）是计算机科学的一个分支，专门研究分布式系统（Distributed Systems）。分布式系统是由一组彼此通过网络通信、为了完成共同的目标而一起协调工作的计算机节点组成的系统。例如，我们每天都在使用的万维网就是一个巨大的分布式系统。\n可伸缩性（Scalability）分布式系统的核心特点。当我们的系统达到瓶颈时，我们往往需要通过某种手段来提高系统的处理能力。在集中式系统中，系统能力的提升一般是通过升级服务器到中大型机、增加 CPU 核数等硬件升级手段来完成，这就是垂直扩展（Scale vertically）；而在分布式系统中，我们通过增加更多的计算机来提升系统的处理能力，这就是水平扩展（Scale horizontally）。当系统能力过剩时，分布式系统只需减少计算机数量即可，而集中式系统需要更换硬件，相当麻烦。\n冗余（Redundancy）也是分布式系统的一大特点。在分布式系统中，多台计算机往往提供相同的服务。当某台机器发生故障时，整个系统并不会瘫痪，其它机器可以顶上来继续提供服务。而单机系统中的服务器故障可能导致整个系统的瘫痪。\n","href":"/distributed_computing/","title":"分布式计算"},{"content":"JVM(Java Virtual Machine) 是一台非常抽象的计算机，它拥有一个指令集，并且在运行时操作内存。JVM可以被移植到不同的平台，以实现Java语言跨平台的特性。\n","href":"/java/jvm/","title":"JVM"},{"content":"","href":"/posts/getting_started/","title":"Getting Started"},{"content":"","href":"/operating_systems/","title":"操作系统"},{"content":"","href":"/java/jakartaee/","title":"Jakarta EE"},{"content":"HTTP是Web的基石，Web浏览器、服务器和相关的Web应用程序都是通过HTTP相互通信的。\n","href":"/computer_networks/http/","title":"HTTP"},{"content":"","href":"/computer_networks/","title":"计算机网络"},{"content":"","href":"/computer_networks/fundamentals/","title":"基础知识"},{"content":" 在设计中思考什么应该变化，并封装会发生变化的概念。 ——设计模式：可复用面向对象的基础\n ","href":"/posts/design_patterns/","title":"设计模式"},{"content":"行万里路，找寻远方。\n","href":"/itinerary/","title":"游山玩水"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/tags/java/","title":"Java"},{"content":" Notes from Java Performance, 2nd Edition by Scott Oaks.\n To be a good Java Performance engineer, we need some specific knowledge. This knowledge falls into two broad categories:\n The performance of the Java Virtual Machine (JVM) itself: the way that the JVM is configured affects many aspects of a program’s performance. To understand how the features of the Java platform affect performance.  JVM tuning flags With a few exceptions, the JVM accepts two kinds of flags: boolean flags, and flags that require a parameter.\n Boolean flags use this syntax: -XX:+FlagName enables the flag, and -XX:-FlagName disables the flag. Flags that require a parameter use this syntax: -XX:FlagName=something, meaning to set the value of FlagName to something.  The process of automatically tuning flags based on the environment is called ergonomics.\nHardware Platform Two popular platform in use today: Virtual Machines and Software Containers (such as Docker).\nVirtual machine sets up a completely isolated copy of the operating system on a subset of the hardware on which the virtual machine is running.\nDocker container is just a process (potentially with resource constraints) within a running OS, it provides isolation between processes. By default, a Docker container is free to use all of the machine’s resources. But when the heap grows to it\u0026rsquo;s maximum size and that size is larger than the memory assigned to the Docker container, the Docker container (and hence the JVM) will be killed.\nThe Complete Performance Story A good algorithm is the most important thing when it comes to fast performance.\nA small well-written program will run faster than a large well-written program (write less code).\nWe should forget about small efficiencies, say about 97% of the time; premature optimization is the root of all evil. But premature optimization doesn\u0026rsquo;t mean avoiding code constructs that are known to be bad for performance. For example:\nlog.log(Level.FINE, \u0026#34;I am here, and the value of X is \u0026#34; + calcX() + \u0026#34; and Y is \u0026#34; + calcY()); This code does a string concatenation that is likely unnecessary, since the message won’t be logged unless the logging level is set quite high. The suggested imporvement is:\nif (log.isLoggable(Level.FINE)) { log.log(Level.FINE, \u0026#34;I am here, and the value of X is {} and Y is {}\u0026#34;, new Object[]{calcX(), calcY()}); } Remember, increasing load to a component in a system that is performing badly will make the entire system slower (For example, the database is always the bottleneck).\nWe should focus on the common use case scenarios. This principle manifests itself in several ways:\n Optimize code by profiling it and focusing on the operations in the profile taking the most time. Apply Occam’s razor to diagnosing performance problems. Write simple algorithms for the most common operations in an application.  Performance testing categories Three categories of code can be used for performance testing: microbenchmarks, macrobenchmarks, and mesobenchmarks.\nMicrobenchmarks A microbenchmark is a test designed to measure a small unit of performance in order to decide which of multiple alternate implementations is preferable.\nBut just-in-time compilation and garbage collection in Java make it difficult to write microbenchmarks correctly.\nSome principles:\n Microbenchmarks must test a range of input Microbenchmarks must measure the correct input Microbenchmark code may behave differently in production  In microbenchmarks, a warm-up period is need, because one of the performance characteristics of Java is that code performs better the more it is executed.\nMacrobeanchmarks The best thing to use to measure performance of an application is the application itself, in conjunction with any external resources it uses. This is a macrobenchmark.\nMesobenchmarks Mesobenchmarks are tests that occupy a middle ground between a microbenchmark and a full application.\nThroughput, Batching, and Response Time Performance can be measured as throughput (RPS), elapsed time (batch time), or response time, and these three metrics are interrelated.\nElapsed Time (Batch) Measurements: how long it takes to accomplish a certain task. Performance is most often measured after the code in question has been executed long enough for it to have been compiled and optimized.\nThroughput Measurements: A throughput measurement is based on the amount of work that can be accomplished in a certain period of time. This measurement is frequently referred to as transactions per second (TPS), requests per second (RPS), or operations per second (OPS).\nResponse Time: the amount of time that elapses between the sending of a request from a client and the receipt of the response. One difference between average response time and a percentile response time is in the way outliers affect the calculation of the average: since they are included as part of the average, large outliers will have a large effect on the average response time.\nVariability Test results vary over time. Understanding when a difference is a real regression and when it is a random variation is difficult.\nTesting code for changes like this is called regression testing. In a regression test, the original code is known as the baseline, and the new code is called the specimen.\nIn general, the larger the variation in a set of results, the harder it is to guess the probability that the difference in the averages is real or due to random chance.\nStatistical significance does not mean statistical importance.\nCorrectly determining whether results from two tests are different requires a level of statistical analysis to make sure that perceived differences are not the result of random chance. The rigorous way to accomplish that is to use Student’s t-test to compare the results. Data from the t-test tells us the probability that a regression exists, but it doesn’t tell us which regressions should be ignored and which must be pursued. Finding that balance is part of the art of performance engineering.\nTest early, Test Often Early, frequent testing is most useful if the following guidelines are followed:\n Automate everything Measure everything Run on the target system  Operating System Tools and Analysis CPU Usage CPU usage is typically divided into two categories: user time and system time (Windows refers to this as privileged time). User time is the percentage of time the CPU is executing application code, while system time is the percentage of time the CPU is executing kernel code.\nThe goal in performance is to drive CPU usage as high as possible for as short a time as possible. Driving the CPU usage higher is always the goal for batch jobs, because the job will be completed faster.\nBoth Windows and Unix systems allow you to monitor the number of threads that can be run (meaning that they are not blocked on I/O, or sleeping, and so on). Unix systems refer to this as the run queue: the first number in each line of vmstat\u0026rsquo;s output is the length of the run queue. Windows refers to this number as the processor queue and reports it (among other ways) via typeperf.\nDisk Usage Monitoring disk usage has two important goals. The first pertains to the application itself: if the application is doing a lot of disk I/O, that I/O can easily become a bottleneck. The second is even if the application is not expected to perform a significant amount of I/O—is to help monitor if the system is swapping.\nA system that is swapping—moving pages of data from main memory to disk, and vice versa—will have quite bad performance.\nNetwork Usage Network usage is similar to disk traffic: the application might be inefficiently using the network so that bandwidth is too low, or the total amount of data written to a particular network interface might be more than the interface is able to handle.\nBe sure to remember that the bandwidth is measured in bits per second (bps), although tools generally report bytes per second (Bps).\nJava Monitoring Tools JDK provides many Java monitoring tools to help us gain insight in to the JVM:\n jcmd: Prints basic class, thread, and JVM information for a Java process. Usage: jcmd process_id command optional_arguments jconsole: Provides a graphical view of JVM activities, including thread usage, class usage, and GC activities.  Since jconsole requires a fair amount of system resources, running it on a production system can interfere with that system. But we can set up jconsole so that it can be run locally and attach to a remote system, which won\u0026rsquo;t interfere with that remote system\u0026rsquo;s performance.   jmap: Provides heap dumps and other information about JVM memory usage. jinfo: Provides visibility into the system properties of the JVM, and allows some system properties to be set dynamically. jstack: Dumps the stacks of a Java process. Suitable for scripting. jstat: Provides information about GC and class-loading activities. jvisualvm: A GUI tool to monitor a JVM, profile a running application, and analyze JVM heap dumps.  Among these tools, nongraphical tools (expect jconsole and jvisualvm) are suitable for scripting.\nThese tools fits into broad areas:\n Basic VM information Thread information Class information Live GC analysis Heap dump postprocessing Profile a JVM  There\u0026rsquo;s a lot of overlap with each tool\u0026rsquo;s work. So it\u0026rsquo;s better to focus on different areas.\nBasic VM information Uptime The length of time the JVM has been up can be found via this command:\n$ jcmd \u0026lt;process_id\u0026gt; VM.uptime System properties The set of items in System.getProperties() can be displayed with either of these commands:\n$ jcmd \u0026lt;process_id\u0026gt; VM.system_properties or\n$ jinfo -sysprops \u0026lt;process_id\u0026gt; JVM version The version of the JVM is obtained like this:\n$ jcmd \u0026lt;process_id\u0026gt; VM.version JVM command line The command line can be displayed in the VM summary tab of jconsole, or via jcmd:\n$ jcmd \u0026lt;process_id\u0026gt; VM.command_line JVM tuning flags The tuning flags in effect for an application can be obtained like this:\n$ jcmd \u0026lt;process_id\u0026gt; VM.flags [-all] Or, you can use -XX:+PrintFlagsFinal option on the command line to see platform-specific defaults.\n$ java other_options -XX:+PrintFlagsFinal -version Yet another way to see JVM options for a running application is with jinfo. Retrive the values of all flags in the proces:\n$ jinfo -flags \u0026lt;process_id\u0026gt; If -flags option is present, this command will provide information about all flags; ohterwise, it prints only those specified on the command line.\nIf you want to inspect the value of an individual flag, use -flag option:\n$ jinfo -flag PrintGCDetails \u0026lt;process_id\u0026gt; -XX:+PrintGCDetails The advantage of jinfo is that is allows certain flag values to be changed during execution of the program. For example:\n$ jinfo -flag -PrintGCDetails \u0026lt;process_id\u0026gt; # turns off PrintGCDetails But this technique works only for those flags marked manageable in the output of PrintFlagsFinal command.\nThread Information If you have GUI, jconsole and jvisualvm are good tools to display real time information of threads running in an application.\nThe stacks can be obtained via jstack:\n$ jstack \u0026lt;process_id\u0026gt; Or jcmd:\n$ jcmd \u0026lt;process_id\u0026gt; Thread.print Class Information Information about the number of classes in use by an application can be obtained from jconsole or jstat. jstat can also provide information about class compilation.\nLive GC Analysis Virtually every monitoring tool can report something about GC activity\n jconsole displays live graphs of the heap usage; jcmd allows GC operations to be performed; jmap can print heap summaries or information on the permanent generation or create a heap dump; jstat produces a lot of views of what the garbage collector is doing  Heap Dump Postprocessing The heap dump is a snapshot of the heap that can be analyzed with various tools, including jvisualvm and the Eclipse Memory Analyzer Tool (MAT). GUI tool jvisualvm and command line tool jcmd or jmap can capture heap snapshots.\nProfile a JVM Profilers are the most important tool in a performance analyst’s toolbox. Profiling happens in one of two modes: sampling mode or instrumented mode.\nSampling Profilers Sampling mode is the basic mode of profiling and carries the least amount of overhead.\nIn the common Java interface for profilers, the profiler can get the stack trace of a thread only when the thread is at a safepoint, so sampling becomes even less reliable.\nThreads automatically go into a safepoint when they are:\n Blocked on a synchronized lock Blocked waiting for I/O Blocked waiting for a monitor Parked Executing Java Native Interface (JNI) code (unless they perform a GC locking function)  In addition, the JVM can set a flag asking for threads to go into a safepoint.\nInstrumented Profilers Instrumented profilers are much more intrusive than sampling profilers (they could have a greater effect on application than a sampling profiler), but they can also give more beneficial information about what’s happening inside a program.\nInstrumented profilers work by altering the bytecode sequence of classes as they are loaded (inserting code to count the invocations, and so on).\nBecause of the changes introduced into the code via instrumentation, it is best to limit its use to a few classes. This means it is best used for second-level analysis: a sampling profiler can point to a package or section of code, and then the instrumented profiler can be used to drill into that code if needed.\nJava Flight Recorder Java Flight Recorder (JFR) is a feature of the JVM that performs lightweight performance analysis of applications while they are running.\nJFR collects a set of event data, the data stream is held in a circular buffer, so only the most recent events are available.\nBy default, JFR is set up so that it has very low overhead: an impact below 1% of the program’s performance.\nJFR is initially disabled. To enable it, add the flag -XX:+FlightRecorder to the command line of the application (In Oracle\u0026rsquo;s JDK 8, you must specify XX:+UnlockCommercialFeatures prior to it).\nIf you forget to include these flags, you can use jinfo to change their values and enable JFR.\nFlight recordings are made in one of two modes: either for a fixed duration (1 minute in this case) or continuously.\nWe can start JFR when the program initially begins by -XX:StartFlightRecording=string flag. Or, use jcmd \u0026lt;process_id\u0026gt; JRF.start [options_list] to start a recording during a run, use jcmd \u0026lt;process_id\u0026gt; JRF.dump [options_list] to dump current data in the circular buffer, and use jcmd \u0026lt;process_id\u0026gt; JRF.stop [options_list] to abort a recording in process.\nJFR is useful in performance analysis, but it is also useful when enabled on a production system so that you can examine the events that led up to a failure.\nJava Mission Control The usual tool to examine JFR recordings is Java Mission Control (jmc).\n","href":"/notebook/reading_notes/java_performance/java_performance_fundamentals/","title":"Java Performance: foundamentals"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/authors/zhannicholas/","title":"zhannicholas"},{"content":"","href":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","title":"读书笔记"},{"content":"","href":"/tags/shell/","title":"Shell"},{"content":"","href":"/categories/shell/","title":"Shell"},{"content":"文件输入和输出是通过整数句柄（integer handle）实现的——每个打开的文件都会被赋予一个数字，这个数字就是文件描述符（file descriptor）。\n最常见的文件描述符是标准输入（stdin）、标准输出（stdout）和标准错误（stderr），它们的文件描述符数字分别是 0、1、2。这些数字和相应的设备都是系统保留的，我们可以通过命令 ls -l /dev/std* 查看。\n~$ ls -l /dev/std* lrwxrwxrwx 1 root root 15 Oct 16 18:47 /dev/stderr -\u0026gt; /proc/self/fd/2 lrwxrwxrwx 1 root root 15 Oct 16 18:47 /dev/stdin -\u0026gt; /proc/self/fd/0 lrwxrwxrwx 1 root root 15 Oct 16 18:47 /dev/stdout -\u0026gt; /proc/self/fd/1 每个进程看到的 /proc/self 目录下的东西都不一样，因为 /proc/self 实际上是一个指向 /proc/\u0026lt;process_ID\u0026gt; 的符号链接。\n输出重定向  COMMAND_OUTPUT \u0026gt;：重定向输出到一个文件。如果文件不存在，则创建并写入，否则覆盖原文件 : \u0026gt; filename：删除（truncate）文件的内容，如果文件不存在，则创建它。: 是一个占位符，代表无。 \u0026gt; filename：和 : \u0026gt; filename 类似，但是在某些 shell 下工作不正常 COMMAND_OUTPUT \u0026gt;\u0026gt;：重定向输出到一个文件，已追加的方式写入 1\u0026gt;filename：重定向标准输出到文件，以覆盖的方式写入 1\u0026gt;\u0026gt;filename：重定向标准输出到文件，以追加的方式写入 2\u0026gt;filename：重定向标准错误到文件，以覆盖的方式写入 2\u0026gt;\u0026gt;filename：重定向标准错误到文件，以追加的方式写入 \u0026amp;\u0026gt;filename：重定向标准输出和标准错误到文件 M\u0026gt;N：重定向文件描述符 M（如果未给出，则默认为 1）对应的文件到文件 N 。注意：M 是一个文件描述符，N 是一个文件。 M\u0026gt;\u0026amp;N：定向文件描述符 M（如果未给出，则默认为 1）对应的文件到文件描述符 N 对应的文件。注意：这里 M 和 N 都是文件描述符 2\u0026gt;\u0026amp;1：重定向标准错误到标准输出 i\u0026gt;\u0026amp;j：重定向文件描述符 i 到 j \u0026gt;\u0026amp;j：重定向标准输出到文件描述符 j  输入重定向  0\u0026lt; filename：从文件获取输入 [j]\u0026lt;\u0026gt;filename：打开文件 filename 进行读写，若该文件不存在则创建它。如果文件描述符 j 未声明，则默认为 0  关闭文件描述符  n\u0026lt;\u0026amp;-：关闭输入文件描述符 n 0\u0026lt;\u0026amp;- 和 \u0026lt;\u0026amp;-：关闭标准输入 n\u0026gt;\u0026amp;-：关闭输出文件描述符 n 1\u0026gt;\u0026amp;- 和 \u0026gt;\u0026amp;-：关闭标准输出  Here documents here document 是一个有着特殊用途的代码块，它告诉 shell 从当前位置读取输入，直到遇到结束字符串。然后，读取到的内容会被用作命令的标准输入。语法如下：\nCOMMAND \u0026lt;\u0026lt;InputComesFromHERE ... ... ... InputComesFromHERE 这里包含一个命令 COMMAND，一个限制字符串（limit string）InputComesFromHERE。限制字符串之间的内容就是 COMMAND 的标准输入，特殊字符 \u0026lt;\u0026lt; 出现在限制字符串的前面。由于 here document 使用限制字符串来判断输入是否结束，所以限制字符串的选取就非常重要，它不应该出现在给 COMMAND 的内容中。\nHere strings here string 可以被看做 here document 的简单版本。它的形式为：COMMAND \u0026lt;\u0026lt;\u0026lt; $WORD。其中 $WORD 展开后的结果就会做为 COMMAND 的标准输入。\n参考资料  Bash Guide for Beginners. Advanced Bash-Scripting Guide.  ","href":"/posts/linux/shell_scripting_io_redirection/","title":"Shell 脚本：I/O 重定向"},{"content":"正则表达式（regular expression, RE）在 Shell 中的应用非常广泛，我们常用的 find、grep、sed、awk 等命令都涉及到正则表达式……\n在 Shell 中，表达式就是一个字符串。字符串由字符组成，其中有些字符除了字面含义之外还有特殊含义，这些具有特殊含义的字符就是元字符（metacharacter）。\n正则表达式主要用于搜索文本和操作字符串，它包含以下内容：\n 字符集（character set）。字符集内所有的字符都只具有字面含义，不包括元字符 锚点（anchor）。锚点标识着正则表达式要匹配的文本中的位置，比如 ^ 和 $ 修饰符（modifier）。修饰符的作用是扩展或者缩小正则表达式要匹配的文本的范围，包括星号（*）、方括号（[）和反斜杠（/）  标准正则表达式中的特殊字符  *：匹配前一个字符出现任意次数，包括 0 次 .：匹配除换行符（\\n）之外的任何单字符 ^：匹配字符串的开始位置 $：匹配字符串的结束位置 [...]：封装正则表达式中用到的一组字符 \\：对特殊字符进行转义，转移后的特殊字符只具备字面含义 \\\u0026lt;...\\\u0026gt;：标记单词边界  扩展正则表达式中的特殊字符 扩展正则表达式给标准正则表达式中加入了新的元字符，主要用在 egrep、awk 和 Perl 中。\n ?：匹配前一个字符出现零次或一次 +：匹配前面的子表达式出现一次或多次 \\{\\}：限定前面的子表达式出现的次数 ()：封装一组正则表达式 |：从一组选择中选择一个，即或的含义  Globbing Bash 本身并不能识别正则表达式，解释正则表达式的是一些像 sed 和 awk 这样的命令和工具。Shell 展开中有一种类型叫文件名展开（filename expansion），但展开的事情并不是 Bash 自己完成的，而是由一个叫做 globbing 的进程完成的。但是 globbing 本身并不能使用标准的正则表达式，它只能识别通配符 *、?、[] 和某些其它的特殊字符。需要注意的是：* 并不会匹配以 . 开头的文件名。\n参考资料  Bash Guide for Beginners. Advanced Bash-Scripting Guide.4w  ","href":"/posts/linux/shell_scripting_regular_expressions/","title":"Shell 脚本：正则表达式"},{"content":"在 shell 将读取的命令分割成符号（token）之后，这些符号（或单词）会被展开或解析。Shell 会按照顺序执行八种类型的展开（expansion）：\n Brace expansion Tilde expansion Shell parameter and variable expansion Command substitution Arithmetic expansion Process substitution Word splitting File name expansion  大括号展开 大括号展开的形式为：一个可选的前导符（PREAMBLE）、一组位于一对大括号之间的由逗号分隔的字符串和一个可选的跋（POSTSCRIPT）。例如：\n~$ echo sp{el,il,al}l spell spill spall 波浪号展开 如果一个单词以没有被引起来的波浪号（~）开始，则在第一个没有被引起来的斜杠（若没有斜杠，则一直到最后一个字符）之前的字符将被视作波浪号前缀（tidle-prefix）。如果波浪号前缀中没有字符被引起来，那么波浪号前缀中的这些字符就会被当作一个可能的登录用户名。如果这个登录用户名是 null 字符串，则波浪号被替换为 shell 变量 HOME。如果 HOME 变量没有被设置，则替换为执行这个 shell 的用户的主目录。 如果波浪号前缀是”~+“，那么它会被替换为变量 PWD 的值。如果波浪号前缀是”~-“，那么它会被替换为变量 OLDPWD 的值。\n参数或变量展开 美元符号（$）用于参数展开、命令替换或算术展开。被展开的参数名或符号可能被包裹在大括号中。 最基本的参数展开的形式是”${PARAMETER}“。如果我们想在某个变量不存在时创建这个变量，则可以使用”${VAR:=value}“\n命令替换 命令替换（command substitution）允许我们用命令的输出来替换命令本身，它有两种形式：\n $(command) `command`  其中前者工作得更好，是用来取代后者的。\n例如：\n~$ whoami ubuntu2004 ~$ echo `whoami` ubuntu2004 ~$ echo $(whoami) ubuntu2004 命令替换会调用一个 subshell。\n A subshell is a child process launched by a shell (or shell script).\n 算术展开 算术展开（arithmetic expansion）允许我们用一个算术表达式计算得到的值替换表达式本身。它也有两种形式：\n $(( EXPRESSION )) `$[ EXPRESSION ]`  例如：\n~$ a=1 ~$ echo $(( $a+1 )) 2 ~$ echo $[ $a+1 ] 2 如果执行算术运算的操作数是一个字符串，那么它会被当作 0 处理。例如：\n~$ a=1 ~$ echo $(( $a+1 )) 1 ~$ echo $[ $a+1 ] 1 进程替换 进程替换（Process substitution）在支持命名管道（FIFO）和命名打开文件的 /dev/fd 方法的系统上。形式也有两种：\n \u0026lt;(LIST) \u0026gt;(LIST)  单词拆分 Shell 会扫描不在双引号之内的参数展开、命令替换和算术展开的结果，用于单词拆分。分隔符为变量 IFS 中的每个字符，默认为 \u0026lt;space\u0026gt;\u0026lt;tab\u0026gt;\u0026lt;newline\u0026gt;。\n文件名展开 单词拆分之后，如果 bash 的 -f 选项没有被设置，bash 就会扫描每个单词，寻找字符 *、?、[。如果这三个字符有所出现，单词会被视为一个模式（PATTERN），然后被替换为一个按照字段序排列与这个模式相匹配的文件名列表。\n参考资料  Bash Guide for Beginners. Advanced Bash-Scripting Guide.  ","href":"/posts/linux/shell_scripting_shell_expansion/","title":"Shell 脚本：shell 展开"},{"content":"和其它编程语言类似，bash 也给我们提供了条件语句（conditional statements）。\n条件分支 if 在 shell 中，if/then 的语法为：if TEST-COMMANDS; then CONSEQUENT-CONMMANDS; fi。\nTEST-COMMANDS 列表执行后，如果它的返回状态是 0，就执行 CONSEQUENT-COMMANDS 列表，其中最后一条命令的退出状态就是整个 if 表达式的返回状态。\n在 UNIX/Linux 中，通常用 0 表示成功，非零表示失败。\nif/then/else If/then/else 的语法为：if TEST-COMMANDS; then CONSEQUENT-CONMMANDS; else ALTERNATE-CONSEQUENT-COMMANDS; fi\nif/then/elif/else if 表达式的完整的语法为：if TEST-COMMANDS; then CONSEQUENT-CONMMANDS; elif MORE-TEST-COMMANDS; then MORE-CONSEQUENT-COMMANDS; else ALTERNATE-CONSEQUENT-COMMANDS; fi\ntest test 是一个 bash 内置的命令，用于检查文件的类型和进行值的比较。特殊字符 [ 是 test 的同义词。\n[] 与 [[]] 的区别在于：[[]]（extended test command）比 [] 更强，它会阻止 shell 进行变量名的分词操作、阻止路径名展开。[ 和 [[ 的类型也不同：\n~$ type [ [ is a shell builtin ~$ type [[ [[ is a shell keyword 还有更加有趣的：\n~$ type ] -bash: type: ]: not found ~$ type ]] ]] is a shell keyword  A builtin is a command contained within the Bash tool set, literally built in.\n  A keyword is a reserved word, token or operator. A keyword is not in itself a command, but a subunit of a command construct.\n 与文件相关的测试操作符很多，进一步查看：File test operators。 比较操作符也有很多，进一步查看：Other Comparision Operators。\ncase case 表达式的语法为：case EXPRESSION in CASE1) COMMAND-LIST;; CASE2) COMMAND-LIST;; ... CASEN) COMMAND-LIST;; esac\n每个 case 都是一个匹配模式的表达式，若某个 case 的模式匹配成功，则执行 ) 后的 COMMAND-LIST。) 的代表着模式的结束，它前面甚至可以写多个模式，每个模式之前用 | 分隔，例如：\ncase $1 in start | Start) start ;; *) echo \u0026#34;Default branch\u0026#34; ;; esac 每个 case 加上对应的 COMMAND-LIST 就构成了一个子句（clause），每个子句都必须以 ;; 结束\nselect select 语句的语法为：select WORD [in LIST]; do RESPECTIVE-COMMANDS; done。select 结构一般用于生成菜单。\n循环 for for 循环的语法为：for NAME [in LIST ]; do COMMANDS; done。如果 [in LIST] 不存在，则它会被替换为 in $@，for 循环会为每一个位置参数执行一遍 COMMANDS。\nwhile while 循环的语法为：while CONTROL-COMMAND; do CONSEQUENT-COMMANDS; done\nuntil until 循环的语法为：until TEST-COMMAND; do CONSEQUENT-COMMANDS; done。until 循环与 while 循环稍有不同，它会在 TEST-COMMAND 为 false 时继续循环，在 TEST-COMMAND 为 true 时结束循环，二者循环进行的条件刚好相反。\nbreak \u0026amp; continue break 和 continue 是两个能够影响循环执行的命令：前者终止整个循环，后者跳过当前循环进入下一次循环。\n参考资料  Bash Guide for Beginners. Advanced Bash-Scripting Guide.  ","href":"/posts/linux/shell_scripting_conditional_statements_and_loops/","title":"Shell 脚本：条件分支与循环"},{"content":"Shell 脚本用变量（variable）来表示数据，变量仅仅只是一个标签（label），它实际上是一个引用（reference）或指针（pointer），指向着与变量相关联的实际数据所存放的内存地址。 变量的名字就是变量的值的占位符（placehoder），变量的值（value）持有着数据。引用（reference）或检索（retrive）变量的值的操作被称为变量替换（variable substitution）。\n变量的名字和值是完全不同的，如果 variable1 是一个变量的名字，那么 $variable1（$variable1 是 ${variable1} 的简单形式）就是对该变量值的引用。$ 符号在这里的作用就是进行变量替换。\n按照作用域的不同，变量分为全局变量（global variables）和局部变量（local variables）\n 全局变量也就是我们通常说的环境变量（environment variables），对所有的 shell 都有效。查看环境变量的命令：env 或 printenv。 局部变量只对当前的 shell 有效。使用 shell 内置的 set 命令（不带任何选项参数）就可以输出所有变量（包括环境变量）和一系列的函数。  创建变量 变量名是大小写敏感的，一般用大写命名全局变量，用小写命名局部变量。变量名可以包含数字，但是不能以数字开头。定义变量的语法为：VARNAME=value。\n~$ variable1=123 ~$ echo variable1 variable1 ~$ echo $variable1 123 ~$ echo ${variable1} 123 与我们常用的编程语言不同，shell 中变量定义中的 等号两侧是不能有空格的，否则会出错。这与 shell 对命令的解释有关，例如：\n shell 会将 VARIABLE =value 当作命令 VARIABLE 运行，它有一个参数 =value shell 会将 VARIABLE= value 当作命令 value 运行，并且认为有一个值为 \u0026quot;\u0026quot; 的环境变量 VARIABLE  ~$ VARIABLE= value value: command not found ~$ VARIABLE =value VARIABLE: command not found ~$ VARIABLE = value VARIABLE: command not found 从技术的角度来说，变量的名字因出现在赋值语句的左边而被称为 lvalue，变量的值因出现在赋值语句的右边而被称为 rvalue。\n变量的类型 Bash 并不像其它编程语言那样看重变量的类型。Bash 变量本质上都是字符串，但是根据上下文的不同，如果bash 也是允许在变量上进行算术运算和比较操作的。\n如果变量的值只包含数字，那么我们是可以在它上面进行正常的算术运算的。如果变量的值为 null 或包含数字以外的字符，在它上面进行算术运算时，变量的值会被当做整数 0。\n设置变量的类型 如果我们硬是要指定变量的类型，也是可以的。Shell 内置的 declare 和 typeset 命令都可以设置变量的值和属性，这两个命令是同义词（synonym）。declare 语句的语法为：declare OPTION(s) VARIABLE=value，它支持的选项可以用 man 命令查看。\n导出变量 在 shell 中创建的变量只对当前 shell 有效，它们都是局部变量。如果我们在当前 shell 中使用 bash 或类似的命令开启一个子 shell，父 shell 中定义的变量是不可用的。要将变量传递到子 shell 中，我们可以使用 export 命令导出变量，导出后的变量就会被当作环境变量对待。\n特殊参数（Special parameters） 从命令行传递给脚本的参数用：$0,$1,$2,$3... 表示。其中，$0 时脚本本身的名字，$1 是第一个参数，$2 是第二个参数，依次类推。$9 之后的参数必须用大括号包裹起来，例如：${10}。这些参数由于和出现位置有关，所以被称为位置参数（positional parameters）。特殊变量 $* 和 $@ 表示所有的参数。\nQuoting characters 在有些上下文中，很多字符或者单词都有特殊的含义，用特殊符号将它们引起来可以消除这些特殊含义。\n~$ a=123 转义字符（\\） 转义字符（Escape characters）用来移除其后 单个字符 的特殊含义。Bash 中的转义字符是反斜杠（\\）。\n~$ echo \\$a $a ~$ echo \\${a} ${a} 转义符在行尾（对换行符进行转义）时，表示当前行的命令还未写完。这允许我们写出可读性更强的长命令，例如：\n~$ echo foo\\ \u0026gt; bar foobar 单引号（''） 将变量的引用放在单引号中（'…'）就会让变量名被当作字面值处理，即不会发生变量替换，所以单引号被称为“full quoting”，有时又称“strong quoting”。\n~$ echo \u0026#39;$a\u0026#39; $a ~$ echo \u0026#39;${a}\u0026#39; ${a} 单引号（Single quotes）被用来移除两个单引号之间 所有字符 的特殊含义，即单引号中的所有字符都是字面值（literal value）。两个单引号之间是不能有单引号的，即便这个单引号被转义都不行。\n双引号（\u0026quot;\u0026quot;） 将变量的引用放在双引号中（\u0026quot;…\u0026quot;）并不会对变量替换造成影响，所以双引号被称为“partial quoting”，有时又称“weak quoting”。\n双引号（Double quotes）会移除两个双引号之间除美元符号（$）、反引号（`）和反斜杠（\\）之外的所有字符的特殊含义。美元符号和反引号的特殊含义在双引号之间会保留，而反斜杠只有在其后的字符是美元符号、反引号、双引号、反斜杠或换行符时才保留特殊含义，输入流会将这些字符前的反斜杠去掉。双引号之间是可以有由反斜杠转义的双引号的。\n~$ echo \u0026#34;$a\u0026#34; 123 ~$ echo \u0026#34;${a}\u0026#34; 123 双引号还可以保留变量值中的空白符，防止 shell 在解释命令时去掉多余的空白符。\n~$ hello=\u0026#34;A B C D\u0026#34; echo $hello A B C D echo \u0026#34;$hello\u0026#34; A B C D 参数替换 ${parameter}：获取变量 parameter 的值，同 $parameter，但是前者的含义更加明确\n~$ param=123 ~$ echo ${param} 123 ${parameter-default} 或 ${parameter:-default}：如果参数 parameter 未设置，则使用 default 作为结果，但不会将 default 赋值给 parameter。: 的作用是：如果 parameter 已声明且值为 null，则使用 default。若没有 :，则只要 parameter 已声明，就不会使用 default\n~$ echo ${param-1} 123 ~$ echo ${param:-1} 123 ~$ echo ${param1-1} 1 ~$ echo ${param2:-2} 2 ~$ param3= # param3 is declared, but is null ~$ echo ${param3:-3} 3 ~$ echo ${param3-3} ~$ echo ${param3} ${parameter=default} 和 ${parameter:=default}：如果 parameter 未设置，则使用 default 并将其赋值给 parameter。这里 : 的含义与 ${parameter:-default} 中的 : 类似。\n~$ param4= # param4 is declared, but is null ~$ echo ${param4:=4} 4 ~$ echo ${param5=5} 5 ~$ echo ${param5} 4 ${parameter+alt_value} 和 ${parameter:+alt_value}：如果 parameter 已设置，则使用 alt_value 但不进行赋值，否则使用 null。\n${parameter?err_msg} 和 ${parameter:?err_msg}：如果 parameter 已设置，则使用它，否则打印 err_msg 并终止脚本执行。\n${#var}：计算字符串 var 的长度，或数组 var 中第一个元素的长度。\n${var#Pattern} 和 ${var##Pattern}：从 $var 首部开始移除 $Pattern 最短（#）或最长（##）匹配上的部分。\n${var%Pattern} 和 ${var%%Pattern}：从 $var 尾部开始移除 $Pattern 最短（%）或最长（%%）匹配上的部分。\n${var:pos}：从偏移量 pos 处（即跳过 pos 个字符）展开 $var。\n${var:pos:len}：从偏移量 pos 处展开最大长度为 len 个字符。\n${var/Pattern/Replacement}：使用 Replacement 替换 var 中第一个匹配上 Pattern 的部分。\n${var//Pattern/Replacement}：使用 Replacement 替换 var 中所有匹配上 Pattern 的部分。\n${var/#Pattern/Replacement}：若 var 的前缀匹配上了 Pattern，则使用 Replacement 替换 Pattern。\n${var/%Pattern/Replacement}：若 var 的后缀匹配上了 Pattern，则使用 Replacement 替换 Pattern。\n${!varprefix*} 和 ${!varprefix@}：筛选出所有名字以 varprefix 开头的已声明的变量。\n参考资料  Bash Guide for Beginners. Advanced Bash-Scripting Guide.  ","href":"/posts/linux/shell_scripting_variables/","title":"Shell 脚本：变量"},{"content":" 接下来将会有一系列与 Shell 脚本相关的笔记文章。在之前的工作中，作为一个 Java Boy，我几乎不需要自己编写 Shell 脚本，所以大学学过的 Shell 脚本编程基本忘完（实际上我并没有系统的学过它😂），工作中碰到相关东西时也是 Google 一下就搞定了。但是，现在的工作要求我能够编写 Shell 脚本实现一些自动化操作，所以我决定系统地学习一下 Shell 脚本编程的相关知识。主要参考的学习资料是 The Linux Documentation Project 网站上推荐的两本在线书籍：Bash Guide for Beginners 和 Advanced Bash-Scripting Guide。前者适合入门，后者适合精进，感谢作者的无私分享。\n Shell Shell 既是一个命令解释器（command interpreter），又是一门编程语言。Shell 脚本（shell scripts）是用 shell 编程语言编写的程序，它可以将系统调用、各种工具和已编译的二进制文件粘合在一起，形成新的应用。\nShell 脚本是解释执行的，shell 从脚本中逐行读取命令，然后在系统中昂搜索这些命令并执行。\nShell 有很多种，比如 sh（Bourne Shell）、bash（Bourne Again shell）、csh（C shell）、tcsh（TENEX C shell）、ksh（Korn shell）、tmux 等\n查看系统内已经有的 shell：cat /etc/shells 查看当前用户默认的 shell：cat /etc/passwd | grep $USER | awk 'BEGIN { FS=\u0026quot;:\u0026quot; } { print $7 }\nBash 绝大多数 Linux/Unix 系统默认的 shell 都是 bash，bash 与 sh 相兼容，但是能力更强。\nBash 启动时加载的文件 当 bash 在不同的情况下被调用时（invoked），它会在启动时会读取并执行一些特定的文件，文件的读取顺序会在下面列出，如果某个文件不存在，bash 就会跳过它继续查找下一个文件。\nInvoked as an interactive login shell，or with --login 交互式（interactive）bash 意味着我们可以输入命令，shell 通常会从它连接到的终端（terminal）读取命令，然后将执行结果输出到终端。而“login shell”则表示系统会在使用者使用 shell 之前对其进行身份的认证（通常要使用者提供用户名和密码）。这种情况下 bash 启动时读取的文件有：\n /etc/profile ~/.bash_profile or ~/.bash_login or ~/.profile：第一个存在的可读文件会被读取 ~/.bash_logout：退出时读取  Invoked as an interactive non-login shell non-login shell 表示系统在使用者使用 shell 之前不会对其进行身份的认证。这种情况下读取的文件有：\n ~/.bashrc  Invoked non-interactively 所有 bash 脚本使用的都是非交互式（non-interactively）shell。这时读取的文件有：\n 由变量 BASH_ENV 定义的文件，不过在搜索这个文件时并不会用上 PATH  Invoked with the sh command 这个时候读取的文件有：\n /etc/profile ~/.profile  Shell 语法 如果输入不是注释（comment），shell 会读入并将其拆分成单词（word）和操作符（operator），使用“quoting rules”确定输入中每个字符的含义。然后，这些单词和操作符会被翻译成命令和其它结构（construct）。Shell 分析输入的过程如下：\n Shell 从文件、字符串或用户的终端读取输入 按照“quoting rules”将输入分解为单词和操作符。这些符号（token）被元字符（metacharacter）分隔。别名（alias）展开被执行 Shell 将这些符号解析（分析和替换）成简单命令或复合命令 Bash 执行各种 shell 展开（expansion），将展开的符号分解成文件名（filename）、命令（command）和参数（argument）列表 如果有必要的话，重定向被执行。重定向操作符和它们的操作数被从参数列表中移除 命令被执行 （可选）shell 等待命令执行完毕并收集命令的退出状态。  She-Bang Sha-Bang，又叫 Shebang、Hashbang，是一个由井号（sharp）和叹号（bang）构成的字符串 #!。若文件的第一行（空白行也是有效行）的前两个字符是 Sha-Bang，Linux 操作系统的进程加载器会分析 Sha-Bang 后面的内容，将这些内容作为解释器指令并调用该指令，载有 Sha-Bang 的文件路径会作为该解释器的参数（常见的 $0）。所以 Sha-Bang 的作用是告知系统这个文件包含着要交给命令解释器执行的命令（即整个脚本），而 Sha-Bang 后面的路径就是这个命令解释器的路径。很多 shell 脚本的第一行都是 #!/bin/bash 或 #!/usr/bin/bash。\n要执行某个脚本，用户不仅需要具备该脚本的读取权限，还需要具备该脚本的执行权限，光有执行权限是不够的。\n调试 Shell 脚本 若要调试（debug）shell 脚本，可以使用 bash 的 -x 选项 （在执行命令之前打印命令的 trace 信息）选项，例如：bash -x your_script.sh。\nbash -x 是作用域整个脚本的，即脚本中所有命令的 trace 信息都会被打印出来。有时候，我们只想关注脚本中的某一部分，这个使用可以使用 shell 内置的 set 命令：使用 set -x 开启调试，使用 set +x 关闭调试。Shell 选项中的 +/- 与我们平常使用其它程序的习惯刚好相反，需要注意。set -x 和 set +x 可以在脚本中出现任意多次。\n还有一种开启整个脚本调试信息的方法，那就是将 -x 选项直接加到 Sha-Bang 行解释器路径的后面，例如：#!/bin/bash -x。\n参考资料  Bash Guide for Beginners. Advanced Bash-Scripting Guide.  ","href":"/posts/linux/shell_scripting_introduction/","title":"Shell 脚本：初见"},{"content":"","href":"/tags/rust/","title":"Rust"},{"content":"","href":"/categories/rust/","title":"Rust"},{"content":"任何软件都不可避免地出现各种或大或小的 Bug，软件设计的一大目标就是软件的健壮性，而错误处理就是提高软件健壮性的一大利器。作为一门对安全比较执着的编程语言，Rust 给我们开发者提供了很多处理错误的功能特性。\nRust 中的错误（error）分为两种：可恢复错误（recoverable error）和不可恢复错误（unrecoverable error）。在 Java 中，不仅有错误（error），还有异常（exception），但 Rust 中是没有异常的。对于可恢复错误，Rust 给我们提供了 Result\u0026lt;T, E\u0026gt; 这种数据类型，供我们作进一步决定。而对于不可恢复错误，Rust 给我们提供了一个叫做 panic! 的宏，panic! 会在程序遭遇不可恢复错误时立即停止程序的执行。\n不可恢复错误与 panic! 有时候，你碰到的错误（比如数组下标越界）会让你束手无策。这时，你可以使用 Rust 提供的 panic! 宏直接终止程序，防止更坏的事情发生。当 panic! 执行时，Rust 程序默认会先打印出失败信息，解退（unwinding）并清理栈，然后退出。解退是一种比较优雅退出方式，与之相对的是直接终止（abort）。二者的区别在于，解退会清理数据，归还内存，而直接终止就不会清理数据，回收内存的工作交给了操作系统。\n先来看一个简单的 panic!：\nfn main() { panic!(\u0026#34;crash and burn\u0026#34;); } 这个程序一运行，就会出现错误信息：\n\u0026gt; cargo run Compiling error_handling v0.1.0 (F:\\Code\\Rust\\rust-study\\error_handling) Finished dev [unoptimized + debuginfo] target(s) in 3.19s Running `target\\debug\\error_handling.exe` thread \u0026#39;main\u0026#39; panicked at \u0026#39;crash and burn\u0026#39;, src\\main.rs:2:5 note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace error: process didn\u0026#39;t exit successfully: `target\\debug\\error_handling.exe` (exit code: 101) 倒数第三行展示了 panic 出现的具体位置——src/main.rs 的第二行，倒数第二行则告诉了我们展示回溯（backtrace）的方法，最后一行则是给出了进程成功退出以及退出码的信息。\nBacktrace 如果 panic 出现在我们写的代码还好，这时可以直接看到错误发生在哪一行。但如果 panic 出现在 Rust 的核心库或者标准库内部会怎样呢？我们来看一个数组下标越界的例子：\nfn main() { let v = vec![1, 2, 3]; v[99]; } 这份代码能够正常通过编译，但是在运行的时候会出现以下错误：\n\u0026gt; cargo run Compiling error_handling v0.1.0 (F:\\Code\\Rust\\rust-study\\error_handling) Finished dev [unoptimized + debuginfo] target(s) in 2.67s Running `target\\debug\\error_handling.exe` thread \u0026#39;main\u0026#39; panicked at \u0026#39;index out of bounds: the len is 3 but the index is 99\u0026#39;, src\\main.rs:3:5 note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace error: process didn\u0026#39;t exit successfully: `target\\debug\\error_handling.exe` (exit code: 101) 虽然错误信息告诉了我们错误发生在第三行，但没有给出具体是哪个地方导致了 panic 的出现。根据提示，我们可以使用环境变量 RUST_BACKTRACE=1 来展示 backtrace。\nBacktrace 是一个由执行到 panic 发生位置过程中被调用的函数组成的列表。通过它，我们就能一步一步地走到 panic 出现的具体位置。我们修改环境变量，重新运行程序：\n\u0026gt; $env:RUST_BACKTRACE=1 \u0026gt; cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.06s Running `target\\debug\\error_handling.exe` thread \u0026#39;main\u0026#39; panicked at \u0026#39;index out of bounds: the len is 3 but the index is 99\u0026#39;, src\\main.rs:3:5 stack backtrace: 0: std::panicking::begin_panic_handler at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633\\/library\\std\\src\\panicking.rs:515 1: core::panicking::panic_fmt at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633\\/library\\core\\src\\panicking.rs:92 2: core::panicking::panic_bounds_check at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633\\/library\\core\\src\\panicking.rs:69 3: core::slice::index::{{impl}}::index\u0026lt;i32\u0026gt; at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633\\library\\core\\src\\slice\\index.rs:184 4: core::slice::index::{{impl}}::index\u0026lt;i32,usize\u0026gt; at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633\\library\\core\\src\\slice\\index.rs:15 5: alloc::vec::{{impl}}::index\u0026lt;i32,usize,alloc::alloc::Global\u0026gt; at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633\\library\\alloc\\src\\vec\\mod.rs:2428 6: error_handling::main at .\\src\\main.rs:3 7: core::ops::function::FnOnce::call_once\u0026lt;fn(),tuple\u0026lt;\u0026gt;\u0026gt; at /rustc/a178d0322ce20e33eac124758e837cbd80a6f633\\library\\core\\src\\ops\\function.rs:227 note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace. error: process didn\u0026#39;t exit successfully: `target\\debug\\error_handling.exe` (exit code: 101) 这次输出的信息就多了不少。在 backtrace 中，最下面是我们写的代码，最上面则是库中发生 panic 的具体位置。若要避免 panic，我们就需要修改自己的代码，防止代码缺陷被库函数触发库函数内的 panic。\n可恢复错误与 Result\u0026lt;T, E\u0026gt; panic! 一般用于那些我们无法处理的错误，但在实际的编程过程中，大部分失败情况都是我们能够处理的。这时，我们一般会拿到错误信息，然后根据错误信息进行相关处理，确保程序能够继续执行下去。\nResult\u0026lt;T, E\u0026gt; 这个枚举类型就是专门用来处理可能出现的失败的，它的定义如下：\nenum Result\u0026lt;T, E\u0026gt; { OK(T), Err(E), } Result 包含 OK 和 Err 这两个 variant，分别表示成功和失败的情况。这里的 T 和 E 都是泛型参数，分别用来表示 OK 中成功的返回值的类型和 Err 中失败的错误类型。\n我们来看一个打开文件的例子：\nuse std::fs::File; fn main() { let f = File::open(\u0026#34;hello.txt\u0026#34;); } 打开一个文件可能有多种结果：如果文件存在并且是可读的，我们就能从 Result 的 OK 中得到一个文件句柄（file handle），否则得到的将会是 Err 中携带的错误。错误可能是文件不存在，也可能是我们没有文件的读取权限……。如果是文件不存在，我们就可以主动创建文件，然后继续执行后续步骤。这就是一个可恢复错误的例子。先来看最简单的，文件不存在时直接退出：\nuse std::fs::File; fn main() { let f = File::open(\u0026#34;hello.txt\u0026#34;); let f = match f { Ok(file) =\u0026gt; file, Err(error) =\u0026gt; panic!(\u0026#34;Problem opening the file: {:?}\u0026#34;, error), }; } 以上代码用 match 去匹配枚举类 Result，成功时得到文件句柄，失败时输出错误信息并退出程序。如果我们想在文件不存在时创建文件该怎么做呢？这种情况下，就需要去判断错误的类型：\nuse std::fs::File; use std::io::ErrorKind; fn main() { let f = File::open(\u0026#34;hello.txt\u0026#34;); let f = match f { Ok(file) =\u0026gt; file, Err(error) =\u0026gt; match error.kind() { ErrorKind::NotFound =\u0026gt; match File::create(\u0026#34;hello.txt\u0026#34;) { Ok(fc) =\u0026gt; fc, Err(e) =\u0026gt; panic!(\u0026#34;Problem creating the file: {:?}\u0026#34;, e), }, other_error =\u0026gt; { panic!(\u0026#34;Problem opening the file: {:?}\u0026#34;, other_error) } }, }; } 现在代码开起来开始有点复杂了。实际上，Result 有一个方法叫 unwrap_or_else，它可以代替上面的 match 表达式：\nuse std::fs::File; use std::io::ErrorKind; fn main() { let f = File::open(\u0026#34;hello.txt\u0026#34;).unwrap_or_else(|error| { if error.kind() == ErrorKind::NotFound { File::create(\u0026#34;hello.txt\u0026#34;).unwrap_or_else(|error| { panic!(\u0026#34;Problem creating the file: {:?}\u0026#34;, error); }) } else { panic!(\u0026#34;Problem opening the file: {:?}\u0026#34;, error); } }); } 错误的传播 有时候，我们可能并不像自己处理错误，而是希望将错误的处理交给调用我们代码的人。当我们这么做时，就将错误传播（propagating）给了调用方，给了调用方更多的控制能力。例如，当文件不存在时，直接返回错误给调用方：\nuse std::fs::File; use std::io; use std::io::Read; fn read_username_from_file() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let f = File::open(\u0026#34;hello.txt\u0026#34;); let mut f = match f { Ok(file) =\u0026gt; file, Err(e) =\u0026gt; return Err(e), }; let mut s = String::new(); match f.read_to_string(\u0026amp;mut s) { Ok(_) =\u0026gt; Ok(s), Err(e) =\u0026gt; Err(e), } } 为了提高代码的可读性，Rust 还给我们提供了一个传播错误的简便写法——使用 ? 操作符。例如，? 操作符改写上述代码的结果如下：\nfn read_username_from_file() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let f = File::open(\u0026#34;hello.txt\u0026#34;)?; let mut s = String::new(); f.read_to_string(\u0026amp;mut s)?; OK(s) } ? 甚至还支持链式操作：\nfn read_username_from_file() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let mut s = String::new(); File::open(\u0026#34;hello.txt\u0026#34;)?.read_to_string(\u0026amp;mut s)?; OK(s) } 但是，? 的使用是有限制的。它只能用在返回结果类型是 Result、Option 或实现了 std::ops::Try 的类型的函数上。\n参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language.  ","href":"/rust/rust_basics_error_handling/","title":"Rust 基础：错误处理"},{"content":"枚举（enum or enumeration） 是一种允许我们列举出所有可能的值的数据类型，每一个可能的值都是一个 variant。举个简单的例子，常见的 IP 地址有 IPv4 和 IPv6 两种，这时就可以定义一个表示 IP 地址类型的枚举类。\n枚举的定义 我们先来定义一个表示 IP 地址类型的枚举类：\nenum IpAddrKind { V4, V6, } 现在我们可以使用刚定义的 IpAddrKind 了。在 Rust 中，枚举类型实例的通过 :: 操作符创建的，:: 的左边是命名空间（枚举类型），右边是枚举类型中的可能值（variant）。例如：\nlet four = IpAddrKind::V4; let six = IpAddrKind::V6; 在 Rust 中，枚举类型中所有 variant 的类型都是相同的，这一点与其它编程语言有些不同。例如 IpAddrKind::V4 和 IpAddrKind::V6 的类型都是 IpAddrKind。现在的 IpAddrKind 只有类型的含义，并不能存储任何 IP 地址数据，我们把它稍微改造一下：\nenum IpAddrKind { V4(String), V6(String), } let home = IpAddrKind::V4(String::from(\u0026#34;127.0.0.1\u0026#34;)); let loopback = IpAddrKind::V6(String::from(\u0026#34;::1\u0026#34;)); 现在，IpAddrKind 都能存储地址数据了。不过它还可以变得更加高级，IPv4 的地址可以用四个 0~255 之间的整数表示：\nenum IpAddrKind { V4(u8, u8, u8, u8), V6(String), } let home = IpAddrKind::V4(127, 0, 0, 1); let loopback = IpAddrKind::V6(String::from(\u0026#34;::1\u0026#34;)); 这就是枚举比结构体更灵活的地址。枚举类型中的每一个值都可以是不同的类型，而结构体则做不到这一点。要同时表示两种类型的 IP 地址，需要定义两种不同类型的结构体，而只需要一种类型的枚举类即可！\n枚举也是支持使用 impl 定义方法的。例如：\nimpl IpAddrKind { fn doSomething(\u0026amp;self) { println!(\u0026#34;do something\u0026#34;); } } fn main() { let home = IpAddrKind::V4(String::from(\u0026#34;127.0.0.1\u0026#34;)); let loopback = IpAddrKind::V6(String::from(\u0026#34;::1\u0026#34;)); home.doSomething(); loopback.doSomething(); } 以上代码为 IpAddrKind 定义了一个 doSomething 方法。枚举类型是不支持关联函数的。\nOption 与 Null 在支持 Null 的编程语言中，数据的值要么是 null，要么是 not-null。在这些编程语言中，我们必须非常小心，因为 Null Pointer Exception 经常出现。但在 Rust 中，是没有 Null 这个概念的，取而代之的是 Option。Option 是 Rust 标准库定义的一个枚举类型。它的作用非常大，可以表示有或者没有的概念。\n我们可以换个角度来看待 null 与 not-null：not-null 表示值存在，而 null 表示值缺失。Option 恰好用Some 和 None 表达出了有与没有的概念：\nenum Option\u0026lt;T\u0026gt; { None, Some\u0026lt;T\u0026gt;, } 这里的 \u0026lt;T\u0026gt; 是 Rust 中的泛型语法，可以表示任何类型。我们不需要导入任何模块就可以直接使用 Option，因为它实在是太重要了。此外，None 和 Some 在使用时也可以不加 Option:: 这个前缀。例如：\nfn main() { let some_number = Some(5); let some_string = Some(\u0026#34;a string\u0026#34;); let absent_number: Option\u0026lt;i32\u0026gt; = None; } 有一点需要注意：在使用 None 的时候，我们必须显式地给出数据的类型。因为 None 可以表示任何类型，Rust 的编译器是无法推断出它的实际类型的。\n那么，为什么 Option\u0026lt;T\u0026gt; 比 null 要好呢？这时因为 Option\u0026lt;T\u0026gt; 和 \u0026lt;T\u0026gt; 表示的是不同的类型，编译器是不会允许我们将 Option\u0026lt;T\u0026gt; 当作绝对有效的值（编译器会确保 \u0026lt;T\u0026gt; 表示的值总是有效的）使用的，它会确保我们在使用 Option\u0026lt;T\u0026gt; 之前处理了空值的情况。\n使用 match 进行模式匹配 Rust 提供了一个极其强大的控制流操作符——match。我们可以使用 match 进行模式匹配（pattern matching），然后在不同的模式下执行不同的代码。这种操作有点像 Java 和其它编程语言种的 switch...case。但是 Rust 种的模式匹配更加强大，模式可以是字面值、变量名、通配符等等。并且，Rust 的编译器会确保我们处理了所有可能的情况，这就是利用 match 进行模式匹配的强大之处。\n现在来看一个例子：\nenum IpAddrKind { V4(u8, u8, u8, u8), V6(String), } fn is_ipv4(ip: IpAddrKind) -\u0026gt; bool { match ip { IpAddrKind::V4(a, b, c, d) =\u0026gt; true, IpAddrKind::V6(s) =\u0026gt; false, } } fn main() { let home = IpAddrKind::V4(127, 0, 0, 1); let loopback = IpAddrKind::V6(String::from(\u0026#34;::1\u0026#34;)); println!(\u0026#34;home is Ipv4 address: {}\u0026#34;, is_ipv4(home)); println!(\u0026#34;loopback is Ipv4 address: {}\u0026#34;, is_ipv4(loopback)); } 以上代码定义了一个判断 IP 地址是否为 IPv4 地址的函数 is_ipv4。程序的最终运行结果为：\nhome is Ipv4 address: true loopback is Ipv4 address: false 你可能会觉得 match 代码块种的 (a, b, c, d) 和 (s) 有些奇怪。这时因为我们定义的 IPAddrKind::V4 包含四个字段，IpAddrKind::V6 包含一个字段，在使用时必须给出所有字段的值，否则编译会报错。\n我们来仔细看一下 match 表达式。整个表达式以 match 关键字开始，然后是被匹配的数据（ip），再往后则是由尖括号 {} 包裹的具体分支。每个分支之间用 , 隔开。Rust 管每个分支叫 arm，一个 arm 由三部分组成：\n 模式（例如 IpAddrKind::V6(s)） =\u0026gt; 操作符 匹配成功后执行的代码  再来看一个使用 match 匹配 Option\u0026lt;T\u0026gt; 的例子：\nfn main() { fn plus_one(x: Option\u0026lt;i32\u0026gt;) -\u0026gt; Option\u0026lt;i32\u0026gt; { match x { None =\u0026gt; None, Some(i) =\u0026gt; Some(i + 1), } } let five = Some(5); let six = plus_one(five); let none = plus_one(None); } 如果省略 match 代码块中的 None =\u0026gt; None, 会怎么样呢？答案是编译失败，并且编译器会提示我们没有覆盖到 None 的情况。这再次说明了 Rust 要求我们在使用 match 进行模式匹配必须要覆盖所有的情况。\n有时候，我们可能并不希望把模式所有可能的值都列举出来。这时可以使用 Rust 给我们提供的占位符 _。_ 是一种特殊的模式，可以匹配所有的情况。一般将它放到最后，用于表示当前剩下的未列举出来的所有模式。例如：\nfn plus_one(x: Option\u0026lt;i32\u0026gt;) -\u0026gt; Option\u0026lt;i32\u0026gt; { match x { Some(i) =\u0026gt; Some(i + 1), _ =\u0026gt; None, } } 仔细看上面这个 plus_one 函数，实际有意义的操作只发生在 x 为 Some(i) 的时候，_ 的情况略显多余。对于只需要匹配一模式并忽略剩余所有模式的情况，Rust 给我们提供了 if let 语法。例如：\nfn main() { fn print_i32(x: Option\u0026lt;i32\u0026gt;) { if let Some(i) = x { println!(\u0026#34;x = {}\u0026#34;, i); }; } print_i32(Some(1)); print_i32(None); } 程序的最终输出结果为：\nx = 1 if let 后面跟的分别是模式（Some(i)）、= 、被匹配的值（x）和匹配成功后进行的操作（println!(\u0026quot;x = {}\u0026quot;, i);）。\nif let 也可以搭配 else 使用，例如：\nfn plus_one(x: Option\u0026lt;i32\u0026gt;) -\u0026gt; Option\u0026lt;i32\u0026gt; { if let Some(i) = x { Some(i + 1) } else { None } } 参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language.  ","href":"/rust/rust_basics_enums_and_patterns_matching/","title":"Rust 基础：枚举与模式匹配"},{"content":"在 Rust 中，结构体（struct or structure） 是一种可自定义的数据类型，允许我们将一组相关数据组织在一起，形成一个有意义的整体。Rust 中的结构体和 C 语言中的结构体或 Java 中对象的数据属性类似。\n结构体的定义 虽然 Rust 中的元组（tuple）也可以将一组相关数据组织在一起，但是元组中的各个数据项的含义却不那么直观，因为我们在访问元组中的元素时需要使用下标，而下标是没有任何特定的业务含义的。相比之下，结构体则更要灵活一些，它允许我们给结构体中的每个数据指定一个名字（这有点像键值对了）。\nRust 使用关键字 struct 定义结构体，下面是一个例子：\nstruct User { username: String, email: String, sign_in_count: u64, active: bool, } 结构体 User 的定义主要由三部分组成：\n struct 关键字 结构体的名字，这里是 User 结构体的字段（field）定义，每个字段定义从左到右分别是字段的名字（比如 username）、冒号（:）和字段类型（比如 String），不同字段定义之间用逗号（,）隔开。最后一个逗号是可选的  需要注意的是：结构体的定义末尾不能加分号（;），否则编译器会抛出类似于下面这样的错误：\nerror: expected item, found `;` --\u0026gt; src\\main.rs:6:2 6 | }; | ^ help: remove this semicolon | = help: braced struct declarations are not followed by a semicolon 结构体字段的访问与更新 结构体定义好了，接下来就是使用它。为了使用 User 结构体，我们需要创建一个 User 的实例（instance）：\nfn main() { let user1 = User { email: String::from(\u0026#34;someone@example.com\u0026#34;), username: String::from(\u0026#34;someone\u0026#34;), active: true, sign_in_count: 1 }; println!(\u0026#34;{}\u0026#34;, user1.username); } 创建结构体实例时，字段的顺序必须严格按照定义结构体时的顺序。但所有字段都必须给出，否则编译器会报错。以上示例还展示了读取结构体实例字段值的方法，那就是使用点符号（.）。\n那么如何修改某个字段的值呢？答案还是使用点符号（.），并且还要将整个实例声明为可变的（Rust 并不允许我们将部分字段标记为可变的），例如：\nfn main() { let mut user1 = User { email: String::from(\u0026#34;someone@example.com\u0026#34;), username: String::from(\u0026#34;someone\u0026#34;), active: true, sign_in_count: 1 }; user1.username = String::from(\u0026#34;anotherone\u0026#34;); println!(\u0026#34;{}\u0026#34;, user1.username); } 这里我还发现了一个有趣的事情：String 和 \u0026amp;str 是不同的类型，虽然 \u0026amp;str 可以替代 String，但它们却是有着很大的不同！如果将 user.username = String::from(\u0026quot;anotherone\u0026quot;); 替换成 user1.username = \u0026quot;anotherone\u0026quot;;，编译器就会抛出类型不匹配的错误：\nerror[E0308]: mismatched types --\u0026gt; src\\main.rs:18:22 | 18 | user1.username = \u0026#34;anotherone\u0026#34;; | | | expected struct `String`, found `\u0026amp;str` | help: try using a conversion method: `\u0026#34;anotherone\u0026#34;.to_string()` 变量与字段同名时字段初始化的简写方法 除了在 main 函数中初始化结构体外，我们也可以单独定义一个函数来返回一个结构体的实例：\nfn build_user(email: String, username: String) -\u0026gt; User { User { email: email, username: username, sign_in_count: 1, active: true } } 有没有觉得这里的 email 和 username 的初始化略显重复？毕竟函数的参数名也是 email 和 username。这时，我们可以使用 字段初始化简写语法（field init shorthand syntax） 来简化代码：\nfn build_user(email: String, username: String) -\u0026gt; User { User { email, username, sign_in_count: 1, active: true } } 即：当函数的参与名与结构体的字段名相同时，可以省略字段的值，但必须保留字段名。\n使用结构体更新语法从其它实例创建实例 有时候，我们会基于现有的实例去创建一个新的实例，然后仅改动新实例中的少部分字段的值，新实例中绝大部分字段值还是和旧实例一致。结构体更新语法（struct update syntax） 可以帮助我们更加优雅地实现这个目标。\n首先看不使用结构体更新语法的写法：\nlet user2 = User { email: String::from(\u0026#34;another@example.com\u0026#34;), username: String::from(\u0026#34;another\u0026#34;), active: user1.active, sign_in_count: user1.sign_in_count, }; 再来看使用结构体更新语法的写法：\nlet user2 = User { email: String::from(\u0026#34;another@example.com\u0026#34;), username: String::from(\u0026#34;another\u0026#34;), ..user1 }; 结构体更新语法 .. 的作用是让剩下那些没有显式给出的字段的值与 .. 后面给出的实例一致。需要注意的是 ..user1 后面是不能跟 , 的，原因是 error: cannot use a comma after the base struct。\n元组结构体 Rust 还支持没有字段名的结构体，这种结构体和元组类似，被称为 元组结构体（tuple struc）。元组结构体的定义由三部分组成：\n struct 关键字 结构体名字 字段类型组成的元组  例如：\nfn main() { struct Color(i32, i32, i32); struct Point(i32, i32, i32); let black = Color(0, 0, 0); let origin = Point(0, 0, 0); } 以上代码定义了两个结构体 Color 和 Point。虽然二者元组中的类型一样，但它们却是两个不同的结构体，不可混用！元组结构体的使用和元组类似，我们可以解构它，也可以是使用 . 加下标去访问特定的元素。\nRust 还支持没有任何字段的结构体，这种结构体被称为 类单元结构体（Unit-Like Struct），它就像空元组 () 一样。\n单元结构体 单元结构体（unit struct）不包含任何字段，例如：struct Unit;\n结构体与所有权 结构体拥有所有字段值的所有权，并且结构体内所有数据的有效期与结构体本身一致。只要结构体有效，结构体内的数据就有效。还记得上文中提到的字段不匹配的错误吗？那个问题其实也和所有权有关系，因为 \u0026amp;str 是引用，它不具备数据的所有权。\n方法 方法（method） 和函数（function）类似，二者除了定义的位置和参数列表不同外，其它完全相同。方法在结构体（或枚举（enum）和 trait 对象）的上下文中定义，并且第一个参数始终是 self（其它参数都在 self 后面）。self 表示的是当前的结构体实例，被调用的方法就在它身上。\n方法的定义 前面提到过，方法的定义出现在一个某个上下文中。来看一个例子：\nstruct Rectangle { width: u32, height: u32, } impl Rectangle { fn area(\u0026amp;self) -\u0026gt; u32 { self.width * self.height } } fn main() { let rect1 = Rectangle { width: 30, height: 50, }; println!( \u0026#34;The area of the rectangle is {} square pixels.\u0026#34;, rect1.area() ); } 以上代码在 Rectangle 上下文中定义了一个求面积的方法 area。这里用到了 impl 关键字，其后是上下文 Rectangle，再往后则是方法的定义。\n虽然 area 方法的定义中包含参数 \u0026amp;self，但是我们在调用它的时候，并没有显式地传入任何参数，而是简单地使用了 rect1.area()，这是因为 \u0026amp;self 在这里指代的是 rect1。仔细一看，似乎 \u0026amp;self 的类型与 rect1 的类型并不匹配，但为何没有出现错误呢？要回答这个问题，我们需要了解 Rust 中的 自动引用和解引用（automatic referencing and deferencing） 功能。\n方法调用时发生自动引用与解引用的少数几个地方之一。这个功能的大致工作过程是这样的：当我们使用 object.something() 调用方法时，Rust 会自动添加 \u0026amp;、\u0026amp;mut 或 *，使得 object 与方法签名匹配。我们可以将上面代码中的 self.width * self.height 替换成 \u0026amp;self.width * \u0026amp;self.height，或将 rect1.area() 替换成 \u0026amp;rect1.area()，程序依然能正确运行。我们甚至可以将：\nimpl Rectangle { fn area(\u0026amp;self) -\u0026gt; u32 { self.width * self.height } } 替换为：\nimpl Rectangle { fn area(self) -\u0026gt; u32 { \u0026amp;self.width * \u0026amp;self.height } } 关联函数 除了定义方法外，impl 还允许我们定义不带 self 参数的函数。这种函数被称为 关联函数（associated function），因为它们与结构体本身相关联。关联函数不是方法，因为它不涉及任何结构体实例。我们从字符串字面量创建 String 的 from 函数就是一个关联函数。在调用关联函数时，需要使用操作符 ::，例如：let s = String::from(\u0026quot;hello\u0026quot;);。\n参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language.  ","href":"/rust/rust_basics_struct/","title":"Rust 基础：结构体"},{"content":"所有权（ownership） 是 Rust 独一无二的功能，也是 Rust 中的核心功能之一。Rust 不需要开发者手动回收内存，也没有垃圾收集器，但它还能保证内存安全，这就是所有权的强大之处。\n在 Rust 中，内存的管理是通过所有权系统（ownership system）进行的，编译器会在编译时根据一系列的规则进行检查。更加令人赞叹的是，所有权系统中的任何功能都不会减慢程序的运行速度！\n栈与堆 在开始学习所有权这个概念之前，我们有必要先回顾一下数据在内存中的存放形式。在 Rust 中，数据被存放在栈（stack）或堆（heap）中。\n栈与堆组织数据的方式不同。在栈中，数据时先入后出（FILO）的，即最后存入的数据最先被使用。而在堆中，数据的使用就没有顺序要求，数据在堆中的组织情况比栈中要差很多。此外，存放在栈中的数据占用的空间必须是 编译期间已知且固定的，对于那些大小不固定或编译时无法知道大小的数据，应当被存放在堆上。在我们往堆中存数据时，内存分配器会从堆中找出一块大小合适的内存空间，标记其已使用，然后返回给我们一个指向分配地址的指针，后续我们就可以使用这个指针访问堆中的数据。这个指针会被存储在栈上，因为它的大小是固定且已知的。\n将数据存储在栈上的速度要快于存储在堆上的速度。因为当数据被放在栈上时，内存分配器不需要为新数据找出一块空闲空间，数据总是被放在栈顶。如果数据要被放到堆上，内存分配器不仅需要从堆中找出一块合适的空闲空间，还需要防止这块空间被其它数据抢占。类似地，访问栈上数据的速度要快于访问堆上数据的速度。因为堆上的数据需要通过栈上的指针才能定位到，这比栈上的数据多了一次内存访问。\n当我们调用一个函数时，传递给函数的值（或指向堆中数据的指针）和函数中的局部变量都是存放在栈上的。当函数调用结束后，这些值都会从栈中弹出。\n讲了这么多，现在该所有权出场了。所有权负责 跟踪哪部分代码正在使用堆中的哪部分数据，最大限度地减少堆中的重复数据，清理堆中未使用的数据，防止内存空间被耗尽。简单来说，所有权管理着堆中的数据。\n那么，Rust 中的哪些数据会被分配在堆上呢？答案是那些大小在编译期间不可知或者大小不固定的数据，比如字符串类型（String）。标量类型（整型、浮点型、布尔类型和字符类型）或复合类型（元组和数组）的数据都是存放在栈上的，作用域结束时就会被从栈上弹出。\n需要注意的是：字符串类型（String）和字符串字面量（string literal）是不同的。字符串字面量（比如 let s = \u0026quot;hello, world\u0026quot;;）是代码中硬编码的（编译时大小已知）、不可变的，而字符串类型的数据被存储在堆上，字符串类型的数据是可变的。我们可以使用 String 的 from 函数从字符串字面量创建一个 String 类型的变量，比如：\nfn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); s.push_str(\u0026#34;, world!\u0026#34;); // push_str() appends a literal to a String  println!(\u0026#34;{}\u0026#34;, s); // This will print `hello, world!` } 所有权规则 所有权的规则如下，它们不可被违反：\n Rust 中的每个值都有一个变量，这个变量就是这个值的 所有者（owner） 每个值在同一时刻有且只有一个所有者 当所有者（变量）超出作用域（scope），值就会被丢弃（drop），值占据的内存被归还  变量在进入作用域后开始生效，此后，变量在超出作用域之前一直都是有效的。举个例子：\nfn main() { { // s is not valid here, it’s not yet declared  let s = \u0026#34;hello\u0026#34;; // s is valid from this point forward  // do stuff with s  } // this scope is now over, and s is no longer valid } 实际上，当变量超出作用域时，Rust 会自动为我们调用一个叫 drop 的特殊函数进行内存的归还。在上面这个例子中，drop 会在第一个 } 处被调用。\n变量与数据的交互方式 在 Rust 中，多个变量可以以不同的方式与同一份数据进行交互，最常见的交互方式有移动（move）和克隆（clone）。\n移动（Move） 先来看一段简单的代码：\nfn main() { let x = 5; let y = x; println!(\u0026#34;x = {}, y = {}.\u0026#34;, x, y); } 这段代码很简单，也能通过编译，最终运行会打印出结果 x = 5, y = 5.。如果将 let x = 5; 修改为 let x = \u0026quot;5\u0026quot;;，仍然能得到一样的运行结果。但是，如果我将 let x = 5; 修改成 let x = String::from(\u0026quot;5\u0026quot;);，编译都过不去了😂。我并没有改动代码的结构，唯一改变的只有变量 x 的类型，这里面到底发生了什么？🤔\n要回答这个问题，我们需要了解 Rust 中 String 在内存中的存储形式。为了能够使用官方的图，让我将代码修改为官方的代码😁：\nfn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1; println!(\u0026#34;s1 = {}, s2 = {}\u0026#34;, s1, s2); } 在 Rust 中，String 由三部分组成：指向字符串实际内容的指针、长度和容量。它们存放在栈上，对应下图中的左侧部分，字符串的实际内容则存放在堆上，对应下图中的右侧部分：\n当变量 s1 被赋值给 s2 后，字符串 s1 在栈上的数据被复制了一份给 s2 用，而堆上的内容没有被复制：\n这个过程似曾相识，它就像 Java 中的浅拷贝（shallow copy）一样。在，Rust 中，实际的过程不完全是这样，因为当 s1 和 s2 都超出作用域时，显然不应该归还两次内存。Rust 为了保证内存安全，并没有进行复制操作，而是在在执行 let s2 = s1; 之后使 s1 失效。这样，当 s1 超出作用域时，Rust 就不需要因为它释放任何内存了。让 s1 失效的操作在 Rust 中被称为 移动（move），let s2 = s1; 的实际作用是让 s1 移动到 s2。所以，正确的图是下面这样的：\n这就是上面代码编译失败的原因，这下应该能看懂编译器给出的信息了：\n\u0026gt; cargo run Compiling ownership v0.1.0 (F:\\Code\\Rust\\rust-study\\ownership) warning: unused variable: `s2` --\u0026gt; src\\main.rs:3:9 | 3 | let s2 = s1; | ^^ help: if this is intentional, prefix it with an underscore: `_s2` | = note: `#[warn(unused_variables)]` on by default error[E0382]: borrow of moved value: `s1` --\u0026gt; src\\main.rs:4:28 | 2 | let s1 = String::from(\u0026#34;hello\u0026#34;); | -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait | -- value moved here 4 | println!(\u0026#34;{}, world!\u0026#34;, s1); | ^^ value borrowed here after move error: aborting due to previous error; 1 warning emitted For more information about this error, try `rustc --explain E0382`. error: could not compile `ownership` To learn more, run the command again with --verbose. F:\\Code\\Rust\\rust-study\\ownership [master +4 ~0 -0 !]\u0026gt; cargo run Compiling ownership v0.1.0 (F:\\Code\\Rust\\rust-study\\ownership) error[E0382]: borrow of moved value: `s1` --\u0026gt; src\\main.rs:4:34 | 2 | let s1 = String::from(\u0026#34;hello\u0026#34;); | -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait 3 | let s2 = s1; | -- value moved here 4 | println!(\u0026#34;s1 = {}, s2 = {}\u0026#34;, s1, s2); | ^^ value borrowed here after move error: aborting due to previous error For more information about this error, try `rustc --explain E0382`. error: could not compile `ownership` To learn more, run the command again with --verbose. 克隆（Clone） Rust 不会自动进行数据的深拷贝（deep copy），因为深拷贝会对程序的性能造成很大的影响。如果我们希望 Rust 进行数据的深拷贝，需要使用 clone 方法。例如：\nfn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1.clone(); println!(\u0026#34;s1 = {}, s2 = {}\u0026#34;, s1, s2); } 这样一来，s1 和 s2 涉及的数据就是下图中展示的这样了：\n栈上数据的复制（Copy） 现在来思考为啥 let x = 5; 或 let x = \u0026quot;5\u0026quot;;时代码能够正常运行。这是因为 5 和 \u0026quot;5\u0026quot; 的大小都是固定的，程序编译时就可以知道它们的大小，所以数据被分配在了栈上。而 Rust 对栈上数据的复制采取的是深拷贝，深拷贝就不存在移动了。\n还记得编译错误“move occurs because s1 has type String, which does not implement the Copy trait”吗？Rust 中有一个特殊的注解叫做 Copy。如果某种数据类型实现了 Copy，旧变量在赋值之后仍然是可以使用的。此外，Rust 中还有一个特殊的注解叫做 Drop，它会在变量超出作用域后做些事情。Rust 不允许我们将 Copy 放在了实现了Drop 的类型上，如果我们这么做，编译会失败。\n那么，哪些类型实现了 Copy 呢？说实话，有点多，具体的内容可以查看 Trait Copy。你会发现，所有的标量类型都实现了 Copy。\n所有权与函数 传递变量给函数时所有权的变化与变量赋值一样。根据数据类型的不同，可能发生移动（丢失所有权）或复制，下面例子中的注释解释得非常清楚：\nfn main() { let s = String::from(\u0026#34;hello\u0026#34;); // s comes into scope  takes_ownership(s); // s\u0026#39;s value moves into the function...  // ... and so is no longer valid here  let x = 5; // x comes into scope  makes_copy(x); // x would move into the function,  // but i32 is Copy, so it\u0026#39;s okay to still  // use x afterward  } // Here, x goes out of scope, then s. But because s\u0026#39;s value was moved, nothing  // special happens.  fn takes_ownership(some_string: String) { // some_string comes into scope  println!(\u0026#34;{}\u0026#34;, some_string); } // Here, some_string goes out of scope and `drop` is called. The backing  // memory is freed.  fn makes_copy(some_integer: i32) { // some_integer comes into scope  println!(\u0026#34;{}\u0026#34;, some_integer); } // Here, some_integer goes out of scope. Nothing special happens. 如果函数带返回值呢？这个时候所有权也会发生转移：\nfn main() { let s1 = gives_ownership(); // gives_ownership moves its return value into s1  let s2 = String::from(\u0026#34;hello\u0026#34;); // s2 comes into scope  let s3 = takes_and_gives_back(s2); // s2 is moved into takes_and_gives_back, which also  // moves its return value into s3 } // Here, s3 goes out of scope and is dropped. s2 goes out of scope but was  // moved, so nothing happens. s1 goes out of scope and is dropped.  fn gives_ownership() -\u0026gt; String { // gives_ownership will move its  // return value into the function that calls it  let some_string = String::from(\u0026#34;hello\u0026#34;); // some_string comes into scope  some_string // some_string is returned and moves out to the calling function } // takes_and_gives_back will take a String and return one fn takes_and_gives_back(a_string: String) -\u0026gt; String { // a_string comes into scope  a_string // a_string is returned and moves out to the calling function } 引用 如果每次都进行 takes_and_gives_back 这种夺取所有权而后又归还所有权的操作，未免也太过繁琐。幸运的是，Rust 为我们提供了 引用（reference），它可以消除 takes_and_gives_bakc 的尴尬之处。\n下面是使用引用的一个例子：\nfn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let len = calculate_length(\u0026amp;s1); println!(\u0026#34;The length of \u0026#39;{}\u0026#39; is {}.\u0026#34;, s1, len); } fn calculate_length(s: \u0026amp;String) -\u0026gt; usize { // s is a reference to a String  s.len() } // Here, s goes out of scope. But because it does not have ownership of what it refers to, nothing happens. 引用允许我们引用某个变量，而不会夺取变量对数据的所有权。引用相关的操作符是 \u0026amp;，与之相反的操作叫解引用（dereferencing），操作符是 *。在以上代码中，\u0026amp;s1 创建了一个指向 s1 的引用，s1 依然持有字符串 \u0026quot;hello\u0026quot; 的所有权。\n由于 \u0026amp;s1 不具有数据的所有权，所以当 \u0026amp;s1 超出作用域时，数据不会被丢弃。Rust 将引用作为函数参数的操作称为 借用（borrowing），这有点像现实生活中别人借了我们的东西又归还一样，所有权不会转移。\n默认情况下，Rust 是不允许我们修改借用的数据的：\nfn main() { let s = String::from(\u0026#34;hello\u0026#34;); change(\u0026amp;s); } fn change(some_string: \u0026amp;String) { some_string.push_str(\u0026#34;, world\u0026#34;); } 以上代码会编译失败，并且编译器会告诉我们错误原因，并给出正确的修改建议：\n\u0026gt; cargo run Compiling ownership v0.1.0 (F:\\Code\\Rust\\rust-study\\ownership) error[E0596]: cannot borrow `*some_string` as mutable, as it is behind a `\u0026amp;` reference --\u0026gt; src\\main.rs:7:5 | 6 | fn change(some_string: \u0026amp;String) { | ------- help: consider changing this to be a mutable reference: `\u0026amp;mut String` 7 | some_string.push_str(\u0026#34;, world\u0026#34;); | ^^^^^^^^^^^ `some_string` is a `\u0026amp;` reference, so the data it refers to cannot be borrowed as mutable error: aborting due to previous error For more information about this error, try `rustc --explain E0596`. error: could not compile `ownership` To learn more, run the command again with --verbose. 可变引用 在上面的错误中，编译器建议我们将 change 函数的参数由 \u0026amp;String 修改为 \u0026amp;mut String，即从不可变引用（immutable reference）修改为可变引用（mutable reference）。这操作和普通变量的可变与不可变类似。正确的代码应该是：\nfn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); change(\u0026amp;mut s); } fn change(some_string: \u0026amp;mut String) { some_string.push_str(\u0026#34;, world\u0026#34;); } 这里进行了三处修改：首先将 s 改为可变的 mut s，然后是传递给 change 的参数修改为 \u0026amp;mut s，最后是 change 函数的定义修改为 \u0026amp;mut String，三者缺一不可。\n但是，可变引用有一个非常大的限制：同一时刻，同一数据的可变引用只能有一个，而不可变引用却可以有多个。例如，下面的代码会编译失败：\nfn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); let r1 = \u0026amp;mut s; let r2 = \u0026amp;mut s; println!(\u0026#34;{}, {}\u0026#34;, r1, r2); } 编译器简单明了地指出了我的错误：\n\u0026gt; cargo run Compiling ownership v0.1.0 (F:\\Code\\Rust\\rust-study\\ownership) error[E0499]: cannot borrow `s` as mutable more than once at a time --\u0026gt; src\\main.rs:4:14 | 3 | let r1 = \u0026amp;mut s; | ------ first mutable borrow occurs here 4 | let r2 = \u0026amp;mut s; | ^^^^^^ second mutable borrow occurs here 5 | println!(\u0026#34;{}, {}\u0026#34;, r1, r2); | -- first borrow later used here error: aborting due to previous error For more information about this error, try `rustc --explain E0499`. error: could not compile `ownership` To learn more, run the command again with --verbose. Rust 这么做的原因是为了防止编译期间的数据竞争（和 Java 并发中的数据竞争类似）。数据竞争会导致程序的行为超出预期，加大运行时诊断问题的难度。\n那么应该如何修改上面有问题的代码呢？拆分两个变量的作用域即可：\nfn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); { let r1 = \u0026amp;mut s; } let r2 = \u0026amp;mut s; println!(\u0026#34;{}, {}\u0026#34;, r1, r2); } 在 Rust 中，引用的作用域从声明开始，结束于引用最后一次被使用。Rust 不允许同一份数据在一个作用域内同时出现可变引用和不可变引用的情况。例如：\nfn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); let r1 = \u0026amp;s; // no problem  let r2 = \u0026amp;s; // no problem  let r3 = \u0026amp;mut s; // BIG PROBLEM  println!(\u0026#34;{}, {}, and {}\u0026#34;, r1, r2, r3); } 会出现以下编译错误：\n\u0026gt; cargo run Compiling ownership v0.1.0 (F:\\Code\\Rust\\rust-study\\ownership) error[E0502]: cannot borrow `s` as mutable because it is also borrowed as immutable --\u0026gt; src\\main.rs:5:14 | 3 | let r1 = \u0026amp;s; // no problem | -- immutable borrow occurs here 4 | let r2 = \u0026amp;s; // no problem 5 | let r3 = \u0026amp;mut s; // BIG PROBLEM | ^^^^^^ mutable borrow occurs here 6 | println!(\u0026#34;{}, {}, and {}\u0026#34;, r1, r2, r3); | -- immutable borrow later used here error: aborting due to previous error For more information about this error, try `rustc --explain E0502`. error: could not compile `ownership` To learn more, run the command again with --verbose. 但如果我将代码修改为：\nfn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); let r1 = \u0026amp;s; // no problem  let r2 = \u0026amp;s; // no problem  println!(\u0026#34;{} and {}\u0026#34;, r1, r2); // variables r1 and r2 will not be used after this point  let r3 = \u0026amp;mut s; // no problem  println!(\u0026#34;{}\u0026#34;, r3); } 程序就能通过编译并正常运行。这是因为，r3 被声明之前，r1 和 r2 的作用域已经结束了，不存在作用域的重叠。\n切片（Slice） 切片（slice）允许我们引用集合中的一段连续元素，它是一种没有所有权的数据类型。\n字符串切片 字符串切片（string slice）是一个指向 String 中的一部分内容的引用，这部分内容是原字符串的一个子串。例如：\nfn main() { let s = String::from(\u0026#34;hello world\u0026#34;); let hello = \u0026amp;s[0..5]; // hello  let world = \u0026amp;s[6..11]; // world  println!(\u0026#34;{}, {}\u0026#34;, hello, world); // hello, world } 在上面这段代码中，hello 和 world 都只引用了 s 中的一部分：\n字符串切片类型在 Rust 中用 \u0026amp;str 表示，它比 String 更加灵活。因为 \u0026amp;str 不仅可以引用 String 的一部分，还能引用 String 的所有内容。利用 \u0026amp;str，我们可以编写获取字符串中第一个单词的函数：\nfn first_word(s: \u0026amp;String) -\u0026gt; \u0026amp;str { let bytes = s.as_bytes(); for (i, \u0026amp;item) in bytes.iter().enumerate() { if item == b\u0026#39; \u0026#39; { return \u0026amp;s[0..i]; } } \u0026amp;s[..] } 现在可以来回答为啥字符串字面量是不可变的这个问题了。因为字符串字面量的数据类型是 \u0026amp;str，而 \u0026amp;str 是不可变引用。\n其它类型的切片 除了字符串切片（\u0026amp;str），Rust 中还有很多其它类型的切片，比如数组的切片：\n#![allow(unused)] fn main() { let a = [1, 2, 3, 4, 5]; let slice = \u0026amp;a[1..3]; assert_eq!(slice, \u0026amp;[2, 3]); } 在以上代码中，slice 是数组 a 的切片，它的数据类型是 \u0026amp;[int32]。\n参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language.  ","href":"/rust/rust_basics_ownership/","title":"Rust 基础：所有权"},{"content":"","href":"/tags/communication/","title":"Communication"},{"content":"","href":"/categories/communication/","title":"Communication"},{"content":"最近工作中需要用到 gPRC，赶紧学了一下，顺便做了些笔记。\ngPRC 与 Protocol Buffers 经常一起使用，二者的关系非常密切。因为 gPRC 不仅将 Protocol Buffers 作为自己的 IDL（Interface Definition Language），还将其用作底层的消息交换格式。\ngRPC  A hight performance, open source universal RPC framework.\n gPRC 是一个由 Google 出品的 RPC（Remote Procedure Call） 框架。在笔者写这篇文章的时候，gPRC 还是云原生计算基金会（CNCF） 的一个孵化项目。\ngRPC 的核心思想和其它的 RPC 框架一样，都是定义一个服务，然后声明可以被远程调用的方法以及方法的参数和返回类型。服务端实现定义的接口并处理客户端的调用，而客户端则有一个桩对象（stub），它提供了与服务端相同的方法。\ngRPC 调用过程 gRPC 的使用者通常会在服务端实现 .proto 文件中描述的服务 API，然后在客户端进行 API 的调用。每一个 API 都涉及服务端和客户端两方面，服务端负责实现，而客户端进行调用，它们是一一对应的。\n服务端不仅会实现 .proto 所描述的服务中声明的方法，还会运行一个处理客户端调用的服务器。gRPC 会解码传入请求，执行服务方法，并编码服务响应。而在客户端这一侧，会有一个实现了相同服务方法本地对象（一般叫 stub，有些语言也称 client）。客户端会直接调用这个本地对象上的方法，使用恰当的 Protocol Buffers 消息类型包裹方法参数。gRPC 会将请求发送给服务端，并获取服务端的响应数据。\n下面这张图来自 gRPC 官网，它不仅展示出了 gRPC 中客户端与服务端交互的过程，还暗示了 gRPC 是跨语言的：\n当下分布式系统和微服务架构非常流行，RPC 的这一大特点使得我们创建分布式应用和服务更加简单。\n定义服务 默认情况下，gRPC 使用 Protocol Buffers 描述服务接口和消息负载（message payload）的结构。我们也可以按照实际需要，选择其它的语言来描述接口和消息的结构。\nservice HelloService { rpc SayHello (HelloRequest) returns (HelloResponse);}message HelloRequest { string greeting = 1;}message HelloResponse { string reply = 1;}以上代码使用 Protocol Buffers 描述了 HelloRequest 和 HelloResponse 这两个消息的结构，以及服务接口 HelloService。在服务接口 HelloService 中，有一个简单的方法叫 SayHello，它从客户端接收 HelloRequest 并以 HelloResponse 响应。SayHello 作为一个入门的例子，给我们展示的是 gRPC 中最简单的通信模式——Unary RPC。\n实际上，gRPC 中有四种通信模式：\n Unary RPC：客户端向服务端发送单个请求，并从服务端获取单个响应。用 Protocol Buffers 描述服务接口的方法，就是：rpc SayHello (HelloRequest) returns (HelloResponse)。 Server Streaming RPC：客户端向服务端发送一个请求，并从服务端获得一个响应流。客户端会从这个响应流中读取一个消息序列，gRPC 会保证每个 RPC 调用中消息的顺序。用 Protocol Buffers 描述服务接口的方法，就是：rpc SayHello (HelloRequest) returns (stream HelloResponse) Client Streaming RPC：客户端向流中写入一个消息序列，然后把它发给服务端，等待服务端返回单个响应。gRPC 依然会保证客户端流中消息的顺序。用 Protocol Buffers 描述服务接口的方法，就是：rpc SayHello (stream HelloRequest) returns (HelloResponse) Bidirectional Streaming RPC：客户端以消息流的方式给服务端发送数据，而服务端也以消息流的方式响应数据。gRPC 会保证每个流当中消息的顺序。用 Protocol Buffers 描述服务接口的方法，就是：rpc SayHello (stream HelloRequest) returns (stream HelloResponse)  不难发现，流式通信与普通通信在描述时只有一个差别：是否有 stream 关键字。\nProtocol Buffers  Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.\n Protocol Buffers 是一种对结构化数据进行序列化的机制，它不仅跨语言、跨平台，还可扩展。\n消息类型的定义 直接看官网的例子：\nsyntax = \u0026#34;proto3\u0026#34;;message SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3;}以上定义了一个非常简单的消息类型（message type）。如果要使用 proto3，文件的第一非空白且非注释的行必须这么写，否则编译器会认为我们使用的是 proto2。在 SearchRequest 内部，声明了三个字段（键值对），每个字段都有自己的名字和类型。此外，每个字段还有一个与之相关联的数字（unique number），一个消息中的每个字段的数字各不相同。其实，这些数字是用来识别二进制格式的消息中的字段的，所以它们应该是唯一的，并且在消息类型投入使用之后不应该再被修改。\n标识字段的数字也是有范围限制的，允许的范围是 $[1, 2^{29} - 1]$。其中，[19000, 19999] 是 Protocol Buffers 自己使用的，我们不可使用它们。范围在 [1, 15] 内的数字采用一个字节编码（编码内容包括数字本身和字段类型），应当被用在使用最频繁的字段上；而范围在 [16, 2047] 内的数字在编码时采用两个字节，适合被用在使用没那么频繁的字段上。\n字段类型 消息的字段可以是标量类型（scalar type），也可以是复合类型（比如枚举类型或其它消息类型）。\n标量类型 以下表格列出了 Protocol Buffers 中的标量类型与其在 Java/Python 中对应的数据类型：\n   .proto Type Java Type Python Type     double float float   float float float   int32 int int   int64 long int/long   uint32 int int/long   uint64 long int/long   sint32 int int   sint64 long int/long   fixed32 int int/long   fixed64 long int/long   sfixed32 int int   sfixed64 long int/long   bool boolean bool   string String str/unicode   bytes ByteString str    不同类型的具体细节可以参考 原表格。\n枚举类型 如果我们希望消息字段的值位于某个特定的集合中，则可以将该字段定义成枚举类型（enumeration）。例如：\nmessage SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } Corpus corpus = 4;}在定义枚举类型时，我们必须让数字 0 与枚举类型的第一个元素相关联。因为 0 是数值类型的默认值，0 与第一个元素相关联的目的是为了维持对 proto2 的兼容性。\n默认值 不同的数据类型有着不同的默认值。当 Protocol Buffers 解析消息时，如果发现编码后的消息中并不存在某个字段，则会将对应字段的值设置为字段类型的默认值。不同类型的默认值如下：\n   字段类型 默认值     string empty string   bytes empty bytes   bool false   numeric types zero   enum first defined enum value, which must be 0   message not set   repeated field empty    在 Java 中使用 Protocol Buffers Protocol Buffers 的编译器 protoc 能够根据目标语言为我们生成 .proto 文件对应的代码。对于 Java 来说，protoc 会为每一种消息类型都生成一个 .java 文件，每个 .java 文件中都有一个对应的 Builder 负责创建消息类的实例。\n直接来看官网的一个例子：\nsyntax = \u0026#34;proto3\u0026#34;;package tutorial;option java_multiple_files = true;option java_package = \u0026#34;com.example.tutorial.protos\u0026#34;;option java_outer_classname = \u0026#34;AddressBookProtos\u0026#34;;message Person { optional string name = 1; optional int32 id = 2; optional string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { optional string number = 1; optional PhoneType type = 2; } repeated PhoneNumber phones = 4;}message AddressBook { repeated Person people = 1;}这段代码首先告诉编译器使用 proto3，然后声明了当前 .proto 文件所属的包（package），用于解决不同项目之间可能出现的命名冲突，它的作用和 Java 中的包很相似。不过你会发现，后面还有一个 java_package，它给出了 Java 的包名，用于存放生成的 Java 类文件。如果没有这一行，编译器会使用 package 的值作为 Java 的包名。\njava_outer_classname 则定义了表示整个 .proto 文件的 Java 类名，而 java_multiple_files 则告诉编译器要将每个生成的 Java 类都放到一个单独的 .java 文件中。\nBuilder 编译器 protoc 生成的所有 Java 消息类都是 不可变 的。这意味着，消息对象在被构造出来之后，就不能被修改，就像 Java 中的 String 一样。要构建一个消息对象，我们必须先构造出对应的 Builder 对象，使用 Builder 为字段赋值，最后调用 Builder 的 build() 方法完成对象的创建。\n参考资料  gPRC Documentation. Protocol Buffers Guide.  ","href":"/posts/communication/gprc-and-protocol-buffers/","title":"gPRC 与 Protocol Buffers"},{"content":"","href":"/tags/rpc/","title":"RPC"},{"content":"","href":"/categories/rpc/","title":"RPC"},{"content":"几乎所有的编程语言中都有控制流（control flow）的概念，控制流即根据条件的成立与否决定代码的执行逻辑。在 Rust 中，控制流分为 if 表达式和循环。\nif 表达式 一个简单的 if if 表达式告诉程序：如果条件成立，则执行某段代码，否则不执行那段代码。举个例子：\nfn main() { let condition = true; if condition { println!(\u0026#34;The condition was true.\u0026#34;) } } 使用 cargo run 运行这段代码，程序的输出结果是：\nThe condition was true. 但如果你将 let condition = true; 替换成 let condition = false;，就不会看到任何输出。\n在 Rust 中，所以的 if 表达式都以关键字 if 开头，随后是一个 bool 类型的条件，条件满足时执行的代码放在条件后面的尖括号里。Rust 要求条件的数据类型必须是布尔类型，因为 Rust 不会自动进行非布尔类型到布尔类型的值转换。所以，以下代码是不会通过编译的：\nfn main() { let number = 1; if number { println!(\u0026#34;The value of number is: {}\u0026#34;, number); } } 使用 else 我们也可以给 if 表达式加一个可选的 else 表达式，用来告知程序当条件不满足的时候应该执行何种操作。例如：\nfn main() { let condition = false; if condition { println!(\u0026#34;The condition was true.\u0026#34;) } else { println!(\u0026#34;The condition was false.\u0026#34;); } } 使用 cargo run 运行以上代码，你会看到屏幕上打印出了以下内容：\nThe condition was false. 使用 else if 如果我们有多个条件，那么该如何写呢？答案是使用 else if。举个例子：\nfn main() { let number = 6; if number % 4 == 0 { println!(\u0026#34;number is divisible by 4\u0026#34;); } else if number % 3 == 0 { println!(\u0026#34;number is divisible by 3\u0026#34;); } else if number % 2 == 0 { println!(\u0026#34;number is divisible by 2\u0026#34;); } else { println!(\u0026#34;number is not divisible by 4, 3, or 2\u0026#34;); } } 以上程序中有四条执行路径，但是只会打印一条路径上的结果：\nnumber is divisible by 3 这是因为 Rust 会依次检查每一个条件，遇到第一个值为 true 的条件就执行与之关联的代码块，然后结束对所有后续条件的检查。\n在 let 语句中使用 if 因为 if 是一个表达式，所以我们可以在 let 语句中使用它，例如：\nfn main() { let condition = true; let number = if condition {5} else {6}; println!(\u0026#34;The value of number is: {}\u0026#34;, number); } if 表达式的返回值会被绑定到变量 number 上。很明显，这里绑定给变量 number 的值是 5。我们可以使用 cargo run 检查一下运行结果：\nThe value of number is: 5 Rust 要求所有可能从 if 表达式返回的值都必须是同一个类型。例如：\nfn main() { let number = if true {5} else {\u0026#34;six\u0026#34;} } 以上代码会由于 if 表达式可能返回的 5 和 \u0026quot;six\u0026quot; 两个值类型不同而无法通过编译。\n循环 如果我们想让某一段代码执行多次，该怎么实现呢？一种简单且笨的方法是将那段代码复制多次，例如：\nfn main() { println!(\u0026#34;again!\u0026#34;); println!(\u0026#34;again!\u0026#34;); println!(\u0026#34;again!\u0026#34;); } 显然复制不是一个好主意。以上代码打印了三次 again!，但如果我们想让程序一直不停地打印 again! 呢？这时候复制就无能为力了（毕竟复制是个体力活😏）。正确的姿势是使用循环，循环可以不断地执行，我们也可以在某个时候中止循环。Rust 中有三类循环：loop、while 和 for。\nloop loop 告诉 Rust 不停地重复执行某个代码块。例如：\nfn main() { loop { println!(\u0026#34;again!\u0026#34;); } } 如果运行这段代码，你的屏幕上会不断地打印出 again!。在你手动停止程序之前，这个打印过程不会停止。\n你可能想问，如果我想在某个时候退出循环，应该怎么做？答案就是使用 break 关键字显式地告诉程序不要再执行循环了。例如：\nfn main() { let mut counter = 0; loop { if counter == 2 { println!(\u0026#34;The value of counter is: 2. Over!\u0026#34;); break; } println!(\u0026#34;The value of counter is not 2. Again!\u0026#34;); counter += 1; } } 这段代码的结果如下：\nThe value of counter is not 2. Again! The value of counter is not 2. Again! The value of counter is: 2. Over! 在 Rust 中，break 不仅可以终止循环，还可以在终止循环的时候返回一个值供程序中的其它代码使用。这使得 break 有点像 Rust 函数中的 return 了。我们来看一个例子：\nfn main() { let mut counter = 0; let result = loop { counter += 1; if counter == 2 { break counter; } }; println!(\u0026#34;The result is {}\u0026#34;, result); } 以上代码中声明了一个 result 变量，它将持有从 loop 循环返回的值。最终程序的输出结果为：\nThe result is 2 while 在 loop 的例子中，我们不断检查 counter 的值，当 counter 的值为 2 时就使用 break 结束循环，否则继续执行循环内的代码块。这也可以使用 Rust 中的 while 循环来实现，而且它看起来更加简洁。让我们使用 while 循环来改造第一个 counter 示例：\nfn main() { let mut counter = 0; while counter != 2 { println!(\u0026#34;The value of counter is not 2. Again!\u0026#34;); counter += 1; } println!(\u0026#34;The value of counter is: 2. Over!\u0026#34;); } 程序的运行结果依然是：\nThe value of counter is not 2. Again! The value of counter is not 2. Again! The value of counter is: 2. Over! 在 while 循环中，条件位于 while 关键字后面。每次循环之前都会检查条件是否成立，若成立则执行循环体内的代码，否则结束循环。\nfor Rust 中的 for 循环与其它编程语言中的 for 有些不同。它主要用于对集合或某个范围进行迭代操作，需要搭配关键字 in 使用，在某些情况下可以替代 loop 和 while。例如：\nfn main() { for element in [1, 2, 3] { println!(\u0026#34;{}\u0026#34;, element); } } 以上代码的运行结果为：\n1 2 3 再来看一个范围迭代：\nfn main() { for element in 1..4 { println!(\u0026#34;{}\u0026#34;, element); } } 以上代码的运行结果依然是：\n1 2 3 在 Rust 中，范围（range）默认是左闭右开的。当然，范围也有左闭右闭的写法。例如，1..4 与 1.=3 是等价的。\n参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language.  ","href":"/rust/rust_basics_control_flow/","title":"Rust 基础：控制流"},{"content":"在 Rust 中，函数无处不在。我们学习“Hello, world!”时接触到的 main 函数就是 Rust 中最重要的函数。我们也知道，声明 main 函数的时候需要用到关键字 fn。实际上，Rust 中的所有函数都是通过 fn 声明的。\n在正式学习函数之前，有必要先了解一下 Rust 中的函数与变量命名风格。不同于 Java 中的驼峰命名（camel case），Rust 中推崇使用蛇形命名（snake case）。在蛇形命名中，所有字母都是小写的，单词之间使用下划线（_）分隔，比如 hello_world。\n一个简单的函数 下面代码展示了一个简单地无参函数的使用：\nfn main() { println!(\u0026#34;Hello, world!\u0026#34;); another_function(); } fn another_function() { println!(\u0026#34;Another function.\u0026#34;); } 使用 cargo run 运行程序，结果如下：\nHello, world! Another function. 代码中定义的 another_function 的定义出现在 main 函数之后，它还可以出现在 main 函数之前。其实，Rust 并不关心我们在何处定义函数，只要它能找到就行。\n再来看一下 another_function 的组成：首先是 fn 关键字，往后是函数名，函数名后跟了一对圆括号，最后则是有尖括号包裹的函数体。因为 another_function 不需要任何参数，所以圆括号内没有任何内容。\n函数参数 Rust 中的函数是可以有参数的，我们只需要把参数信息放到函数名后面的圆括号中，就可以在函数体中使用它们了。例如，我们可以对 another_function 稍加改造，把它变成一个有参函数：\nfn main() { another_function(5); } fn another_function(x: i32) { println!(\u0026#34;The value of x is: {}\u0026#34;, x); } 使用 cargo run 运行程序，你就会看到控制台打印出了以下内容：\nThe value of x is: 5 在这个例子中，another_function 有一个类型为 i32 的参数 x。需要注意的是，这里的 i32 是不可省略的。我们必须在函数签名中显式给出每个参数的类型，否则就会编译失败。如果我们的函数有多个参数，使用逗号进行分隔即可，多个参数的类型不必相同。比如：\nfn another_fuction(x: i32, y: i8) { println!(\u0026#34;x = {}, y = {}\u0026#34;, x, y); } 上面代码中的 another_fuction 就有 x 和 y 两个参数。其中，参数 x 的类型是 i32，而参数 y 的类型是 i8。\n语句与表达式 Rust 是一门基于表达式的语言。表达式（expressions）会计算并返回一个值。语句（statements）则是执行某些动作的指令，并不会返回一个值。来看一个例子：\nfn main() { let x = 1; } 在上面的代码中，1 是一个表达式，它返回的值恰好是 1。而 let x = 1 则是一个语句，它没有任何返回值。因此，下面这种写法是无法通过编译的：\nfn main() { let y = (let x = 1); } 因为 let x = 1 是一个语句，它没有返回值，所以没有任何东西绑定到变量 y 上。Rust 在这一点上跟其它编程语言有很大不同，在很多编程语言中 y = x = 1 是合法的，而 Rust 中是不允许的。\n函数调用是表达式，宏调用是表达式，我们用来创建新作用域的代码块（{}）也是一个表达式。例如：\nfn main() { let x = 5; let y = { let x = 3; x + 1 }; println!(\u0026#34;The value of y is: {}\u0026#34;, y); } 在以上代码中，表达式：\n{ let x = 3; x + 1 } 返回的值是 4，而这个 4 最终会被绑定到变量 y 上。我们可能会觉得这个表达式看上去很奇怪，尤其是最后一行 x + 1 居然没有用分号结尾。这是因为，在 Rust 中，表达式是不包括末尾的分号的，如果我们将分号加到表达式后面，那就不是表达式而是一个语句了。\n带返回值的函数 迄今为止，我们写的函数都是不带返回值的。那么带返回值的函数该怎么写呢？先来看一个例子：\nfn five() -\u0026gt; i32 { 5 } fn main() { let x = five(); println!(\u0026#34;The value of x is: {}\u0026#34;, x); } 和之前的函数不同的是，我们在圆括号后面加了 -\u0026gt; 和返回值类型 i32，同时，函数体中的最后一行是一个表达式 5。这就是带返回值的函数的写法，即在函数签名上加上 -\u0026gt; 和返回值类型，并在函数体的最后一行写上一个表达式，这个表达式就是我们的返回值，表达式的类型就是函数签名上 -\u0026gt; 后面的函数类型。返回值并不一样要在函数体的最后一行，我们也可以在函数体的中间使用关键字 return 加上一个表达式提前返回。使用 return 是显式返回，而函数体中最后一行的表达式则是隐式返回。\n参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language.  ","href":"/rust/rust_basics_functions/","title":"Rust 基础：函数"},{"content":"变量 变量与可变性 在 Rust 中，变量（variables）有两种：\n 不可变变量：一经赋值，就不再允许对变量的值进行修改 可变变量：可可对变量的值进行多次修改  在 Rust 中，我们使用 let 关键字声明一个变量，例如：let x = 1。默认情况下，let 声明的变量是 不可变的。如果代码里面出现不可变变量被多次赋值的情况，则代码是不会编译通过的。例如：\nfn main() { let x = 5; println!(\u0026#34;The value of x is: {}\u0026#34;, x); x = 6; println!(\u0026#34;The value of x is: {}\u0026#34;, x); } 如果使用 cargo run 运行这段代码，则会出现以下错误：\n\u0026gt; cargo run Compiling variables v0.1.0 (F:\\Code\\Rust\\rust-study\\variables) error[E0384]: cannot assign twice to immutable variable `x` --\u0026gt; src\\main.rs:4:5 | 2 | let x = 5; | - | | | first assignment to `x` | help: consider making this binding mutable: `mut x` 3 | println!(\u0026#34;The value of x is: {}\u0026#34;, x); 4 | x = 6; | ^^^^^ cannot assign twice to immutable variable error: aborting due to previous error For more information about this error, try `rustc --explain E0384`. error: could not compile `variables` To learn more, run the command again with --verbose. Cargo 给了我们很多有用的错误信息，其中就一行被标成了红色，十分醒目：cannot assign twice to immutable variable。也就是说，不可以再次给不可变变量赋值。\n解决错误的方法很简单，将第二行替换成 let mut x = 5 即可。我们在变量 x 之前加了一个关键字 mut，表明 x 是一个可变变量。再次运行 cargo run，我们就能看到正确的输出了。\n常量 和其它编程语言一样，Rust 中也有常量（constants）这个概念。Rust 中的常量在赋值之后就不能修改。有人可能会问，这不就跟不可变变量一样吗？实际上，这只是常量与不可变变量的一个相同点，二者还是有差异的。\n首先，声明常量的关键字是 const，而声明变量的关键字是 let。\nconst ONE: u32 = 1; 以上代码声明了一个名为 ONE 的常量，并赋值为 1。常量名约定使用大写字母，多个单词之间用下划线隔开。\n其次，常量的赋值部分只能是一个常量表达式，不能是某个函数调用的结果或运行期间计算出来的值。而变量就没有这个限制。\nlet x = 1; const SQUARE = x * x; 上面这段代码是无法正常编译的，因为 x 是变量，而 SQUARE 是常量，x * x 的值是在程序运行过程中计算出来的。\n此外，static 关键字也可以用来声明一个常量，使用 static 关键字声明的常量代表的是一个内存地址，它的生命周期为 'static，而 const 声明的常量代表的是一个值。0246-const-vs-static 描述了二者的不同。\n隐藏 我们可以多次定义一个同名的变量，后面定义的变量会隐藏（shadowing）它之前的同名变量，即程序只能看到最新一个同名变量的值。\nfn main() { let x = 5; let x = x + 1; { let x = x * 2; println!(\u0026#34;The value of x in the inner scope is: {}\u0026#34;, x); } println!(\u0026#34;The value of x is: {}\u0026#34;, x); } 以上这段代码很有意思，它首先将变量 x 的值绑定为 5，然后通过 let x = 隐藏 5 这个值，真正可见的值是 6。而在内部作用域中，第二个 let 再次隐藏 x 之前的值，真正可见的值是 12。内部作用域结束，作用域内的隐藏效果也结束了，x 的值又变成 6。所以，这个程序的输出结果应该是：\nThe value of x in the inner scope is: 12 The value of x in the inner scope is: 6 如果我们去掉第二行开头的 let，程序就因不可变变量被再次赋值而编译失败。不可变变量的隐藏与不可变变量的赋值是完全不同的，我们可以认为隐藏就是重新声明了一个新的变量。\n既然如此，为啥不直接使用可变变量呢？原因是隐藏更加灵活，因为不可变变量的变量类型在声明之后就不能再改了，而隐藏可以修改变量的数据类型。例如，下面的代码是可以编译成功的：\nlet a = 1; let a = \u0026#34;1\u0026#34;; 而下面这段代码将会编译失败：\nlet mut a = 1; a = \u0026#34;1\u0026#34;; 冻结（Freezing） 当数据被绑定到不可变的同名变量上时，该数据会被冻结。被冻结的数据只有在超出不可变绑定的作用域之外才能被修改。例如：\nfn main() { let mut _mutable_integer = 7i32; { // Shadowing by immutable `_mutable_integer`  let _mutable_integer = _mutable_integer; // Error! `_mutable_integer` is frozen in this scope  _mutable_integer = 50; // FIXME ^ Comment out this line  // `_mutable_integer` goes out of scope  } // Ok! `_mutable_integer` is not frozen in this scope  _mutable_integer = 3; } 数据类型 Rust 是一门 静态类型 语言，在编译期间就必须确定所有变量的类型。Rust 中的每一个变量的值都属于某一数据类型，Rust 通过数据类型得知变量的值是何种数据，进而决定如何处理这个值。\n我们并不需要一一声明变量所属的数据类型，Rust 的编译器是很智能的，它可以根据值及其使用方式推断出我们想要使用的数据类型。但是，当某个值可能属于多种数据类型时，我们就必须通过类型注解显式地给出变量的数据类型，否则编译就会出错。\nlet x = 1; // 编译器可以推断出 x 的数据类型 let guess: u32 = \u0026#34;42\u0026#34;.parse().expect(\u0026#34;Not a number!\u0026#34;); // \u0026#34;42\u0026#34;.parse() 的结果可能对应多种类型，所以需要使用类型注解 在 Rust 中，数据类型分为两类：标量类型（scalar types）和复合类型（compound types）\n标量类型 一个标量类型表示的是单个值。Rust 中有四种基本的标量类型：整型、浮点型、布尔类型和字符类型。\n整型 整型（integer）是没有小数部分的数字。根据是否可以表示负数，整型又可以分为有符号整型和无符号整型。下面的表格列出了 Rust 中的所有整型：\n   Length Signed Unsigned     8-bit i8 u8   16-bit i16 u16   32-bit i32 u32   64-bit i64 u64   128-bit i128 u128   arch isize usize    其中，有符号整型以字母 i 开头，无符号整型以字母 u 开头。i 或 u 后面的数字即该类型所占的二进制位数。假设整型的长度为 n 位，则有符号整型能表示数值范围的区间是 $[-2^{n - 1}, 2^{n - 1} - 1]$，而无符号整型能表示数值范围区间是 $[0, 2^n - 1]$。例如，i8 可存储的数值范围是 [0, 255]，而 u8 可存储的数值范围是 [-128, 127]。表格中的 isize 和 usize 类型由运行程序的机器架构决定：对于 64 位架构的计算机来说，它们就是 64 位的，而对于 32 位架构的计算机来说，它们就是 32 位的。Rust 中默认的整型是 i32。\n除了将整数写成最常见的十进制形式外，Rust 还允许我们将数字写成十六进制、八进制等形式。为了便于阅读，我们不仅可以在数值后面跟上它的具体类型（比如 58u8 表示 u8 类型的数字 58），还可以使用下划线（_）对数字进行分隔（比如 10_000 就是数字 10000）。下面表格列出了 Rust 中的各种整数表示法，除了默认的十进制表示法外，各个表示形式都有特定的前缀：\n   Number Literals Example     Decimal 98_222   Hex 0xff   Octal 0o77   Binary 0b1111_0000   Byte(u8 only) b\u0026rsquo;A'    浮点型 浮点型（floating-point）是带小数点的数字。Rust 有两种原生的浮点类型：单精度浮点型 f32（占 32 位）和双精度浮点型 f64（占 64 位）。f64 是 Rust 中默认的浮点类型。例如：\nlet x = 2.0; // f64 let y: f32 = 3.0; // f32 和大多数编程语言一眼，Rust 中的浮点型是使用 IEEE-754 标准表示的。\n布尔类型 Rust 中的布尔类型有两个可能的值：true 和 false。布尔类型占一个字节，类型注解为 bool。\n字符类型 字符类型（char）占 4 个字节，可以表示 U+0000 到 U+D7FF 之间和 U+E000 到 U+10FFF 之间的 Unicode 字符。\n复合类型 复合类型可以将多个值组合到一个类型中。Rust 内置了元组（tuples）和数组（arrays）这两种复合类型。\n元组 元组是一个由多个值组成的集合，每个值的类型可以不同，其长度在指定后就不能修改。创建一个元组的方式很简单，将各个值用逗号分隔，放在小括号当中即可：\nlet tup: (i32, f64, u8) = (500, 6.4, 1); 以上代码创建了一个包含三个不同类型数值的元组 tup。那么如何访问元组中的元素呢？Rust 给我们提供了两种方法，其中一种是使用模式匹配对元组进行解构（destructure）：\nlet tup = (500, 6.4, 1); let (x, y, z) = tup; println!(\u0026#34;The value of y is: {}\u0026#34;, y); 以上代码首先将元组 (500, 6.4, 1) 绑定到了变量 tup 上，然后使用模式 (x, y, z) 将 tup 分成了三个不同的部分，这个过程就是解构（destructring）。除了使用模式匹配解构外，我们还可以通过点号（.）加索引的方式访问元组中的值：\nlet tup = (500, 6.4, 1); let x = tup.0; // 500 let y = tup.1; // 6.4 let z = tup.2; // 1 println!(\u0026#34;x = {}, y = {}, z = {}\u0026#34;, x, y, z); 和大多数编程语言一样，Rust 中的下标也是从 0 开始的。\n元组甚至可以包含元组，例如：\nlet tuple_of_tuples = ((1u8, 2u16, 2u32), (4u64, -1i8), -2i16); 如果元组中只有一个值，那么末尾的那个逗号是不能省略的，否则编译器会认为那是一个字面量。\nprintln!(\u0026#34;A tuple: {:?}\u0026#34;, (5,)); // A tuple: (5,) print!(\u0026#34;An integer: {:?}\u0026#34;, (5)); // An integer: 5 不包含任何值的元组（空元组）是一种特殊的数据类型——单元类型（unit type）。这种数据类型只有一个可能的值，即 ()，它被称为单元值（unit value）。虽然单元值是一个元组，但是它并不属于复合类型，因为它并不包含多个值。\n数组 数组只能包含多个相同类型得值，其长度在指定后也不能修改。创建一个数组的方式也很简单，将各个值用逗号分隔，然后放到中括号当中即可：\nlet arr = [1, 2, 3]; 我们可以通过下标访问数组中的元素：\nlet arr = [1, 2, 3]; let x = arr[0]; // 1 let y = arr[1]; // 2 let z = arr[2]; // 3 println!(\u0026#34;x = {}, y = {}, z = {}\u0026#34;, x, y, z); 我们也可以在声明数组的时候显式给出数组元素的类型和数组长度，例如：\nlet arr: [i32; 3] = [1, 2, 3]; 以上代码声明了一个长度为 3 的数组 [1, 2, 3]，数组中的元素类型为 i32。\n此外，Rust 还给我们提供了将数组初始化为相同值的简便写法：\nlet arr = [1; 3]; 以上代码声明了一个长度为 3 的数组，数组中所有元素都是 1。该写法等价于：let arr = [1, 1, 1]。\n自定义数据类型 Rust 当然支持我们创建自定义的数据类型，创建自定义数据类型的主要途径有两种：\n 使用 enum 定义枚举类 使用 struct 定义结构体  类型转换 基本类型之间的类型转换 Rust 是不允许基本类型的数据之间进行隐式类型转换的。不过，我们可以使用 as 关键字显式要求进行数据转换。例如：\nlet decimal = 6.5; // Error! No implicit conversion // let integer: u8 = decimal; let integer = decimal as u8; let character = integer as char; Rust 管基本类型数据之间的类型转换叫 Casting。\n自定义类型数据之间的类型转换 在 Rust 中，自定义类型数据之间的类型转换是通过使用 traits 实现的。一般情况下使用的 traits 是 From 和 Into。如果转换可能失败，则使用 TryFrom 和 TryInto 会更合适。此外，数据类型和字符串之间的转换使用的 traits 是 FromStr 和 ToString。\nRust 管自定义类型数据之间的类型转换叫 Conversion。\n类型别名 Rust 允许我们使用 type 关键字给已存在的数据类型设置新的名字（alias）。如果新的名字不是驼峰命名的，编译器会发出警告，但编译还是会通过。如果不想让编译器发出警告，可以使用 #[allow(non_camel_case_types)] 属性。类型别名并不是新的数据类型，它只是一个别名而已。类型别名主要用于减少样板代码，比如 IoResult\u0026lt;T\u0026gt; 就是 Result\u0026lt;T, IoError\u0026gt; 的一个别名。下面是使用类型别名的例子：\nype NanoSecond = u64; type Inch = u64; #[allow(non_camel_case_types)] type u64_t = u64; // NanoSecond, Inch and u64_t are the same type as u64 参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language. Rust by Example.  ","href":"/rust/rust_basics_variables_and_data_types/","title":"Rust 基础：变量与数据类型"},{"content":" 年初学过几天 Rust，走马观花，很快就忘完了。现在有了些空闲时间，准备重新好好地把 Rust 学一遍，为后续操作系统的深入学习做好铺垫。在此之前，我主要写 Java，几乎所有的 Java 开发者都有过内存溢出（OOM）或内存泄露的噩梦。为了避免各种内存问题，Java 开发者通常需要熟悉垃圾回收，以及如何进行垃圾收集器的调优。当我第一次遇到 Rust 的时候，就被 Rust 惊人的运行速度和极高的内存利用率所吸引，这门语言居然没有运行时（runtime）和垃圾回收器（garbage collector），多么地神奇！\n 安装 Rust 学习一门编程语言的最佳方式就是实践。在实践之前，我们需要准备好相应的开发环境。Rust 推荐我们使用 rustup 安装 Rust。rustup 是一个命令行工具，用于管理 Rust 版本和相关的其它工具。在 Windows 平台上，我只需要先下载 rustup，运行它，然后根据提示一步一步操作即可。除了 rustup，还有很多其它的安装方法，具体可以参考官方文档。\n安装完成后，我们可以通过 rustc --version 检查是否安装成功。若控制台输出了 rustc x.y.z (abcabcabc yyyy-mm-dd) 形式的版本号，则表示安装成功。\nrustup rustup 是一个很有用的工具，它给我们提供了很多实用的功能。\n 更新 Rust 到最新版本：rustup update 卸载 Rust 和 rustup：rustup self uninstall 查看 Rust 的本地文档：rustup doc  Hello, world! 从大学的 C/C++，到后来的 Java、Python，我们学习一门新的编程语言几乎都是从“Hello, world!”开始。下面来写一个 Rust 版本的“Hello, world!”。\n为了组织好代码，我们通常会将不同的代码放到不同的文件夹中。先给“Hello, world!”创建一个文件夹 hello_world：\n\u0026gt; mkdir hello_world \u0026gt; cd hello_world 现在，创建 Rust 的源代码文件，这里我管它叫 hello_world.rs。Rust 的源代码文件总是以 .rs 结尾，如果文件名包含不止一个单词，建议使用下划线（_）分隔。\nfn main() { println!(\u0026#34;Hello, world!\u0026#34;); } 这就是 Rust 版本的“Hello, world!”源代码，和 Java 比起来，简单了太多。下一步要做的就是编译源文件得到可执行文件，然后运行：\n\u0026gt; rustc hello_world.rs \u0026gt; ./hello_world.exe Hello, world! 到此，我们“Hello, world!”就全部完成了。\nHello, world! 详解 “Hello, world!” 虽然只有短短三行，但麻雀虽小，五脏俱全。它包含了一个简单 Rust 程序必要的组成部分。首先是\nfn main() { } 这里定义了一个 Rust 函数。和很多编程语言一样，main 函数有着特殊的地位，它是 Rust 程序的执行入口。我们的“Hello, world!” 中的 main 函数是没有参数的，如果你需要参数，把它们放到 () 中即可。函数体被包裹在 {} 中，Rust 中习惯将 { 放在函数定义的同一行，中间用一个空格隔开。\n 实际上，我们不需要过于在意代码的风格。因为 Rust 给我们提供了一个代码自动格式化工具——rustfmt，它是随着 Rust 的分发包一起发布的，也就是说，你的电脑上已经有它了。\n 在 main 函数内部有一行代码：\nprintln!(\u0026#34;Hello, world!\u0026#34;); 这行代码正是整个“Hello, world!”的核心部分，它将“Hello, world!”打印到了我们的屏幕上。它包含了四个值得我们注意的点：\n Rust style 的缩进是四个空格，而不是一个 TAB println! 调用了 Rust 宏（macro）。如果是调用普通函数的话，是没有后面那个 ! 的，这就是调用宏与普通函数的区别 \u0026quot;Hello, world!\u0026quot; 是一个字符串，我们将它作为参数传递给 println!，最终被输出到了屏幕上 代码上的末尾有一个 ;，代表着当前表达式结束，它后面就是一个新的表达式了  除了宏与函数外，剩余三个部分都和 Java 等编程语言类似。\n从编译到运行 Rust 是一种 预编译（ahead-of-time compiled） 语言，我们可以将编译出来的 Rust 程序直接交给别人，他们不需要安装 Rust 就可以直接运行。与 Python 和 JavaScript 等将编译和运行放在一起的动态语言不同，Rust 中编译和运行是分开的两个步骤。在“Hello, world!”中，我们先通过 Rust 的编译器 rustc 编译源代码：\n\u0026gt; rustc hello_world.rs 在 Windows 上，如果编译成功，我们就会得到两个文件： hello_world.exe和 hello_world.pdb。其中，hello_world.exe 是 Windows 平台上的二进制可执行文件，hello_world.pdb 则是针对 Windows 创建的文件，它包含符号（symbol）、类型（type）和模块（module）等调试信息。pdb 即“Program Database”。\n在 Linux 上，如果编译成功，则只会得到一个二进制可执行文件：hello_world。\n不管是 Windows 上的 hello_world.exe，还是 Linux 上的 hello_world，我们都可以把它们直接放到另一台没有安装 Rust 的机器上运行。这就是 Rust 没有运行时（runtime）的具体体现。与之相对的 Java 语言就是一个很好的例子，我们先将 Java 的源代码文件 .java 编译成字节码文件 .class，再使用 java 命令运行编译字节码文件。这就要求目标机器上安装有 JRE（Java Runtime Environment）。\nCargo 如果程序很简单，使用 rustc 就能轻易应付。但是，随着项目的增长，项目的方方面面都需要被管理，这时 rustc 就难以胜任了。所以，Rust 给我们提供了一个工具——Cargo。Cargo 是 Rust 的程序构建系统和包管理器，可以帮我们完成构建代码、下载依赖等任务。Cargo 之于 Rust 就像 Maven 或 Gradle 之于 Java。\n默认情况下，安装 Rust 的时候也会一并安装 Cargo，我们可以通过以下命令检查 cargo 是否被正确安装：\n\u0026gt; cargo --version 使用 cargo 创建新项目 使用 cargo 创建新项目的操作尤为简单。\n\u0026gt; cargo new hello_cargo \u0026gt; cd hello_cargo 在 hello_cargo 文件夹内，Cargo 默认为我们生成了 .git 目录和 .gitignore、源代码目录 src 以及 Cargo 的配置文件 Cargo.toml。源代码目录 src 里面有一个文件叫 main.rs，它的内容和前文的 hello_world.rs 一致。Cargo 期望所有的源代码文件都位于 src 目录内，项目的根目录中一般是配置文件、README、LICENSE 等与源代码无关的文件。\n我们要重点关注的是 Cargo.toml，文件的内容如下：\n[package] name = \u0026#34;hello_cargo\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2018\u0026#34; # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] .toml 是一种文件格式，它通过片段（section）组织文件内容。我们的 Cargo.toml 就包含 [package] 和 [dependencies] 两个片段。其中，[package] 下面罗列的是当前包的配置信息，这里包括项目名称（name）、版本（version）和 Rust 版本（edition）；[dependencies] 下面罗列的则是项目的依赖。在 Rust 中，代码包被称为 crates。由于 hello_cargo 不需要任何依赖（crates），所以 [dependencies] 下面是空的。\n构建并运行 Cargo 项目 构建 Cargo 项目的命令是：\n\u0026gt; cargo build 以 hello_cargo 为例，在 Windows 上执行 cargo build 之后，项目的根目录下会多出一个 target 文件夹和一个 Cargo.lock 文件。你会发现 target 下有一大堆文件，其中大部分文件都不是太重要，真正的可执行文件是 target/debug/hello_cargo.exe。\n\u0026gt; ./target/debug/hello_cargo.exe Hello, world! Cargo.lock 是第一次运行 cargo build 才会创建的文件，Cargo 用它来跟踪项目中依赖的具体版本。这个文件仅仅是给 Cargo 自己使用的，我们不应该去编辑它。\n除了先手动构建项目再手动运行可执行文件外，Cargo 还给我们提供了一条可以自动构建并运行可执行文件的命令——cargo run。\n使用 cargo check 检查错误 Cargo 还给我们提供了一个叫 cargo check 的命令，它会分析当前包内的代码并报告错误，但并不会产生可执行文件。通常情况下，cargo check 的执行速度比 cargo build 要快得多。如果你经常在编写代码的过程中通过编译来检查代码问题，那么 cargo check 绝对会成为你加速开发的好帮手。很多 Rustaceans 都在编写代码的过程中定期运行 cargo check，确保代码可以正常编译。当他们准备好运行可执行文件时，才会使用 cargo build。\nprofiles Cargo 给我们提供了两种 profile，用于不同场景下的程序构建：\n Default Profile：用于开发环境，cargo build 默认选项 \u0026ndash;release Profile：用于发布项目，需要通过 cargo build --release 显式指出  在开发过程中，我们通常希望编译尽可能快，这样我们能够更快地进行验证，所以 cargo build 默认会生成开发用过程使用的可执行文件。在我们使用 cargo build --release 构建发布项目时，Cargo 会在编译发布版本的过程中进行一系列优化。这些优化会使得你的 Rust 程序运行得更快，但是会消耗更多的编译时间。程序的最终用户通常希望程序运行尽可能地快，这正是是 cargo build --realease 花更多编译时间进行优化的原因。\n不同 profile 构建出来的最终文件存放位置是不一样的。cargo build 编译出来的文件位于 target/debug 目录下，而cargo build --release 编译出来的文件位于 target/release 目录下。\n参考资料  Steve Klabnik, Carol Nichols. The Rust Programming Language.  ","href":"/rust/hello_rust/","title":"Hello, Rust!"},{"content":"","href":"/categories/java/","title":"Java"},{"content":"一谈到原子类（或原子变量），我们可能就会想知道它和我们编程中常说的原子性（Atomicy）之间是否有关系。若一组操作具备“要么全部成功，要么全部失败，不能一部分成功一部分失败”的性质，那么这组操作就是原子操作，这组操作具备原子性。\n在 Java 中，原子操作可以通过锁和循环 CAS 的方式实现。其中 CAS 操作是利用处理器提供的 COMPXCHG 指令实现的。自旋 CAS 实现的基本思路是循环进行 CAS 操作直到成功为止。但 CAS 存在三个问题：\n ABA问题。可以使用版本号来解决。 循环时间长开销大。这一般出现在自旋CAS长时间不成功的情况下。 只能保证一个共享变量的原子操作。对于多个共享变量的原子操作，一般采用锁来解决，也可以将多个共享变量封装进一个对象，然后使用AtomicReference类来解决。  JVM 内部实现了很多锁机制，有意思的是除了偏向锁，JVM 实现锁的方式都用了循环 CAS，即当一个线程进入同步代码块时使用循环 CAS 获取锁，离开同步代码块时使用循环 CAS 释放锁。CAS 最直接的体现就是 java.util.concurrent.atomic 包下定义的各种无锁原子类，这些原子类都支持单个变量上的原子操作。举个例子，i++ 在并发环境中并不是线程安全的，要保证线程安全，我们需要给 i++ 加锁。实际上，我们也可以直接使用原子类提供的 getAndIncrement 方法完成同样的操作。由于原子类底层采用的 CAS 实现，在并发竞争不是特别激烈的情况下，效率要高于同步互斥锁。\n根据操作类型的不同，可以大致将 java.util.concurrent.atomic 下的原子类分成六种：\n   类型 具体类     基本类型原子类（AtomicXxx） AtomicInteger, AtomicLong, AtomicBoolean   引用类型原子类（AtomicXxxReference） AtomicReference, AtomicStampedReference, AtomicMarkableReference   数组类型原子类（AtomicXxxArray） AtomicIntegerArray, AtomicLongArray, AtomicReferenceArray   字段更新原子类（AtomicXxxFieldUpdater） AtomicIntegerFieldUpdater, AtomicLongFieldUpdater, AtomicReferenceFieldUpdater   加法器（Adder） LongAdder, DoubleAdder   累加器（Accumulator） LongAccumulator, DoubleAccumulator    下面分别最这六大类型的原子类进行介绍。\n基本类型原子类 基本类型原子类包括三种：AtomicInteger、AtomicLong 和 AtomicBoolean。其中 AtomicInteger 和 AtomicLong 是对 int、long 的封装，而 AtomicBoolean 内部封装的变量的类型并不是 boolean，而是 int，它用 0 和 1 分别表示 false 和 true。三种类型都提供了对内部封装变量的原子性访问和更新操作，我们可以在并发环境中放心使用。\n以 AtomicInteger 为例，它给我们提供的常用方法有：\n get()：获取封装变量的当前值 getAndSet(int newValue)：获取封装变量的当前值，然后将变量的值设置为新的值 getAndIncrement()：获取封装变量的当前值，然后将变量的值加一 getAndDecrement()：获取封装变量的当前值，然后将变量的值减一 getAndAdd(int delta)：获取封装变量的当前值，然后将变量的值加上 delta compareAndSet(int expectedValue, int newValue)：如果封装变量的当前值为 expectedValue，则原子性地将变量的值设置为 newValue。这个方法就是原子类 CAS 的具体体现。  AtomicInteger 其实还给我们提供了很多实用的方法，这里就不一一列举了。AtomicLong 和 AtomicBoolean 提供的方法也与 AtomicInteger 类似，这些原子类的内部都依赖 JDK 提供的 Unsafe 类，它提供了硬件级别的原子操作，允许调用方直接操作内存中的数据。\n引用类型原子类 引用类型原子类也包括三种：AtomicReference、AtomicStampedReference 和 AtomicMarkableReference。引用类型原子类与基本类型原子类的区别在于：引用类型原子类保证的是基本类型的原子性，而引用类型原子类保证的是对象的原子性。\nAtomicReference 内部维护了一个对象的引用，而AtomicStampedReference 和 AtomicMarkableReference 都是 AtomicReference 的增强版本。其中 AtomicStampedReference 内部维护了一个 int 类型的 stamp 变量，它同对象引用一起原子性地更新，用于解决 CAS 中可能出现的 ABA 问题。而 AtomicMarkableReference 内部则维护了一个 boolean 类型的 mark 变量，它同对象引用一起原子性地更新，可以用来标识对象的状态等。\n数组类型原子类 数组类型原子类也包括三种：AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray，它们内部分别维护了一个 int[]、long[] 和 Object[] 类型的数组，对数组中每一个元素的读写操作都具备原子性。\n字段更新原子类 字段更新原子类也包括三种：AtomicIntegerFieldUpdater、AtomicLongFieldUpdater 和 AtomicReferenceFieldUpdater，分别用于原子性更新对象的 int、long 和引用类型的字段。三者都要求被原子更新的字段被 volatile 修饰，因为它们三个都会在构造函数内检查被更新的字段是否被 volatile 修饰，如果被更新的字段不是 volatible 的，就会抛出异常。\n加法器 加法器有两种：DoubleAdder 和 LongAdder。二者都继承自 Stripe64，该类引入了分段累加的概念，内部有两个参数参与累加，其中一个是 long 类型的 base，另一个是 Cell[] 类型的 cells。其中，base 是在竞争不激烈的情况下使用的，累加结果会被直接改到它上面，而 cells 是在竞争激烈的情况下使用的，各个线程分散累加自己所对应的那个 Cell，这就降低了冲突的概率，提供了并发性。\n加法器实现求和的关键在于 sum() 函数，它先加上 base 的值，然后再依次累加各个 Cell 中的值，最终得到总和。既然 base 和 cells 都定义在 Stripe64 中，那么 DoubleAdder 是如何实现 double 数据的求和的呢？答案在于在 Java 中，double 和 long 都占 8 个字节，二者在一定条件下可以互相表示。所以 DoubleAdder 在累加时调用了 Double.doubleToRawLongBits(double value) 计算出 double 变量的 long 值，求和时调用了 longBitsToDouble(long bits) 计算出 long 变量对应的 double 值。\n加法器就像是基本类型原子类的微缩版本，前者主要负责执行加法，而后者还提供了更多高级的操作。在线程竞争没那么激烈的情况下，二者的吞吐量相差不大，而在线程竞争激烈的情况下，加法器求和的吞吐量就高得多了。不过，这更高的吞吐量是有代价的，那就是更多的空间消耗。\n累加器 和加法器类似，累加器也有两种：DoubleAccumulator 和 LongAccumulator。累加器和加法器很相似，实际上，累加器是一个更加通用的加法器，它增强了加法器的功能，允许我们提供自定义的累加函数。\n参考资料  Atomic Variables.  ","href":"/java/concurrency/java_atomic_variables/","title":"Java 中的原子类"},{"content":"","href":"/tags/%E5%B9%B6%E5%8F%91/","title":"并发"},{"content":"并发队列（或线程安全的队列）是在我们在进行多线程并发编程时经常使用的一种数据结构。并发队列不仅具备基本队列的所有特性，还是线程安全的。由于并发队列在实现时已经考虑了各种线程安全问题，所以我们可以在并发环境中直接使用，而不用担心出现线程安全问题，有利于降低开发难度和工作量。\nJava 中的并发队列可以分为阻塞队列和非阻塞队列两大类。\n阻塞队列 除了队列的基本功能外，阻塞队列最大的特点在于 阻塞：在读取元素时，若队列为空，则阻塞读取操作直到队列非空；在写入元素时，若队列已满，则阻塞写入操作直到队列中出现可用空间。\n阻塞队列的典型代表是 BlockingQueue 接口的各个实现类，主要有 ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、DelayQueue、PriorityBlockingQueue 和 LinkedTransferQueue。\n根据阻塞队列容量的大小，又可以将其分为有界队列和无界队列。有界队列的典型代表是 ArrayBlockingQueue，一旦队列满了就无法入队新的元素了，因为它不会扩容。无界队列的典型代表是 LinkedBlockingQueue，其容量最大为 Integer.MAX_VALUE，即 2^31 - 1，这么大的容量几乎不可能被填满，故可以近似看成无限容量。\n在阻塞队列中有很多相似的方法，比较容易混淆。因此，有必要对它们进行分类整理，根据方法是否抛出异常、是否返回特殊值、是否阻塞、是否具有超时时间，JDK 已经分类好了：\n    Throws exception Special value Blocks Times out     Insert add(e) offer(e) put(e) offer(e, time, unit)   Remove remove() poll() take() poll(time, unit)   Examine element() peek() not applicable not applicable    根据操作类型的不同，特殊值（Special Value）略有差异：布尔类型的特殊值为 false，对象类型的特殊值为 null。\n非阻塞队列 非阻塞队列家族则没有阻塞队列家族这么庞大，典型代表是 ConcurrentLinkedQueue，其内部通过 CAS 保证线程安全，不会阻塞线程，适合并发不是特别剧烈的场景。\n","href":"/java/concurrency/java_concurrent_queue/","title":"Java 中的并发队列"},{"content":"有一道经典的 Java 面试题叫：重写了 equals()，为什么还要重写 hashCode()？\n 不幸的是，笔者最近也被问到这个问题的变种了。当时面试官的提问点有点奇葩，问我这两个方法在被调用时谁先谁后的问题。笔者当时想，这面试官是不是八股文看多了，连这两方法调用先后都问出来了吗？严格上来说，equals() 和 hashCode() 在绝大多数情况下都是单独调用的，只有在像 HashMap 这样的数据结构的内部实现中，才会存在方法调用的先后关系。笔者当时也没完全搞清楚面试官到底想问什么，所以就象征性的回答了 hashCode() 先调用。今天，笔者突然想到这个问题，感觉当时面试官想问的是 HashMap 内部实现时对这两个方法的依赖情况。\n 在 Java 中，一切皆对象。并且所有的对象都直接或间接地继承自 java.lang.Object。Object 类定义了一系列 Java 对象所共有的方法，hashCode() 和 equals() 就在其中。\nhashCode() {hashCode()](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Object.html#hashCode()) 方法用于返回对象的哈希值。Object 类定义了 hashCode() 的契约，这里我直接列出：\n The general contract of hashCode is:\n Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result. It is not required that if two objects are unequal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce distinct integer results. However, the programmer should be aware that producing distinct integer results for unequal objects may improve the performance of hash tables.   equals() equals() 方法用来判断两个对象在是否相等。它与 == 有着本质区别：== 比较的是两个对象的内存地址，即判断它们是否是同一个对象，而 equals() 是判断两个对象在逻辑上是否相等。Object 类也同样定义了 equals() 的契约，我还是直接列出：\n he equals method implements an equivalence relation on non-null object references:\n It is reflexive: for any non-null reference value x, x.equals(x) should return true. It is symmetric: for any non-null reference values x and y, x.equals(y) should return true if and only if y.equals(x) returns true. It is transitive: for any non-null reference values x, y, and z, if x.equals(y) returns true and y.equals(z) returns true, then x.equals(z) should return true. It is consistent: for any non-null reference values x and y, multiple invocations of x.equals(y) consistently return true or consistently return false, provided no information used in equals comparisons on the objects is modified. For any non-null reference value x, x.equals(null) should return false. Note that it is generally necessary to override the hashCode method whenever this method is overridden, so as to maintain the general contract for the hashCode method, which states that equal objects must have equal hash codes.   注意：这里用了一个修饰词 generally（通常）。也就是说，我们在重写 equals() 之后是可以不重写 hashCode() 的。只是这样会引发一个问题，不重写就违反了 hashCode() 的契约，因为当两个对象 equals 时，它们的哈希值也应该是一样的。这在 HashMap 这一类数据结构中就会引起问题，典型的问题就是内存泄露。\nhashCode() 和 equals() 重写不当造成的内存泄露 我们先来看一段代码：\nclass Key { String name; public Key(String name) { this.name = name; } } class HashMapMemoryLeakDemo { public static void main(String[] args) { Map\u0026lt;Key, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(new Key(\u0026#34;1\u0026#34;), 1); map.put(new Key(\u0026#34;2\u0026#34;), 2); map.put(new Key(\u0026#34;3\u0026#34;), 3); System.out.println(map.get(new Key(\u0026#34;1\u0026#34;))); } } 程序的输出结果是 null。为什么不是 1 呢？因为 hashCode() 是一个 native 方法，它的实现取决于具体的 JVM 实现。一些 JVM 会将 hashCode() 实现为对象在内存中的地址。上面代码在从 map 中取值的时候使用了一个新创建的对象作为 key，所以会返回 null。类似地，我们使用 map.remove(new Key(\u0026quot;1\u0026quot;)) 也无法将最初用 new Key(\u0026quot;1\u0026quot;) 创建的那个对象从 map 中移除。久而久之，就会出现内存泄露。\nhashCode 和 equals() 在 HashMap 中的使用 HashMap 底层的数据结构是一个 table ，定义如下：\ntransient Node\u0026lt;K,V\u0026gt;[] table; 这个 table 就是我们传统意义上的哈希表。有哈希表就有可能出现哈希冲突，HashMap 采用链表（或红黑树）来解决哈希冲突。当链表长度大于 8 并且容量大于 64 时，链表结构会转换成红黑树结构。Node 表示链表中的节点，同时也被红黑树节点 TreeNode 间接继承。我们将table 数组中的元素称为哈希桶，整个 HashMap 的组成结构如下图所示：\n知道这些之后，就可以尝试回答文章开头那个问题了。HashMap 通过哈希函数将 \u0026lt;K, V\u0026gt; 分散到不同的哈希桶中。然而由于哈希冲突的存在，一个哈希桶中可能放有多个节点。在查找元素的时候，首先会通过哈希函数得到一个哈希值，通过哈希值定位到哈希桶。HashMap 中的哈希函数是这样的：\nstatic final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 这里就用到了 key 的 hashCode()。找到哈希桶之后，会接着遍历桶中所有节点，通过 key 的 equals() 来判断当前节点是否就是我们要查找的节点。\n这应该就是“If a.equals(b), then a.hashCode() == b.hashCode(), not vice versa.” 的原因了。到了这里，也不难回答为什么重写 equals() 就要重写 hashCode() 这个问题了。答案的核心就在于哈希表和哈希冲突。\n","href":"/java/java_lang/java-hashcode-equals-contracts/","title":"Java 中的 hashCode() 与 equals()"},{"content":"","href":"/categories/spring/","title":"Spring"},{"content":"","href":"/tags/spring-boot/","title":"Spring Boot"},{"content":"开发的同学可能都遇到过一个 Spring Boot 应用要在多个环境上部署，而每个环境的配置都不同的情况。比如，开发环境用一套配置，测试环境用另一套配置，生产环境又是一套新配置。如果我们把配置放在同一个地方，然后每次都根据不同的环境进行修改，可能要不了多久，我们的头就大了。因为，我们可能一不小心把开发环境的配置放到测试环境中去了……\n为了方便我们在不同环境中运行应用程序，Spring Boot 允许我们将配置信息外部化。Spring Boot 支持多种外部化的配置源，包括 Java 的 properties 文件、YAML 文件、环境变量和命令行参数。\n配置源 既然配置可以来自很多不同的地方，那么就有可能出现同一个配置项在多个配置源中出现的情况。所以 Spring Boot 有一个配置值覆盖规则，优先级高的配置会覆盖优先级低的，优先级从低到高依次为：\n 默认属性（即通过 SpringApplication.setDefaultProperties 设置的属性） @Configuration 类上的 @PropertySource 注解 配置数据（比如 application.properties）。配置数据文件的优先级如下，当.properties 文件与 .yml 文件同时出现时，前者的优先级会高于后者：  Jar 包内的 application.properties 和 application.yml Jar 包内特定 Profile 的 application-{profile}.properties 和 application-{profile}.yml Jar 包外的 application.properties 和 application.yml Jar 包外特定 Profile 的 application-{profile}.properties 和 application-{profile}.yml   由 RandomValuePropertySource 配置的 random.* 属性值 操作系统环境变量 Java 系统属性（System.getProperties()） 来自 java:comp/env 的 JNDI 属性 ServletContext 的初始化参数 ServletConfig 的初始化参数 SPRING_APPLICATION_JSON（环境变量或系统属性中的单行 JSON） 中的属性值 命令行参数 测试上的 properties 属性。在 @SpringBootTest 和其它测试相关注解上有用 测试上的 @TestPropertySource 注解 当 devtools 激活时，$HOME/.config/spring-boot 目录下的 devtools 全局设置属性  命令行参数 默认情况下，SpringApplication 会将所有以 -- 开头的命令行参数转换为 property 并将它们加入到 Spring 的 Environment 中。例如：\n$ java -jar app.jar --name=value 如果你不希望 SpringApplication 将命令行参数添加到 Environment，可以使用 SpringApplication.setAddCommandLineProperties(false) 关闭此功能。\nSPRING_APPLICATION_JSON 环境变量和系统属性通常有一些限制，导致部分属性名无法使用。为了解决这个问题，Spring Boot 允许我们将一组属性放到单行 JSON 中。在 Spring Boot 应用启动时，所有的 spring.application.json 或 SPRING_APPLICATION_JSON 都会被解析并加入到 Environment 中。例如：\n$ java -Dspring.application.json=\u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;value\\\u0026#34;}\u0026#34; -jar app.jar 外部应用属性 在应用启动时，Spring Boot 会自动从下列位置查找和加载 application.properties、application-{profile}.properties、application.yml 和 application-{profile}.yml。其中，高优先级的配置项会覆盖低优先级的配置项，配置覆盖的优先级从低到高依次为：\n classpath:  classpath 根目录 classpath 目录下的 /config 包   当前目录：  当前目录 当前目录的 /config 子目录 /config 子目录的直接子目录    这里的 application.properties 的文件名也不是写死的，Spring Boot 允许我们通过 spring.config.name 这个环境属性定义自己喜欢的配置文件名。例如，以下命名告知 Spring Boot 去寻找 myname.properties 或 myname.yml 文件：\n$ java -jar app.jar --spring.config.name=myname 此外，我们还可以通过 spring.config.location 和 spring.config.additional-location 环境属性 显式 指出配置文件的位置。由于Spring Boot 需要使用 spring.config.name、spring.config.location 和 spring.config.additional-location 来决定加载哪些配置文件，所以它们必须被定义成环境属性（通常是操作系统的环境变量、系统属性或命令行参数）。\n配置随机值 有时候，我们可能希望某个配置项的值是随机生成的。如果每次都生成一个随机值放到配置文件中去，未免太过繁琐。这时，我们可以使用 RandomValuePropertySource，它支持随机产生整数、长整型数、UUID 和字符串。例如：\nmy: number: \u0026#34;${random.int}\u0026#34; bignumber: \u0026#34;${random.long}\u0026#34; uuid: \u0026#34;${random.uuid}\u0026#34; name: \u0026#34;${random.value}\u0026#34; 使用 YAML YAML 是 JSON 的超集，在声明具有层次化的配置数据时非常方便。不过，YAML 文件中的配置在从层级结构转化为扁平结构之后，才能在 Environment 中使用。例如，下面的 YAML 文件：\ndatabase: username: test-user password: test-password 会被扁平化为：\ndatabase.username=test-user database.password=test-password 属性的注入与使用 Spring Boot 给我们提供了多种注入属性的方法，包括 @Value 和 @ConfigurationProperties。\n@Value @Value 一般用于将单个属性注入到程序中。例如：\n@Value(\u0026#34;${server.port}\u0026#34;) int port; @ConfigurationProperties 当配置信息有很多个时，@Value 的局限性就体现出来了。有多少个配置项，就需要写多少个 @Value😣。这时，我们可以使用 @ConfigurationProperties，它支持将多个配置项绑定到 Bean 的字段上。假设我们有一段数据库的配置信息：\ndatabase: url: jdbc:postgresql:/localhost:5432/instance username: test-user password: test-password 我们可以通过 Java Bean 属性绑定的方式，将配置信息绑定到 Bean 的属性上：\n@ConfigurationProperties(prefix=\u0026#34;database\u0026#34;) public class Database { String url; String name; String password; // getters and setters } 也可以通过构造器绑定的方式，将配置信息绑定到 Bean 的属性上：\npublic class Database { String url; String name; String password; public Database(String url, String name, String password) { this.url = url; this.name = name; this.password = password; } // getters } Profiles Spring Boot 允许我们根据不同的环境组织配置文件，让某些配置只在特定的环境下才生效，这就是 Profile。通过 spring.profiles.active 这个环境属性，我们可以决定激活哪些 profile。例如，我们可以将它配置在 application.yml 中，让 dev 和 hsqldb 两个 profile 生效：\nspring: profiles: active: \u0026#34;dev,hsqldb\u0026#34; 也可以通过命令行参数，只让 dev 这个 profile 生效：\n$ java -jar app.jar --spring.profiles.active=dev 还可以通过环境变量等多种方式指定生效的 profile。如果没有指定生效的 profile 文件，就会启用默认的 profile。\n参考资料  Spring Boot Documentation. Externalized Configuration.  ","href":"/java/spring/configurations-in-spring-boot/","title":"Spring Boot 中的配置体系"},{"content":"","href":"/tags/spring/","title":"Spring"},{"content":"Ioc（Inverse of Control），又叫 DI（Dependence Injection）。它是这样一个过程：对象声明自己的依赖，然后容器在创建 Bean 的时候注入这些依赖。如果不使用 IoC，我们在创建对象之前就需要先把对象的依赖创建出来，这个正向的过程会导致对象与对象之间的强耦合。如果反过来，对象不自己创建依赖，而是由 Spring 的 Ioc 容器自动装配，这就是控制反转。逆向的过程使得程序的结构变得更加灵活，没有强耦合，还有利于对象的复用。\nIoc 容器 在 Spring 中，org.springframework.context.ApplicationContext 接口就是 IoC 容器的抽象表示，它负责 Bean 的实例化、配置以及组装。那么，IoC 容器是如何知道要管理哪些 Bean 呢？答案是配置元数据（Configuration Metadata）。配置元数据可以是 XML、注解，还可以是 Java 代码。\n配置元数据 Spring 的 Ioc 容器会读取配置元数据，然后根据配置元数据去实例化、配置和组装应用程序中的 Bean。在 Spring 的早期版本中，配置信息使用的是 XML。Spring 2.5 开始支持基于注解的配置，我们常用的 @Required 和 @Autowired 就是基于注解的配置。 Spring 3.0 开始支持基于 Java 代码的配置。现在，我们甚至可以同时使用 XML 和 注解去配置 Bean。\n实际上，在 IoC 容器内部，这些配置元数据会被解析成 BeanDefinition 对象，BeanDefinition 封装的就是 Bean 的定义和描述信息，比如类名、构造器参数、属性值、作用域、生命周期等，容器会根据 BeanDefinition 中封装的信息来创建 Bean。\n使用 BeanFactoryPostProcessor 对配置元数据进行个性化配置 在 Spring 中，BeanFactory 提供了一种管理任何类型对象的高级机制，我们经常遇到的 ApplicationContext 则是它的一个子接口。若要用一句话来描述二者的差异，那就是：BeanFactory 提供配置框架和基本功能，ApplicationContext 则是添加了更多的企业级功能。\nBeanFactoryPostProcessor 允许我们对配置元数据（BeanDefinition）进行个性化配置。它与后面介绍的 BeanPostProcessor 功能类似，只不过后者的作用是对 Bean 本身进行个性化配置。接口定义如下：\npublic interface BeanFactoryPostProcessor { /** * Modify the application context\u0026#39;s internal bean factory after its standard * initialization. All bean definitions will have been loaded, but no beans * will have been instantiated yet. This allows for overriding or adding * properties even to eager-initializing beans. */ void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; } 在 postProcessBeanFactory() 方法被调用前，所有的 Bean 定义信息都已经加载完毕。所以 BeanFactoryPostProcessor 可以在 Bean 实例化之前获取 Bean 的配置元数据，并对其进行修改。\n依赖注入 依赖注入主要有两种：\n 基于构造器的依赖注入（Constructor-based DI）：容器调用有参构造函数（或静态工厂方法），传入参数列表，每一个参数代表一个依赖。 基于 Setter 的依赖注入（Setter-based DI）：容器在调用无参构造函数或无参静态工厂方法之后，再调用 Bean 的 setter 方法注入依赖。  IoC 容器甚至支持两种注入方式的混用的情况，你可以先通过构造函数给 Bean 注入一部分依赖，再通过 setter 方法给 Bean 注入另一部分依赖。一般情况下，推荐使用构造器注入强制性依赖，使用 setter 方法注入可选的依赖。\nBean 的依赖项解析过程如下：\n 容器被创建出来，通过描述 Bean 的配置元数据进行初始化，检验每个 Bean 的配置信息。配置元数据可以是 XML，也可以是注解和 Java 代码 对于每一个 Bean，它依赖会被表示成属性、构造器参数或静态工厂方法的参数。这些依赖将会在 Bean 被真正创建的时提供给它 每个属性或构造器参数都是一个具体的值，或者指向容器内另一个 Bean 的引用 对于那些是具体值的属性或构造器参数，相应的值会被转化成属性或构造器参数的实际类型  自动装配 Spring 提供了四种自动装配模式：\n   Mode Description     no 不使用自动装配，通过 ref 定义 Bean 的依赖。这是默认值   byName 通过属性名装配。Spring 会查找与属性同名的 Bean 进行装配   byType 当满足给定类型的 Bean 只有一个时，进行自动装配，有多个时抛出异常。若没有满足类型的 Bean，不进行装配。   constructor 与 byType 类似，只是 Bean 用作构造器参数    Bean 在 Spring 中，Bean 就是那些由 IoC 容器初始化、配置和组装的对象，IoC 容器就是 Bean 的管理者。我们在前面提到，开发者通过配置元数据告诉 IoC 容器 Bean 的相关配置信息，容器将配置元数据解析成 BeanDefinition 对象，再通过它创建 Bean。\nBean 的命名 通常情况下，一个 Bean 只有一个标识符（identifier）。但是，如果 Bean 需要多个标识符，也是可以的，这些多的标识符就是 Bean 的别名（alias）。但是，在同一个 IoC 容器中，是不允许有多个相同的标识符的。\n那么，如何指定 Bean 的标识符呢？在基于 XML 的配置里，Bean 的标识符由 id 和 name 属性指定。其中 id 只允许我们声明一个标识符，而 name 允许我们声明多个，多个标识符之间使用逗号（,）或分号（;）或空格分隔就行了。例如：\u0026lt;bean id=\u0026quot;xx\u0026quot; name=\u0026quot;xx1,xx2\u0026quot;/\u0026gt;有时候，在 name 中声明所有的别名可能并不合适，这时可以使用 \u0026lt;alias/\u0026gt; 标签来单独明别名。例如：\u0026lt;alias name=\u0026quot;fromName\u0026quot; alias=\u0026quot;toName\u0026quot;/\u0026gt;。如果我们没有在 \u0026lt;bean/\u0026gt; 中定义 id 或 name 的值也是可以的，这是容器或自动帮我们生成一个 Bean 的标识符。\n当我们使用像 @Bean 这种基于 Java 的配置创建 Bean 时，Bean 的标识符默认为方法的名字，例如下面代码中创建的 Bean 的标识符就是 transferService：\n@Configuration public class AppConfig { @Bean public TransferServiceImpl transferService() { return new TransferServiceImpl(); } } depends-on 有时候，我们可能是希望某个 Bean 在另一个或多个 Bean 之前初始化，这时我们可以使用 depends-on。例如：\u0026lt;bean id=\u0026quot;beanOne\u0026quot; depends-on=\u0026quot;beanTwo,beanThree\u0026quot;/\u0026gt; 告知容器在初始化 beanOne 之前先初始化 beanTwo 和 beanThree。\n延迟初始化 Bean ApplicationContext 默认会在启动的时候初始化所有的单例 Bean。虽然这种饿汉式的初始化方式能够尽早将问题暴露出来，但是有时候我们希望 Bean 在第一次被使用时才创建，即延迟初始化。为了实现这个目标，我们可以使用 lazy-init 属性。例如，\u0026lt;bean id=\u0026quot;beanOne\u0026quot; lazy-init=\u0026quot;true\u0026quot;/\u0026gt; 告诉容器不要在启动时创建 beanOne，而是在 beanOne 第一次被请求时才创建它。\nBean 的作用域 我们前面有说，IoC 容器会使用 BeanDefinition 中封装的描述信息来创建 Bean。Bean 的作用域（Scope）描述的是：在不同场景下， BeanDefinition 和通过它创建出来的 Bean 的数量关系。在Spring 中，Bean 的作用域是相对于 IoC 容器来说的。Spring 默认支持了六种作用域：\n   Scope Description     singleton 通过单个 BeanDefinition 创建的 Bean 在容器内只有一个实例。这也是默认的作用域   prototype 通过单个 BeanDefinition 可以创建出任意数量的 Bean 实例   request 在每个 HTTP 请求的生命周期内，通过单个 BeanDefinition 创建的实例只有一个   session 在每个 HTTP 会话的生命周期内，通过单个 BeanDefinition 创建的实例只有一个   application 在每个 ServletContext 的生命周期内，通过单个 BeanDefinition 创建的实例只有一个   websocket 在每个 WebSocket 的生命周期内，通过单个 BeanDefinition 创建的实例只有一个    此外，Spring 还支持开发者创建自定义的 Bean 作用域，具体做法可以参考 Custom Scopes。\nBean 的生命周期 Bean 本质上是一个 Java 对象。我们先回顾下 Java 对象的生命周期，JVM 中的对象一生大概会经历以下几个阶段：\n站在 JVM 的角度来看，Bean 的生命周期也就是这样。但是，由于 Bean 是 IoC 容器管理的，容器可能给它定义额外的生命周期，也就是添加新的阶段。所以我们需要从 IoC 容器的角度看 Bean 的生命周期。\nIoC 容器中 Bean 的生命周期主要有 4 个大的阶段：\n 实例化（Instantiation） 初始化（Initialization） 使用（In Use） 销毁（Destruction）  这和 Java 对象的生命周期有点不一样。但如果我们将 Bean 的生命周期看成创建、使用和销毁这三个大的阶段是不是就好理解了一些呢。Bean 的使用就不用说了，销毁也只是在容器关闭前（详见 ConfigurableApplicationContext#close）调用一下，复杂的都在 Bean 的创建过程。所以下面我们重点来看 Bean 的创建过程。\nBean 的创建 先声明以下，这里说的 Bean 的创建并不只是说把 Bean 实例化出来就行了，而是指实例化和初始化两个步骤。普通非延迟初始化的 Bean 创建流程的入口其实在 org.springframework.context.support.AbstractApplicationContext#refresh() 方法中的 finishBeanFactoryInitialization(beanFactory)，感兴趣的同学可以打开源码一探究竟。一番探索之后，发现最终的 Bean 创建逻辑位于 org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#doCreateBean 方法。doCreateBean() 的方法体还是不短，为了抓住主线，先贴出主干代码：\nprotected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { // Instantiate the bean. \tBeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); } Object bean = instanceWrapper.getWrappedInstance(); // ...  // Allow post-processors to modify the merged bean definition. \tsynchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { // ... \t} mbd.postProcessed = true; } } // Eagerly cache singletons to be able to resolve circular references \t// even when triggered by lifecycle interfaces like BeanFactoryAware. \tboolean earlySingletonExposure = (mbd.isSingleton() \u0026amp;\u0026amp; this.allowCircularReferences \u0026amp;\u0026amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { // ... \taddSingletonFactory(beanName, () -\u0026gt; getEarlyBeanReference(beanName, mbd, bean)); } // Initialize the bean instance. \tObject exposedObject = bean; try { populateBean(beanName, mbd, instanceWrapper); exposedObject = initializeBean(beanName, exposedObject, mbd); } catch (Throwable ex) { // ... \t} // Register bean as disposable. \ttry { registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { // ... \t} return exposedObject; } 这代码就清爽多了。所以 Bean 的创建步骤主要是以下几步：\nBeanPostProcessor BeanPostProcessor 允许我们对 Bean 进行个性化定制处理。该接口有两个方法：\n postProcessBeforeInitialization(Object bean, String beanName)：在 Bean 初始化之前调用 postProcessAfterInitialization(Object bean, String beanName)：在 Bean 初始化之后调用  BeanPostProcessor 有一系列的子接口，比如 InstantiationAwareBeanPostProcessor 、DestructionAwareBeanPostProcessor、BeanFactoryPostProcessor、ApplicationContextAwareProcessor 等。BeanPostProcessor 系列接口中的回调是容器级别的，对所有的 Bean 生效。不同子接口发挥作用的时间不同，后面会进行介绍。\n实例化 Bean 创建 Bean 的第一步是实例化，入口方法是 AbstractAutowireCapableBeanFactory#createBeanInstance。容器会根据配置元数据，采取合适的策略（工厂方法或构造器）实例化 Bean。\nInstantiationAwareBeanPostProcessor 提供了三个非常重要的方法：\n postProcessBeforeInstantiation(Class\u0026lt;?\u0026gt; beanClass, String beanName)：在实例化 Bean 之前调用 postProcessAfterInstantiation(Object bean, String beanName)：在实例化 Bean 之后、正式填充属性之前调用 postProcessProperties(PropertyValues pvs, Object bean, String beanName)：在填充属性之前对属性值作处理  加入这三个方法之后的 Bean 创建过程如下：\n初始化 Bean 初始化 Bean 的入口方法是 AbstractAutowireCapableBeanFactory#initializeBean，主干代码如下：\nprotected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) { invokeAwareMethods(beanName, bean); Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { // ... \t} if (mbd == null || !mbd.isSynthetic()) { wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } 从上到下的逻辑主要是：调用 Aware 类型的回调方法，BeanPostProcessor 初始化前置处理，调用初始化方法，调用 BeanPostProcessor 初始化后置处理。我们一个一个来看。\nBean 初始化 Aware Aware 类型的接口众多，但是在 Bean 的初始化过程中只可能会调用 BeanNameAware、BeanClassLoaderAware 和 BeanFactoryAware 三个接口的回调方法。具体逻辑可以参考AbstractAutowireCapableBeanFactory#initializeBean 方法调用的 invokeAwareMethods(beanName, bean) 方法。方法不长，我直接贴出来：\nprivate void invokeAwareMethods(String beanName, Object bean) { if (bean instanceof Aware) { if (bean instanceof BeanNameAware) { ((BeanNameAware) bean).setBeanName(beanName); } if (bean instanceof BeanClassLoaderAware) { ClassLoader bcl = getBeanClassLoader(); if (bcl != null) { ((BeanClassLoaderAware) bean).setBeanClassLoader(bcl); } } if (bean instanceof BeanFactoryAware) { ((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this); } } } 正式初始化 org.springframework.beans.factory.InitializingBean 允许 Bean 在所有属性填充完毕之后执行初始化操作，接口定义如下：\npublic interface InitializingBean { /** * Invoked by the containing BeanFactory after it has set all bean properties * and satisfied BeanFactoryAware, ApplicationContextAware etc. */ void afterPropertiesSet() throws Exception; } afterPropertiesSet() 方法会在 Bean 的属性设置完毕、各种 Aware 调用完成之后执行。实际上，Spring 并不推荐我们使用这个接口来进行初始化操作，因为这会导致我们的代码与 Spring 框架强耦合。推荐的做法是使用自定义的 init() 方法或 JSR-250 定义的 @PostConstruct 注解。\ninvokeInitMethods() 方法会尝试先判断 Bean 是否实现 InitializingBean 接口。若是，则调用 Bean 的 afterPropertiesSet() 方法。然后，判断 Bean 是否有自定义的初始化方法。若有，则调用自定义的 init() 方法。你可能已经发现了，这里没有调用 @PostConstruct 注解的方法，因为它是在 #BeanPostProcessor#postProcessBeforeInitialization 中调用的，感兴趣的同学可以查看 InitDestroyAnnotationBeanPostProcessor#postProcessBeforeInitialization 的源码。invokeInitMethods() 的主干代码如下：\nprotected void invokeInitMethods(String beanName, Object bean, @Nullable RootBeanDefinition mbd) throws Throwable { boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean \u0026amp;\u0026amp; (mbd == null || !mbd.isExternallyManagedInitMethod(\u0026#34;afterPropertiesSet\u0026#34;))) { // ...  ((InitializingBean) bean).afterPropertiesSet(); // ... \t} if (mbd != null \u0026amp;\u0026amp; bean.getClass() != NullBean.class) { String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) \u0026amp;\u0026amp; !(isInitializingBean \u0026amp;\u0026amp; \u0026#34;afterPropertiesSet\u0026#34;.equals(initMethodName)) \u0026amp;\u0026amp; !mbd.isExternallyManagedInitMethod(initMethodName)) { invokeCustomInitMethod(beanName, bean, mbd); } } } 到这里，Bean 的初始化过程基本就分析完了，我们来完善一下之前的图：\n销毁 Bean 跟 InitializingBean 类似，Spring 为我们提供了 DisposableBean 用于容器关闭前执行 Bean 的清理工作。Spring 并不推荐我们使用它，因为这会导致程序代码与 Spring 强耦合。推荐的做法是使用 JSR-250 定义的 @PreDestroy 注解或配置自定义的 destroy() 方法。如果同时使用了几种，那么方法的执行顺序是：\n @PreDestroy 注解的方法 DisposableBean 的 destory() 方法 配置的自定义 destroy() 方法  到这里，Bean 的生命周期基本就介绍完了。用一张图总结下：\n其它 Aware 接口 除了之前提到过的 Aware 接口，Spring 还给我们提供了很多其它的 Aware 接口。这些接口告诉 IoC 容器 Bean 需要对应的依赖。例如 ApplicationContextAware 允许 Bean 获得一个 ApplicationContext 的引用，ResourceLoaderAware 允许 Bean 获得一个 ResourceLoader 的引用。\n一个例子 纸上得来终觉浅，绝知此事要躬行。下面，让我们通过一个例子加深对 Bean 生命周期的理解。\n先定义一个 Bean 类，它实现了众多的接口，包含自定义的初始化方法和自定义的销毁方法，以及一个普通方法 hello()：\npublic class MySpringBean implements ApplicationContextAware, BeanNameAware, BeanClassLoaderAware, BeanFactoryAware, InitializingBean, DisposableBean { @Override public void setBeanClassLoader(ClassLoader classLoader) { System.out.println(\u0026#34;Executing setBeanClassLoader...\u0026#34;); } @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { System.out.println(\u0026#34;Executing setBeanFactory...\u0026#34;); } @Override public void setBeanName(String s) { System.out.println(\u0026#34;Executing setBeanName...\u0026#34;); } @Override public void destroy() throws Exception { System.out.println(\u0026#34;Executing destroy...\u0026#34;); } @Override public void afterPropertiesSet() throws Exception { System.out.println(\u0026#34;Executing afterPropertiesSet...\u0026#34;); } @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { System.out.println(\u0026#34;Executing setApplicationContext...\u0026#34;); } @PostConstruct public void postConstruct() { System.out.println(\u0026#34;Executing @PostConstruct...\u0026#34;); } @PreDestroy public void preDestroy() { System.out.println(\u0026#34;Executing @PreDestroy...\u0026#34;); } public void customInitMethod() { System.out.println(\u0026#34;Executing custom init method...\u0026#34;); } public void customDestroyMethod() { System.out.println(\u0026#34;Executing custom destroy method...\u0026#34;); } public void hello() { System.out.println(\u0026#34;Hello, Spring!\u0026#34;); } } 然后定义一个 BeanPostProcessor 的实现类：\npublic class MyBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(\u0026#34;Executing postProcessBeforeInitialization of \u0026#34; + beanName + \u0026#34; ...\u0026#34;); return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(\u0026#34;Executing postProcessAfterInitialization of \u0026#34; + beanName + \u0026#34; ...\u0026#34;); return bean; } } 再定义一个 InstantiationAwareBeanPostProcessor 的实现类：\npublic class MyInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor { @Override public Object postProcessBeforeInstantiation(Class\u0026lt;?\u0026gt; beanClass, String beanName) throws BeansException { System.out.println(\u0026#34;Executing postProcessBeforeInstantiation of \u0026#34; + beanName + \u0026#34; ...\u0026#34;); return null; } @Override public boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException { System.out.println(\u0026#34;Executing postProcessAfterInstantiation of \u0026#34; + beanName + \u0026#34; ...\u0026#34;); return true; } @Override public PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) throws BeansException { System.out.println(\u0026#34;Executing postProcessProperties of \u0026#34; + beanName + \u0026#34; ...\u0026#34;); return pvs; } } 以及一个配置类：\n@Configuration public class MyLifeCycleConfiguration { @Bean public MyBeanPostProcessor myBeanPostProcessor() { return new MyBeanPostProcessor(); } @Bean public MyInstantiationAwareBeanPostProcessor myInstantiationAwareBeanPostProcessor() { return new MyInstantiationAwareBeanPostProcessor(); } @Bean(initMethod = \u0026#34;customInitMethod\u0026#34;, destroyMethod = \u0026#34;customDestroyMethod\u0026#34;) public MySpringBean mySpringBean() { return new MySpringBean(); } } 最后写一个测试：\n@SpringBootTest class BeanLifecycleApplicationTests { @Autowired private MySpringBean mySpringBean; @Test void testMySpringBeanLifecycle() { mySpringBean.hello(); } } 现在运行测试，各个生命周期回调的执行步骤就一览无余啦。由于容器默认还会创建其它 Bean，数量还不少，影响我们观察。所以下面执行结果中只列出了与 MySpringBean 相关的执行结果：\nExecuting postProcessBeforeInstantiation of mySpringBean ... Executing postProcessAfterInstantiation of mySpringBean ... Executing postProcessProperties of mySpringBean ... Executing setBeanName... Executing setBeanClassLoader... Executing setBeanFactory... Executing setApplicationContext... Executing postProcessBeforeInitialization of mySpringBean ... Executing @PostConstruct... Executing afterPropertiesSet... Executing custom init method... Executing postProcessAfterInitialization of mySpringBean ... Hello, Spring! Executing @PreDestroy... Executing destroy... Executing custom destroy method... 完整代码已经放到 Github。\n未完待续 时间有限，先写到这里。写东西从来都不是一次性过程，要常看，常思考，常更新。\n参考资料  Spring Framework Documentation. 一文读懂 Spring Bean 的生命周期.  ","href":"/java/spring/spring-ioc/","title":"Spring Ioc"},{"content":"","href":"/tags/redis/","title":"Redis"},{"content":"","href":"/categories/redis/","title":"Redis"},{"content":"作为一个内存数据库，Redis 的容量肯定是有限的。如果 Redis 允许我们不断写入数据而又不作任何清理工作的话，内存迟早要被耗尽。此外，若我们在 Redis 中创建了一个不带过期时间的 key，这个 key 即使不被使用，它也会在我们使用 DEL 命令删除它之前一直存在于 Redis 中，这可不是一件好事儿。所以，为 key 设置一个过期时间还是很有必要的。那么，Redis 是如何让 key 过期的呢，它又是如何淘汰过期的 key 的呢？这就是本文要将的主要内容。\nkey 过期 我们可以在创建 key 时指定它的存活时间，也可以用 EXPIRE key seconds [NX|XX|GT|LT] 命令给 key 设置存活时间。当 key 过期时，它会被自动删除。\nRedis 使用 volatile 这个术语来描述带有过期时间的 key，我们会在 key 的淘汰策略中再次与这个术语见面。对于不带过期时间的 key，Redis 也有一个专门的术语，那就是 persistent。\n相关命令 Redis 支持不少与 key 过期有关的命令，包括为 key 设置过期时间、查看 key 还有多久过期和移除 key 的过期时间。\n为 key 设置过期时间 相关命令有：\n EXPIRE：设置 key 的存活时间，单位为秒 PEXPIRE：设置 key 的存活时间，单位为毫秒 EXPIREAT：设置 key 过期的 UNIX 时间戳，单位为秒 PEXPIREAT：设置 key 过期的 UNIX 时间戳，单位为毫秒  EXPIRE 和 PEXPIRE 设置的是 key 的存活时间（time to live），key 的存活时间会随着时间的流逝而不断减少，当值为 0 时，Redis 就会移除这个 key。EXPIREAT 和 PEXPIRE 设置的是 key 的过期时间（expire time），当系统的当前 UNIX 时间超过设置的时间后，key 就会被删除。\n查看 key 还有多久过期 相关命令有：\n TTL：查看 key 的剩余存活时间，单位为秒 PTTL：查看 key 的剩余存活时间，单位为毫秒  移除 key 的过期时间 PERSIST 命令可以移除 key 的过期时间，将 key 从 volatile 变成 persistent。\n具体做法 Redis 中的 key 有两种过期方式：被动过期和主动过期。\nRedis 使用绝对的毫秒级 UNIX 时间戳来存储 key 的过期时间。任何时候，当某个 key 被访问时，如果系统的当前 UNIX 时间超过了它的过期时间，这个 key 就会被动过期。但是，有些 key 可能永远也不会被访问，而它们也需要被过期，这就需要用到主动过期。\nRedis 会周期性地从带过期时间的 key 中随机选取一部分进行检测，然后删除其中已经过期的那些 key。实际上，Redis 每秒都会进行十次下面的操作：\n 从所有带有过期时间的 key 中随机选取 20 个 删除其中过期的那些 key 如果过期的 key 超过 5 个（过期占比大于 25%），继续进入步骤 1  key 的淘汰机制 当把 Redis 作为缓存使用时，我们通常会是希望 Redis 能够在我们添加新数据时自动淘汰旧的数据。当然，作为一个优秀的缓存组件，Redis 早已考虑到了这些，它还给我们提供了多种不同的淘汰策略。redis.conf 中有一个名为 maxmemory 的配置项，用来指定 Redis 可使用的内存阈值。当 key 过期，或者 Redis 在执行新命令时发现实际内存占用超过了 maxmemory 设置的阈值之后，Redis 就会根据maxmemory-policy 配置的淘汰策略淘汰过期或者不活跃的 key，回收内存，供新的 key 使用。maxmemory=0 表示不对内存做限制，可以不断存入数据，Redis 被用作内存数据库，适合数据量较小的业务。如果数据量很大，就需要给 maxmemory 设置一个合适的值，将 Redis 作为缓存使用。\n淘汰策略 Redis 提供了八种淘汰策略：\n   策略（Policy） 描述（Description）     noeviction 在插入新数据时，若内存超过限制，则返回错误，不淘汰任何 key   allkeys-lru 采用 LRU 算法对所有 key 进行淘汰   allkeys-lfu 采用 LRU 算法对所有 key 进行淘汰   allkeys-random 采用随机算法对所有 key 进行淘汰   volatile-lru 采用 LRU 算法对带过期时间的 key 进行淘汰   volatile-lfu 采用 LFU 算法对带过期时间的 key 进行淘汰   volatile-random 采用随机算法对带过期时间的 key 进行淘汰   volatile-ttl 淘汰带过期时间的 key 中存活时间最短的 key    八种淘汰策略各有不同的适用场景：noeviction 是 Redis 的默认 key 淘汰机制，适用于数据量不太大的场景，因为此时 Redis 被当成了内存数据库使用；随机策略适用于 key 没有明显热点的场景；LRU 淘汰策略适用于数据有冷热读写区分的场景；LFU 也适用于数据有冷热读写区分的场景，且越热的数据的访问频率越高。\n需要注意的是：Redis 为了节省内存，并没有实现完整的 LRU 算法和 LFU 算法，而是采用了近似实现。\n参考资料  Redis Command Reference. Redis expires. Using Redis as an LRU cache.  ","href":"/posts/redis/key_expiration_and_eviction_in_redis/","title":"Redis 中的 key 过期与淘汰机制"},{"content":"","href":"/tags/english/","title":"English"},{"content":"","href":"/categories/english/","title":"English"},{"content":" 相信大家都知道，我们在英语中一般用 AM（或 A.M.）表示上午，用 PM（或 P.M.）表示下午。当然，这一点我也知道。不仅如此，笔者还天真的认为 12:00 PM 表示下午十二点，即午夜。今天笔者去某公司面试，做了一道题，其中就需要用到这个 12:00 PM。根据题目要求，我总感觉把 12:00 PM 当成午夜有点怪怪的，于是我打开了我的 Chrome 插件沙拉查词，确认了 PM 的确表示下午，所以我还是把它当成午夜处理了。直到后面，我在给面试官讲我的解题思路时，面试官和蔼的告诉我 12:00 PM 表示中午十二点，我差点当场去世……😶。不过笔者当时的内心仍然表示怀疑，但是这个中午十二点就让题目里面的其它条件有了意义，所以就暂时认为面试官说的是对的。熬过几轮面试，笔者的思维早已被带向了天南海北，回到家后，突然想起了这件尴尬的事情，遂打开 Google，开始探索真知……\n 就连小学生都知道，一天有 24 个小时。但由于历史和文化的差异，不同地方使用了不同的事件表示方法。\n在中国古代，人们使用“铜壶滴漏”计算时间，把一昼夜分为十二时辰，即子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥，这十二时辰刚好对应于今天的二十四小时。半夜十一点到一点为子时，一点到三点为丑时，其余时辰以此类推。\n而在其它的一些国家，比如美国和加拿大，则使用带 AM 和 PM 的十二小时制来表示时间。那么 AM 和 PM 到底是什么含义呢？午夜又应该用哪个表示呢？\n十二小时制 十二小时制将一天的二十四小时分成两段长为十二小时的区间。前一段从午夜到正午，用 AM 表示；后一段从正午到午夜，用 PM 表示。\nAM = A.M. = Ante Meridiem: before noon\nPM = P.M. = Post Meridiem: after noon\n我们使用数字 1-12 加上 AM 或 PM 就可以表示一天中的二十四个小时了。例如，1AM 是午夜后的一个小时，而 11PM 是午夜前的一个小时。\n午夜与正午 在十二小时制中，中午和午夜有点尴尬。因为从逻辑上来讲：午夜既不能被认为是正午前，也不能被认为是正午后，因为不管是从前还是后来计算，午夜与正午之间的时间差值都是十二个小时。正午就是正午，不在正午前，也不在正午后。\n从绝大多数资料来看，一般用 12:00 AM 表示午夜（二十四小时制中的 0:00），用 12:00 PM 表示正午（二十四小时中的 12:00）。\n其实 12:00 AM 表示的午夜也很容易给人造成困扰，因为一天以午夜开始，又以午夜结束。有一个简单的方法，就是降低时间的准确度来消除疑惑。我们可以牺牲掉一分钟，使用 11:59 AM 来表明我们期望的时间，这就没有歧义了。\n参考资料  百度百科. 十二时辰制. Wikipedia. 12-hour clock.  ","href":"/posts/english/am_and_pm/","title":"英语中的时间表达：AM 与 PM"},{"content":"我们常说事务是一组操作，这组操作要么全部成功，要么全部不成功。当事务失败时，事务内所作的所有变更都会被回滚。在单机环境中，若多个线程同时对数据进行操作，事务就是对数据的完整性的保障。类似地，在分布式环境中处理数据变更的时候，需要通过分布式事务来保证数据的正确完整，防止数据变更请求在部分节点上的执行失败而导致的数据不一致问题。\n为了实现分布式事务，人们开发出了很多经典的分布式一致性算法，例如 2PC、3PC、TCC 等。\n2PC 2PC 是两阶段提交（Two-phase Commit）的简称，它是一种实现分布式事务的算法。顾名思义，2PC 中事务的提交过程分两个阶段来完成：准备（Prepare）阶段和提交（Commit）阶段。\n准备阶段：协调者向所有参与者发送事务内容，询问其是否可以提交事务，然后等待所有参与者的答复。参与者执行事务（但不提交），若执行成功，则给协调者反馈 YES，若执行失败，则给协调者反馈 NO。\n提交阶段：当协调者收到所有参与者的反馈信息后，会对信息进行统计。只有当所有的参与者都反馈 YES 时，协调者才会给所有的参与者发送提交事务的命令。否则，协调者会给所有的参与者发送 abort 请求，回滚事务。\n虽然 2PC 可以有效保证分布式环境中的事务，但算法本身也存在不少缺陷：\n 性能问题。在算法的执行过程中，所有的参与者都处于阻塞状态。只有在协调者通知参与者提交或回滚，参与者在本地执行完相应的操作之后，资源才会被释放。 协调者单点问题。若协调者发生故障，参与者收不到提交或回滚的通知，就会一直处于锁定状态。 消息丢失导致的数据不一致问题。在提交阶段，若系统出现分区，部分参与者没有收到提交消息，各节点的数据就会变得不一致。  3PC 由于 2PC 存在各种问题，人们对它进行了改进，衍生出了新的协议。三阶段提交（Three-Phase Commit, 3PC）就是 2PC 的改进版本，它将事务的提交过程分为了 CanCommit、PreCommit 和 DoCommit 三个阶段。\nCanCommit：协调者向所有参与者发送包含事务内容的 CanCommit 请求，询问其是否可以提交事务，然后等待所有参与者的答复。参与者收到请求后，判断自己能够执行事务。若参与者认为自己可以提交，则反馈 YES ，否则反馈 NO。\nPreCommit：协调者接收所有参与者的反馈消息，根据反馈消息决定是否中断事务。当所有参与者都反馈 YES 时，协调者向所有参与者发出 PreCommit 请求，参与者收到请求后执行事务（但不提交），执行成功后向协调者反馈 Ack 表示已经准备好提交事务，否则反馈 NO。若有参与者在 CanCommit 阶段返回 NO 或协调者在超时之前未收到任何反馈，协调者就会向所有的参与者发出 abort 请求，请求中断事务。\nDoCommit：如果 PreCommit 阶段中的所有参与者都反馈 Ack，协调者就会给所有的参与者发送 DoCommit 请求，参与者收到之后会进行真正的事务提交。反之，如果 PreCommit 阶段有一个参与者反馈 NO 或者协调者在超时之前没有收到反馈，则会向所有的参与者发送 abort 请求，中断事务。\n需要注意的是：只要进入到了 DoCommit 阶段，无论协调者出现故障，还是协调者与参与者之间的通信出现问题，都会导致参与者无法收到协调者发出的 DoCommit 请求或 abort 请求。这是，参与者会在等待超时之后，主动提交事务。\n与 2PC 相比，3PC 通过超时机制降低了阻塞范围并解决了协调者单点问题，但还是没有完全解决数据不一致的问题。\n参考资料  Wikipedia. Two-phase commit protocol. Wikipedia. Three-phase commit protocol.  ","href":"/distributed_computing/distributed_transactions/","title":"分布式事务"},{"content":"","href":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/","title":"分布式计算"},{"content":"","href":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/","title":"分布式计算"},{"content":"CAP 定理 计算机科学家 Eric Brewer 指出：一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）这三项中的两项。这就是 CAP 定理，也叫 Brewer 定理。CAP 定理是分布式系统发展的理论基石，对分布式系统的发展有着广泛而深远的影响。\n一致性 是指所有节点在同一时刻的数据都是相同的，所有节点都拥有最新版本的数据。也就是说，在同一时刻，不管访问系统中的哪个节点，所得到的数据不仅是一样的，而且是最新的。\n可用性 是指系统总能对客户端的请求给予响应。系统并不保证响应包含的数据一定是最新的，数据可能是老旧的甚至错误的，但响应是一定会有的。从客户端来看，它发出去的请求总是有响应的，不会出现整个系统无法连接、超时或无响应的情况。\n分区容错性 是指当系统中的部分节点出现消息丢失或分区故障时，系统仍然能够继续对外提供服务。在分布式环境中，每个服务节点都是不可靠的。当某些节点发生故障，或节点与节点之间的通信出现异常时，系统就出现了分区问题。从系统本身来看，当分区问题出现时，它仍然要对外提供稳定服务。而对于客户端而言，系统的分区问题对它来说是透明的，客户端并不会感受到系统的分区问题。\nCAP 定理的证明 根据 CAP 定理，分布式系统中的一致性、可用性和分区容错性不可兼得，最多只能满足其中两点。下面用只有两个服务器的情况来简单证明一下。假定我们有一个分布式系统，它由 node1 和 node2 两个节点组成。在最开始的时候，两个节点之间的数据 X 的值相同，都是 $v_0$。此时，不管用户是访问 node1 还是 node2，所得 X 的值都是 $v_0$。\n在正常的情况下，node1 和 node2 此时都在正常工作，相互之间通信良好。某一时刻，用户向 node1 执行了更新操作，node1 中数据 X 的值被修改成了 $v_1$。此时，node1 会发送一个消息 M 给 node2，告知 node2 将 X 的值修改为 $v_1$。node2 在收到消息 M 之后， X 的值也会被修改成 $v_1$。此后，用户不管是请求 node1 还是 node2，所得 X 的值都是 $v_1$。\n如果网络出现了分区，node1 与 node2 之间无法正常进行通信，消息 M 无法抵达 node2，那么 node1 和 node2 的数据就会出现不一致。\n此时，若用户请求 node2，由于 node1 与 node2 通信故障，node2 无法给用户返回正确的结果 $v_1$，此时有两种处理方案：\n 牺牲一致性，node2 向用户返回老数据 $v_0$ 牺牲可用性，node2 阻塞用户请求，直到完成数据同步  到这里，CAP 定理的就证明完了。分析过程说明，当系统出现分区时，一致性与可用性只能二选一。\nCAP 定理的权衡 根据 CAP 定理，分布式系统无法同时满足一致性、可用性和分区容错性。那么在构建分布式系统时，我们应该如何取舍呢？\nCA 这种方案不支持分区容错，也就是说节点和网络一直会处于理想状态，不会发生故障。但是，分布式系统中的故障是客观存在的，系统中的节点越多，出错的概率就越大。所以 CA 是不切实际的。\nCP 这种方案放弃了可用性，在系统出现分区时，服务会一直阻塞，直到数据达成一致。在此期间，系统对外是不可用的。CP 适合对数据一致性比较敏感的业务场景。我们常用的 Zookeeper 就是优先保证 CP。\nAP 牺牲一致性，换取可用性。当系统出现分区时，用户的请求依然可以得到响应，只是数据可能是老旧的数据。整个系统依然对外提供稳定服务，用户体验会好于 CP。\nCAP 的问题和误区 虽然 CAP 定理极大地促进了分布式系统的发展，但是人们在分布式系统演进的过程中发现，CAP 过于理想化，存在不少问题和误区。\n由于网络分区问题肯定存在，很多人在设计分布式系统时就局限在了 CP 和 AP 的二选一当中。但实际上，分布式系统中的分区问题出现的概率并不高。在没有出现分区问题时，不应该只选择一致性或可用性，而应该同时提供。Brewer 本人也在 CAP 定理提出的 12 年（2012 年）发文 指出人们在使用 CAP 时存在的误区。\n此外，在同一个系统中，不同业务在分区出现时对一致性和可用性的选择取可能是不同的。所以，在设计分布式系统时，要根据实际的业务场景灵活变通。CAP 作为指导分布式系统设计的理论，告诉我们的是实际设计时要考虑的因素，而不是让我们进行绝对的选择。\nBASE 理论 在设计分布式系统时，我们往往需要在一致性和可用性之间权衡，并不一定是二选一。CAP 定理中的一致性强调的是强一致性，但在实际运用中，我们也可能选择弱一致性或最终一致性。这就引出了 BASE 理论，它是 CAP 的延伸。\nBASE 是 Basically Available（基本可用）、Soft State（软状态）和 Eventually Consistent（最终一致）的简写。它的基本思想是：权衡分布式系统的各个功能，尽量保证系统的稳定可用，即便在出现局部故障和异常时，也确保系统的主体功能可用，确保系统的最终一致性。\n基本可用：当分布式系统出现故障时，允许损失部分可用性，保证核心功能可用。\n软状态：允许系统出现中间状态。当系统出现分区时，虽然各分区的数据处于不一致状态，但这并不影响系统对外提供服务。\n最终一致性：分布式系统不需要实时保持强一致状态，当系统发生故障时，数据的不一致是可以容忍的。但在故障恢复后，要进行数据的同步，最终达到一致性状态。\n参考资料  Wikipedia. CAP theorem. Brewer\u0026rsquo;s CAP Theorem. Eric Brewer. CAP Twelve Years Later: How the \u0026ldquo;Rules\u0026rdquo; Have Changed.  ","href":"/distributed_computing/cap_theorem_and_base/","title":"CAP 定理与 BASE 理论"},{"content":"","href":"/categories/db/","title":"DB"},{"content":"","href":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/","title":"数据库"},{"content":"在关系数据库设计中，数据库规范化（Database normalization） 是一个非常重要的概念。一般而言，关系数据库设计的目标是生成一组关系模式，使我们存储数据时避免不必要的冗余，同时让我们可以方便地获取数据。规范化就是组织数据库内数据的一个过程，包括创建数据表和建立这些表之间的关系。而在建立表间关系时，我们既要保护数据的完整性，又要消除冗余数据。冗余数据不仅会浪费磁盘空间，还会增加我们维护的成本。\n相关术语 关系数据库设计中涉及很多专业术语，在进入到主要内容之前，我想先介绍下这些术语，确保我们能够在术语的使用上达成共识。\n 数据库表（Table）：在关系数据库中，数据库表是一个逻辑概念，代表以表格的形式进行组织的一组相关数据构成的集合。它由行和列组成。 行（Row）：数据库表中的每一行（或每一条 记录（Record））都表示一组相关数据，表中的所有行的结构都是相同的。行又称 元组（Tuple）。 列（Column）：在关系数据库中，列是一组特定类型数据构成的集合，集合中的每一项数据（字段）都分散在每一行中。如果一个表有 N 列，那么表中的每一条记录就会有 N 个字段。列又称 属性（Attribute）。  举个例子，我们有一个学生表 student(stu_id, name, gender)，在下面的 SQL 实现中：\nCREATE TABLE student( stu_id INTEGER NOT NULL, name VARCHAR(50) NOT NULL, gender INTEGER NOT NULL, PRIMARY KEY (id) ); INSERT INTO student VALUES(1, \u0026#34;Mike\u0026#34;, 1); create TABLE student(...); 就代表一个表，stu_id INTEGER NOT NULL 就是表中的一列，INSERT INTO student VALUES(1, \u0026quot;Mike\u0026quot;, 1); 就是我们插入表中的一行数据。简单来说，表关注的是数据的组织形式，行关注的是数据的相关性，列关注的是数据的类型。\n数据库中的键 键（Key）是数据库表结构的重要组成部分，通常由表中一个或多个字段构成，用来唯一标识表中的一条记录。此外，键还被用来提高数据的完整性，建立表与表之间的关系。在关系数据模型中，键主要有三种：候选键、主键和外键。\n 超键（Superkey）：超键是一组能唯一标识出表中的每一行记录的属性集合。 候选键（Candidate key）：若超键不包含多余的属性（即最小超键），那么这个超键就是候选键。候选键是超键的子集，每个表都必须有至少一个候选键。例如，在学生表 student(stu_id, name, gender) 中，若 stu_id 和 name 都可以唯一确定出一名学生，那么 stu_id 和 name 都是候选键。组成候选键中的属性叫主属性（prime attributes），不包含在候选键中的属性就是非主属性。 主键（Primary key）：既然候选键可以唯一标识出表中的每一行记录，那么我们就可以从候选键中选取一个作为主键，剩下的那些候选键就被称为替代键（Alternative key）。 外键（Foreign key）：若表 A 中的属性集 S 不是表 A 的主键，但 S 是表 B 的主键，则 S 就是表 A 中指向表 B 的外键。  范式 为了保证数据库的规范化，人们开发出了一系列的设计准则。这些准则就是我们常说的 范式（normal form）。范式也分为很多等级，按照数据的规范化程度，从低到高依次有：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、BC 范式（BCNF）、第四范式（4NF）、第五范式（5NF）。\n满足最低要求的是第一范式，在第一范式的基础上进一步满足一些要求的是第二范式，在第二范式的基础上再进一步满足一些要求的是第三范式……，以此类推。数据库满足的范式越高，数据冗余就越小，一般数据表的数量也会越多。在实际应用中，我们通常只会用到前三个范式，所以本文也只会覆盖带前三个范式。\n需要注意的是，范式只是用来指导数据库设计的，在大多数业务场景下是适用的。而对于一些特殊的业务场景，范式设计的表是无法满足需求的，这时就需要进行反范式设计。\n第一范式 第一范式强调的是列的 原子性，要求表中的所有属性都是原子的，不可再分。\n例如，当学生同时有座机和手机两个电话号码时，将座机和手机放在一列——student(stu_id, name, phone) 的表设计就不满足第一范式。这个时候，将电话拆分为两列：student(stu_id, name, telephone, cellphone)，第一范式就满足了。\n第二范式 第二范式满足第一范式，并且：\n 任何非主属性都必须 完全依赖 候选键，而不能只依赖候选键中的部分属性。  例如，我们有一个 stu_course(stu_id, course_id, course_name, stu_name, grade)，主键为复合主键 (stu_id, course_id)。在这个表中，成绩是完全依赖于主键的，只有当学号和课程号确定了，才能确定成绩。而姓名是不完全依赖于主键的，因为只要学号确定了，姓名也就唯一确定了，不需要再使用课程号。所以，姓名是部分依赖于主键 (stu_id, course_id)的，学生课程表因此不满足第二范式。此外，课程名称也是部分依赖于主键的。那么如何解决这个问题呢，正确的做法是将 stu_course(stu_id, course_id, course_name, stu_name, grade) 拆分为三个表：student(stu_id, stu_name)、course(course_id, course_name) 和 stu_course(stu_id, course_id, grade)。\n第三范式 第三范式满足第二范式，并且：\n 任何非主属性都必须直接依赖候选键，不能出现传递依赖。即不能出现非主属性 A 依赖非主属性 B，而属性 B 又直接依赖于候选键键的情况。  例如，我们有一个student(stu_id, stu_name, dept_id, dept_name)。这里，学号可以唯一确定学院编号，而学院编号可以唯一确定学院名称。这就出现了学院名称 \u0026lt;- 学院编号 \u0026lt;- 学号这样的传递依赖，因此不满足第三范式。解决办法就是将student(stu_id, stu_name, dept_id, dept_name)拆分为两个子表：dept(dept_id, dept_name)和student(stu_id, stu_name, dept_id)。\n反范式 在大多数的业务场景下，范式设计的表都能满足需求。而对于一些特殊的业务场景，范式设计的表无法满足性能的需求。此时就需要根据业务场景，在范式的基础之上进行灵活设计，也就是反范式设计。\n反范式设计主要从业务场景、响应时间和字段冗余三方面考虑。用一句话总结就是：用空间来换取时间，提高业务场景的响应时间，减少多表关联。\n参考资料  Wikipedia. Database normalization. Database Keys.  ","href":"/posts/databases/database_normalization/","title":"数据库规范化"},{"content":"","href":"/tags/linux/","title":"Linux"},{"content":"","href":"/categories/linux/","title":"Linux"},{"content":"在 Linux 中，有一个东西叫链接文件，它有点像 Windows 中的快捷方式，可以很方便地实现文件的共享。链接主要分为两种：硬链接（hard link） 和 软连接（soft link）。链接在 Linux 非常有用，因为在 Linux 中，“every thing is a file”，链接可以带来极大的灵活性。\nInode 在开始之前，让我们先简单了解一下磁盘分区与 索引节点（inode）。\n为了方便管理和保证数据的安全，我们通常会对磁盘进行分区。每个分区都可以被单独挂载（mount）或取消挂载（umount）。每个分区都有自己的文件系统，不同分区的文件系统可以不同。在 Linux 文件系统中，文件就用 inode 表示，每个文件都有自己对应的 inode。每个分区都有一组自己的 inode ，不同分区的 inode 互不影响，因为它们位于不同的文件系统。在建立文件系统时，会建立一个 inode 表，表中包含一定数量的 inode 。每创建一个文件，系统就会分配一个 inode 号（inode number）给它，这个 inode 号就相当于文件的地址。\nLinux 的文件实际上由两部分组成：数据部分和文件名部分。文件名部分保存在目录中，它包含文件的名字以及对应的 inode 号，通过 inode 号我们就可以找到数据部分的相关信息。通过分开存储文件名和文件数据，Linux 系统就能做到多个文件名关联同一个 inode 号。文件的数据部分不仅包括文件所有者、创建时间等元数据，还包括实际文件内容所在的磁盘位置。硬链接和软连接其实就是两种将多个文件名关联到同一份文件数据的方法。\n硬链接 多个文件名可以指向同一个 inode 号，这些文件的关系就是硬链接的关系。简单来说，一个硬链接就是一个目录项，它关联的是文件名与 inode。由于文件本身就是一个目录项，因此一个文件至少有一个硬链接。删除硬链接时，对应 inode 的硬链接数量减一，只有当 inode 的硬链接数量为 0 时，文件才会被删除。硬链接是不能跨分区（或者跨文件系统）的，因为每个分区都有自己独立的 inode。\n软连接 软链接也叫符号链接（symbolic link or symlink），它是一种特殊类型的文件。符号链接的文件内容实际上是另外一个文件（目标文件）的路径。操作系统会把对符号链接的读写操作重定向到目标文件上。由于符号链接指向的是文件路径而不是 inode，所以符号链接是可以跨分区的。删除符号链接并不会对目标文件造成任何影响。但是，如果目标文件被删除或者重命名了，符号链接就成为了死链接。\n用一张图来说明硬链接与符号链接的区别：\n实践 创建链接的命令是 ln targetfile linkname，命令默认创建的是硬链接，若要创建符号链接，需要加上 -s 参数，即 ln -s targetfile linkname。此外，我们可以通过 ls -l 查看文件硬链接的个数，通过 ls -i 查看文件所对应的 inode 号。\n先用 mkdir links 创建一个测试目录，cd links 切换工作目录，再创建一个测试文件 echo \u0026quot;hello\u0026quot; \u0026gt; hello.txt。现在用 ls -li 检查一下：\nubuntu@XPS9380:~/links$ ls -li total 4 3095 -rw-r--r-- 1 ubuntu ubuntu 6 Jul 27 14:25 hello.txt 可以看到，hello.txt 的硬链接个数为 1。现在创建一个硬链接：\nubuntu@XPS9380:~/links$ ln hello.txt hlink ubuntu@XPS9380:~/links$ ls -li total 8 3095 -rw-r--r-- 2 ubuntu ubuntu 6 Jul 27 14:25 hello.txt 3095 -rw-r--r-- 2 ubuntu ubuntu 6 Jul 27 14:25 hlink 可以看到，hello.txt 与 hlink 对应的 inode 号都是 3095。此外，hlink 的文件类型是 -，说明硬链接就是一个普通文件。\n再来检查一下文件内容：\nubuntu@XPS9380:~/links$ cat hello.txt hello ubuntu@XPS9380:~/links$ cat hlink hello 两个文件的内容是一样的。\n现在来创建一个符号链接：\nubuntu@XPS9380:~/links$ ln -s hello.txt slink ubuntu@XPS9380:~/links$ ls -li total 8 3095 -rw-r--r-- 2 ubuntu ubuntu 6 Jul 27 14:25 hello.txt 3095 -rw-r--r-- 2 ubuntu ubuntu 6 Jul 27 14:25 hlink 3633 lrwxrwxrwx 1 ubuntu ubuntu 9 Jul 27 14:38 slink -\u0026gt; hello.txt ubuntu@XPS9380:~/links$ cat slink hello 从执行结果可以看出，slink 的文件类型是 l（符号链接）。虽然 cat 命令的输出内容与 hello.txt 相同，但 slink 有着不同的 inode 号，文件大小也不同。\n现在来试一下删除操作，我们先删除 hello.txt，然后分别查看 slink 与 hlink 的内容：\nubuntu@XPS9380:~/links$ rm hello.txt ; ls -li total 4 3095 -rw-r--r-- 1 ubuntu ubuntu 6 Jul 27 14:25 hlink 3633 lrwxrwxrwx 1 ubuntu ubuntu 9 Jul 27 14:38 slink -\u0026gt; hello.txt ubuntu@XPS9380:~/links$ cat slink cat: slink: No such file or directory ubuntu@XPS9380:~/links$ cat hlink hello ubuntu@XPS9380:~/links$ 可以发现，通过 slink 不能再访问到文件内容了，而通过 hlink 就可以。\n参考资料  Hard link. Symbolic link. Manipulating files. The difference between hard and soft links.  ","href":"/posts/linux/links_in_the_linux_filesystem/","title":"Linux 中的硬链接与软链接"},{"content":"在平常使用 Linux 的过程中，总感觉各个目录的作用存在某种约定，似乎大家都将程序放在 /bin、/sbin、/usr/bin 或 usr/sbin 下，将配置文件放到 /etc 下，将用户的主目录放到 /home 下……深入查阅相关资料后才发现，这背后还隐藏着一个标准——Filesystem Hierarchy Standard。本文的主要内容就来自 FHS 3.0。\n仔细一想，这么做也很有道理。毕竟 Linux 是开源的，发行版众多，更有很多人在 Linux 的基础上进行定制开发。如果大家都按照自己的想法配置 Linux 系统的文件系统结构，对文件的管理将会是一个令人头疼的问题。为了避免五花八门的目录层级结构，Linux 基金会发布了 FHS（Filesystem Hierarchy Standard），标准主要规范了 Linux 系统中一级目录和部分二级目录的用途，以及各个目录应该存储什么样的数据，大多数 Linux 发行版都遵循这一标准。这样，在与 FHS 相兼容的 Linux 系统上，用户可以很方便地进行文件的管理。\n区分文件 FHS 采用了两种不同的方式对文件进行区分：\n 可共享（sharable）vs 不可共享（unsharable） 可变（variable） vs 不可变（static）  可共享文件指那些可以共享给其它主机使用的文件，不可共享文件则相反。不可变文件包括二进制文件、库文件、文档等，文件的改变通常需要系统的管理员介入，可变文件则相反。\n以下是 FHS 给出的一个例子：\n    sharable unsharable     static /usr, /opt /etc, /boot   variable /var/mail, /var/spool/news /var/run, /var/lock    根目录 Linux 将整个文件系统看成一棵树，这棵树的根节点就是根目录，用 / 表示。根目录很重要，所有的目录都是由根目录衍生出来的，它还与开机、还原、系统修复等操作有关。\nFHS 建议将根目录放在不太大的分区，因为分区越大，可放入的数据就越多，出错的可能性就会增大。FHS 要求根目录下应该有以下目录：\n /bin：存放供所有用户使用的完成基本维护任务的命令。这里的 bin 是 binaries 的缩写，代表二进制文件，通常是可执行的。一些常用的命令，比如 cat、ls、cp、rm 等都在这个目录下。FHS 还要求 /bin 目录不能有子目录。 /boot：存放启动 Linux 时用到的一些核心文件，比如操作系统内核、引导程序 Grub 等。 /dev：存放所有的设备文件。从此目录可以访问磁盘、内存、CD-ROM 等系统设备。dev 表示 devices。FHS 还要求 /dev 下有以下设备：  /dev/null：写到这个设备的所有内容都会被丢弃，我们可以通过 \u0026gt; 重定向符号将输出流重定向到 /dev/null 来忽略输出的结果。 /dev/zero：这是一个产生数字 0 的设备，写到这个设备的所有内容都会被丢弃。无论你对它进行多少次读取，都会读到 0。 /dev/tty：表示当前进程的控制终端（如果有的话）。所以，当我们执行 echo \u0026quot;hello\u0026quot; \u0026gt; /dev/tty 时，hello 就会出现在屏幕上。tty 是 Teletype 的缩写。   /etc：存放配置文件。 /home：用户的主目录，存放用户的个人文件。除 root 用户外，每个用户的主目录均在 /home 下以用户自己的名字命名。 /lib：存放启用系统或运行 /bin 和 /sbin 目录中的命令所需的共享库。/lib目录可能会有一些变种，比如 lib32 存放 32 位程序所需的库文件，而 lib64 存放 64 位程序所需的库文件。 /media：挂载可移动设备，一般用于自动挂载 U 盘等媒体。 /mnt：临时用于挂载文件系统的地方， /opt：多数第三方软件默认会安装到此位置。这里的 opt 是 optional 的缩写。 /root：root 用户主目录的默认位置。 /run：存放系统启动后与系统的信息数据，包括 PID(process identifier) 文件，PID 文件通常命名为 \u0026lt;program-name\u0026gt;.pid。在 boot 进程开始之前，这个目录必须被清空。 /sbin：存放与系统启动、修复、恢复等与系统维护有关的命令，例如 shutdown、reboot、fsck、ifconfig 等，需要 root 用户才能执行。sbin 表示 system binaries。 /srv：存放系统所提供的服务的相关数据，例如 /srv/www 可包含 Web 服务器相关的数据。srv 表示 service。 /tmp：存放临时文件。所有用户都可以在这个目录中创建、编辑文件，但只有文件拥有者才能删除文件。tmp 表示 temporary。 /sys：存放与设备、驱动和一些与内核有关的信息。 /proc：存放内核与进程的状态信息。例如 /proc/cpuinfo 含有与 CPU 相关的信息。  /usr 目录 /usr 是一个庞大的文件夹，其下的目录结构与根目录相似。根目录中的文件多是系统级的文件，而 /usr 中是用户级的文件，一般与具体的系统无关。/usr 目录存放的是 可共享、只读 的数据。这意味着 /usr 中的文件可以在与 FHS 兼容的主机之间共享，且不可以被修改。\n在早期的 Unix/Linux 系统中，/usr 被用作用户的主目录（类似于现在的 /home 目录），usr 表示 user。而在现在的 Unix/Linux 系统中，/usr 存放的是用户级的程序和数据，usr 表示 User System Resources。不过在网上，有人说 usr 是 Unix Software Resources 的缩写，还有人说是 Unix System Resources 的缩写……笔者在这里给出上面内容的参考来源——Linux Filesystem Hierarchy：/usr，大家可以自行判断。当然，名字的来源不知重点🤭\n/usr 的子目录一般有：\n /usr/bin：绝大部分用户可使用的命令都放在这里，这些命令通常与系统的维护无关。 /usr/include：主要放置 C/C++ 程序的头文件。 /usr/lib：存放供应用程序使用的库文件。 /usr/local：系统管理员在本机安装的软件一般都在这下面，其目录结构与 /usr 目录类似。 /usr/sbin：存放不是 /sbin 中一定要有的系统管理命令。 /usr/share：存放于系统硬件架构无关的数据。比如 /usr/share/man 存放有在线手册、/usr/share/xml 存放 XML 数据。 /usr/src：存放源代码文件。  /var 目录 /var 目录存放着可变的数据文件，比如运行日志、临时文件等。var 表示 variable。\n/var 的子目录主要有：\n /var/cache：存放应用的缓存数据。 /var/crash：存放系统崩溃后的转储文件（dump）。 /var/lib：存放应用程序运行时需要的状态信息。 /var/lock：存放锁文件。某些设备或资源一次只能被一个应用程序使用，如果资源同时被两个或多个程序使用，就可能产生一些错误，因此需要上锁。 /var/log：存放日志数据。 /var/opt：存放安装在 /opt 目录中的程序用到的可变数据。 /var/run：存放正在执行的程序的信息，例如 PID 文件。 /var/tmp：存放系统重启之间保存的临时文件。  参考资料  FHS 3 Specification Series. Linux Filesystem Hierarchy：/usr. Filesystem Hierarchy Standard. Terminal Special Files such as /dev/tty.  ","href":"/posts/linux/linux_directory_structure/","title":"Linux 目录结构"},{"content":"在 Windows 中，文件类型是根据扩展名来确定的。例如：a.pdf 是一个 pdf 文件，而 b.txt 是一个 txt 文件。但在 Linux/Unix 中，文件类型与文件扩展名没有关系，它们是两个完全不同的概念。Linux/Unix 将一切都看作是文件，了解文件类型有助于我们对它们进行高效的管理。\nPOSIX 中定义了七种标准的文件类型：\n 普通文件（regular file） 目录（directory） 符号链接（symbolic link） 命名管道（FIFO special file or named pipe） 套接字（socket） 字符设备（character special file） 块设备（block special file）  此外，部分 Linux/Unix 操作系统发行版中可能还有 POSIX 规定以外的文件类型，比如 Solaris 中的 door 类型。\n在 Linux/Unix 文件系统中，文件的元数据存储在一个被称为 inode 的数据结构中，每个文件都有一个对应的 inode。我们可以使用 stat 命令来查看某个文件的 inode 信息。若只需要查看文件类型，stat输出内容未免太多了点，这个时候我们可以使用 file 命令或者 ll(或 ls -l) 命令。file 命令会直接展示文件类型，而 ll 命令的文件类型隐藏在输出的第一个字母中，以下面输出为例：\ndrwxr-xr-x 4 ubuntu ubuntu 4096 Feb 24 23:28 test ls -l 命令中的参数 l 表示 “use a long listing format”，该输出的内容从左到右分别是：文件类型（d）、权限（rwxr-xr-x）、硬链接个数（4）、所有者（ubuntu）、所属组（ubuntu）、文件大小（4096）、上次修改时间（Feb 24 23:28）和文件名（test）。其中权限又分为三部分，从左到右分别是所有者权限（rwr）、所属组权限（r-x）和其他人权限（r-x）。我们这里主要关心的是文件类型，即输出中的第一个字母。ls 的文档给出了 13 个字母，用来标识不同的文件类型：\n   character file type     - regular file   b block special file   c character special file   C high performance (“contiguous data”) file   d directory   D door (Solaris 2.5 and up)   l symbolic link   M off-line (“migrated”) file (Cray DMF)   n network special file (HP-UX)   p FIFO (named pipe)   P port (Solaris 10 and up)   s socket   ? some other file type    ls 命令中还有一个有趣的参数是 -F，它的作用是“Append a character to each file name indicating the file type”。同样，ls 的文档也给出了不同字母的含义。可能是为了方便显示和区分，-F 输出的内容中用的是符号：\n   字母 文件类型     * 可执行文件   / 目录   @ 符号链接   | 管道   = 套接字   \u0026gt; door    没有在文件名末尾追加符号的就是普通文件。\n普通文件 普通文件 是一个可随机访问的字节序列。它是 Linux/Unix 中最多的一类文件，包括文本文件、二进制文件、图片、视频、压缩文件等。\n目录 目录是一个包含目录项的文件。目录项将文件名与文件（比如普通文件、目录和各种特殊文件）进行关联。但一个目录中不能有两个同名的目录项。\n符号链接 符号链接指向计算机上的另一个普通文件或目录，它会影响到路径名解析的过程。\n命名管道 命名管道（named pipe）即具备 FIFO 特性的文件，在 Linux/Unix 中常用于进程间通信。与传统的匿名管道（即我们常用的 | 命令）不同的是，命名管道使用到了文件系统，是一种文件类型。我们可以使用 mkfifo 命令创建命名管道，例如：\nmkfifo my_pipe 使用 ls -l 命令可以发现输出结果的第一个字母是 p，同时文件大小是 0：\nprw-r--r-- 1 ubuntu ubuntu 0 Jul 22 16:51 my_pipe 当执行 cat my_pipe 时，你会发现当前的终端一直处于等待状态。因为此时 my_pipe 中没有任何内容（你刚才也看到了，它的文件大小是 0）。此时，若我们另开一个终端写点东西到 my_pipe 中，例如：\necho \u0026#34;test\u0026#34; \u0026gt; my_pipe 这时，刚才执行的 cat my_pipe 就会返回，并打印出 test。\n套接字 套接字作为通信端点用于进程间通信，是一种有着特定用途的文件。在进程间通信方面，它和管道有一些相似之处，但管道是单向的，而套接字是双向全双工的。\n字符设备 字符设备 提供串行输入流或者接收串行输出流，不具备随机访问的能力。例如终端和 /dev/null 都是字符设备文件。\n块设备 块设备具备随机访问的能力，/dev/ 目录下的大多数文件都是块设备。\n参考资料  Unix file types.  ","href":"/posts/linux/file_types_in_linux_or_unix/","title":"Linux/Unix 中的文件类型"},{"content":"","href":"/tags/%E6%B8%B8%E8%AE%B0/","title":"游记"},{"content":"","href":"/categories/%E6%B8%B8%E8%AE%B0/","title":"游记"},{"content":"自大学喜欢上骑行以来，川藏线就一直萦绕在心头，这个念想终于也在我毕业两年后成为了现实。经过一番准备，我于 2021 年 6 月 22 日出发，用车轮去轻吻最美的 318，去追寻大学时的梦。\nD1：成都——雅安（141km） 原本是打算 21 号出发的，但由于快递原因，装备没有到齐，只得推迟一天出发。一直没找到靠谱的队友，只得独自踏上征途，既兴奋又忐忑。\n上午的天气还不算太热，我独自穿过双流——新津，在前往邛崃的路上终于碰到了三个长途骑行者，交谈后发现他们来自飞登骑行，此行的目的地也是拉萨。\n三人中为首的是一位来自山东的大爷，以前挑战过一次川藏线，未成功，这次志在必得。加了胖子的微信，跟他们走了一段发现节奏相差太大，于是我又单飞了。\n到了邛崃，烈日当空，看了一眼手表，已是饭点，遂在路口找了家面馆吃面，顺便休息休息。一碗面下肚，准备出发，发现一白衣骑士向我打招呼，叫我歇会儿再走。想了一下，反正今天也不赶时间，又看了看这头顶的烈日，就坐下和他聊了起来。通过聊天得知，这兄弟来自河南，姓麻，我管他叫“麻兄”。麻兄今天也刚从成都出发，目的地也是拉萨，有缘的是，麻兄成为了我一路的室友，直到拉萨。麻兄面还未吃完，两位女骑士有从旁而过，其中一大姐还叫我帮她看车子的问题。看着问题聊着天，发现她们是从绵阳出发，今天第二天，也是打算到拉萨。问她们今晚准备到那，说是飞仙关镇或者天全县，我默默地掏出手机看了下地图，连道了三句“牛逼”。这两位女骑士中有一位比较厉害，我管她叫“大佬”，大佬管另一位叫“点灯”。\n待麻兄吃完面稍作歇息后，我就和麻兄一起先出发了。我和麻兄一致认为她们到飞仙关或天全的计划是痴心妄想，没想到是真的，因为第二天我们又碰上她们了。原来，她们第一天真的到了天全县，只不过是赶了夜路，晚上十点到的。更有趣的时，她们路上还捡了两个人，组建了微信群“大佬三人行和他们的小跟班”。\n邛崃出去不久，我和麻兄就在路边找了棵大树歇凉，因为天气实在太热了。树下歇了近半小时，也没见一朵云跑来把太阳遮住，无奈之下，我们又出发了。成都的夏天，一个西瓜绝对能让你爽上天，冰镇的就更不用说了。顶着太阳骑行到了甘溪镇，发现马路对面有一水果店，我和麻兄决定进去吃个西瓜，顺便避避天上这个火球。谁知，刚进水果摊，就感觉太阳光的强度变得更大了，也罢，我们就吃着西瓜等……吃瓜过程中，马路对面过去了一辆自行车，最开始我还以为是学生，后来追上后发现他也是准备骑自行车到拉萨，来自广东，所以我们管他叫“小广东”。\n小广东来成都之前在广西游玩，骑行装备都是在成都添置的，今天中午才从邛崃出发，喜欢深度游玩。我和麻兄、小广东三人决定暂时结伴同行，一路有说有笑，于下午五点过抵达雅安马踏飞燕地标。\n其实在距离马踏飞燕还有一公里左右时，就有一个骑着电摩的人追着我们问要不要住宿，此人身份就不细说了。虽是骑行联盟的客栈老板，这揽客未免也太过热情了吧，居然一直跟着我们到了马踏飞燕，还给我们拍了照片。婉言相距之后，我们继续前行，在雅安汽车站附近找了加宾馆入住，结束了第一天的行程。晚饭后，带两位外地朋友尝了下雅安的冰粉，一致好评，以至于后面几天他们都在找冰粉。\nD2：雅安——泸定（133km） 一早起来，吃完早饭，还不到七点，小广东、麻兄和我三人就出发了。一路经过了昨天提到的飞仙关镇，再往前来到了天全县城，入城补给了一些东西之后就开始帮他们寻找捷安特店。地图显示车店位于一个菜市场内，到了地方却不见店铺，反而让我们开始怀疑自己是不是来错地方了。麻兄跟着导航，带着我和小广东一阵瞎转无果，我便停下来看地图，任麻兄继续前去。好在高德有车店老板电话，给老板打电话确认了店还在就放心了，这时麻兄也已经找到了车店，开始联系我们了。通过共享位置，我终于在菜市场旁边的一个小巷子内见到了车店。和老板聊了聊昨天中午帮大姐修车的事情，老板还一个劲的叮嘱我要让大姐她们经常检查后轮的位置，安全第一，让我一定要把话带到。又和老板交谈了一会儿，感觉老板是个十分朴实简单的人。\n待他俩在车店买完需要的东西之后，我们便再次上路。上坡居多，小广东慢慢消失在了回头的视野中。约摸一个小时，我和麻兄就追上了在紫石关和摩旅大哥们合照的他们四人，我和麻兄也应邀加入了合照。\n合影之后我和麻兄加入了他们的群，群因此改名为“大佬五人行和他们的小跟班”😄。继续往前，李兄一骑绝尘，很快就消失在了事业当中，ORZ……11 点半左右，两姐姐拦住了路上卖水果的小货车，买了个西瓜，叫上我和麻兄一起吃，还未吃完，露兄已经赶了上来。摄影师拍下了在江边洗完手的我们。\n今天的午饭是在新沟吃的，由于麻兄在下午一点半有面试，所以我们开始留意合适的地方。在进山之前，我们遇到一处施工队的宿舍，想着前面可能已经没有房子，就进去求助。接见我们的是一位年轻的大哥，几天前也是刚来这里，闲聊中得知他也是骑行爱好者。我打算等麻兄面试完之后一起爬二郎山，所以我看着两位大姐、李兄和露兄路过。后面，小广东也上来了，歇了一会儿，他说他骑得慢就先出发了。等麻兄面完，已是两点过了，我们开始赶路。才走了几公里，麻兄就收到了马上要再次面试的消息，恰好路过一处没人的板房，麻兄就在外面找了个角落继续面试了。面完三点过，看了下地图，我们开始赶路，很快就追上了小广东，他居然还买了路边阿姨卖的蜂蜜，足足两斤，这兄弟带着这东西爬二郎山也是够强的！\n二郎山是川藏线上的第一座大山，海拔 3400 多米，我们需要穿过的二郎山隧道海拔 2200。我和麻兄到达隧道入口时已是下午五点四十多，两人都饿了，所以我们在吃了点东西，顺带歇了歇，准备六点过隧道。隧道前的温度偏低，我不得不加了件皮肤衣保暖，小广东也奇迹般地爬了上来。穿过隧道后，我们三人都傻眼了，居然还有个一公里多的上坡才到垭口，出乎意料！没办法，只能咬咬牙继续干，熬到垭口。垭口风景很棒，一侧写着“千里川藏线，天堑二郎山”，另一侧则是凶险的悬崖。\n天色渐晚，匆匆欣赏完垭口的风光后，我们便开始下山。一路爽坡到了泸定县城，天还未黑，所以我们决定先去泸定桥打个卡。我和麻兄到了泸定桥后，很快就看到了大姐她们，原来他们也刚到一会儿，正买了门票准备到桥上看看。我去买票时，售票系统已经关闭，好心的售票员让我用现金买到了票。来到泸定桥上，这是一座铁索和木板建造的桥，走上去便开始摇晃，脚下又是汹涌的大渡河，不由得把手机放进了包里，深怕一不小心手机掉入江中一去不返。想起当年红军飞夺泸定桥的故事，肃然起敬。\nD3：泸定——折多塘（63km） 从住宿的宾馆出来，吃过早餐，我们便朝康定出发了。刚出城就是一个隧道，隧道紧邻大渡河，不宜久留，迅速通过。穿过隧道，迎来一个小下坡，然后又是一个上坡一个下坡。我们都知道，今天的行程绝大部分都是上坡，但这个坡下得让我们怀疑人生。因为我们前进的方向刚好与水流的方向相反，然而我们的海拔却在不断下降。当坡下完之后，我们差不多也到了瓦斯沟，随后便开始了漫漫的上坡。\n我大概在十二点五十左右抵达康定市，想当年，一首《康定情歌》火遍了大江南北。跑马溜溜地山上，一朵溜溜的云哟……之前一位成都的骑友推荐我到康定一定要尝尝田凉粉，我便打开高德搜索地点，得知田凉粉就在新市后街。想着麻兄还未抵达，便给他发了个消息，说我去田凉粉等他。没想到之后发生的事情就把我打脸了，我对着高德地图上田凉粉的位置转了一圈又一圈，还到隔壁新市前街看了看，就是没找到，直到和麻兄汇合我都没找到。麻兄到了之后，我先是给店家打了个电话，确认了店还在，得到了大概位置。到了附近，麻兄问了一位外卖小哥，立马就找到了店的位置。果然，外卖小哥比地图好用……不幸的是，等我们到了店里坐下才发现，凉粉卖完了😒……\n虽然店里还有不少小吃，但想着下午还有几十公里的坡，得吃点管饱的，要不然顶不住我们就出来找饭店了。吃完歇息后，时间已到下午两点半。刚一出发，天上就下起了雨，还越来越大。我们匆匆忙忙套上雨衣雨裤，穿上雨靴，继续前进。来到城外，爬了一小段坡之后雨就停了。爬坡很热，雨衣下的衣服几乎已经湿透，到了一个路边的停车场，便将雨衣脱下，山上的风带走了多余的热量，很是凉爽，顺便拍了一张不完整的康定城的照片：\n四点一刻，经过大桥湾，继续向前，来到了一处观景平台，看到几个大字——“情歌故里欢迎你”。\n在此小歇一会儿后，继续往折多塘前进。五点过，终于骑到了折多塘村，远远地就看到大佬在一家内江饭店前面向我挥手了。过去才发现，大佬已经问了一遍周围住宿的价格，内江的老乡看我们骑车累，也给了我们很大的优惠，于是我们就决定在这里住下。由于饭店的三人间没有洗浴的地方，所以我和先到的麻兄不厚道地把小广东抛弃了，小广东最后在隔壁青年旅舍入住。小广东到的时候已经是六点半以后了，那时我正在楼顶洗衣服，楼顶的风很大，山上流下来的水很是冰凉。当晚，我们七个人都在内江饭店吃饭，这竟然成了我们七个人最后一次一起吃饭，因为第二天小广东只到了新都桥，而我们六个人翻过了高尔寺山到了雅江。\n饭后得知，饭店早上不提供早餐，我们需要自己想办法。出去打听了一圈，发现周围的饭店都没有早餐，只有青旅到七点后才提供面条。想着七点太晚了，跟老板商量，老板给我们六人一人煮了两个土鸡蛋，早上我们自己用水壶热。于是，我又和麻兄到超市，一人买了通泡面，在此之前，我可能有一年没吃过泡面了。\n准备好第二天早上的东西之后，就睡了。\nD4：折多塘——雅江（126km） ","href":"/itinerary/riding_on_sichuan-tibet_highway/","title":"骑行川藏线"},{"content":"我们在工作中经常听人提起线程安全，但要是被问到什么是线程安全，我们可能就会挠挠脑袋了。因为线程安全并没有一个明确的定义。Java Concurrency in Practice 这本 Java 并发宝典是这样解释线程安全的：\n 当多个线程访问某个类时，这个类始终都能表现出正确的行为，那么就称这个类是线程安全的。线程安全性对多个线程之间的操作提出了要求：多个线程之间的操作无论采用何种执行时序或交替方式，都要保证不变性条件不被破坏。\n若一个类既不包含任何域，也不包含任何对其它类中域的引用，则称这个类是无状态的。无状态的对象一定是线程安全的。\n 本文的绝大部分内容也来自这本书。\n线程安全问题 如果线程不安全，会出现哪些问题呢？常见的问题有三类：\n 运行结果错误 对象逃逸 活跃性问题  运行结果错误 最常见线程安全问题可能就是 运行结果错误 了。比如：\npublic class UnsafeCounter { private static volatile int count = 0; public static void main(String[] args) throws InterruptedException { Runnable r = () -\u0026gt; { for (int i = 0; i \u0026lt; 1000; i++) { count++; } }; Thread t1 = new Thread(r); t1.start(); Thread t2 = new Thread(r); t2.start(); // 等待两个线程运行结束  t1.join(); t2.join(); System.out.println(\u0026#34;count = \u0026#34; + count); } } 这段代码定义了一个静态变量 count，然后启动两个线程，每个线程都对这个变量执行 1000 次递增操作。理论上，最终应该得到的 count 值为 2000，但实际上结果可能是 1662，也可能是 1713，每次运行的结果都还不一样。问题的原因在于：count++ 不是一个原子操作，它的执行步骤主要分为三步：\n 读取 count 的值 将 count 的值加一 保存 count 的值  由于 CPU 是以时间片为单位进行线程调度的，每个线程都可以在某个时间片内使用 CPU。当前时间片被用完后，操作系统就会暂停当前线程，然后把 CPU 分配给其它线程，当其它线程用完之后，被暂停的线程又会得到使用 CPU 的机会。在 count++ 的三个步骤中，每一步操作都有可能被打断。这就是 原子性问题。虽然变量 count 被 volatile 修饰，但由于 volatile 只能保证可见性，并不能保证原子性，所以运行结果还是有问题的。\n对象逃逸 对象发布 “发布”一个对象意味着该对象能够被当前作用域以外的代码使用。例如：将一个指向对象的引用保存到其它代码可以访问的地方，或者在某一个非私有的方法中返回该引用，或者将该引用传递到其它类的方法中。\n发布对象的最简单方法就是将对象的引用保存到一个公有的静态变量中。当发布一个对象时，该对象的非私有域中引用的所有对象也会被发布。还有一种发布对象或其内部状态的方法就是：发布一个内部类的实例。\n对象逃逸 “逃逸”是指某个不应该被发布的对象被发布出去了。下面是一个例子：\npublic class WrongInit { private Map\u0026lt;String, String\u0026gt; colorMap; public WrongInit() { new Thread(() -\u0026gt; { colorMap = new HashMap\u0026lt;\u0026gt;(); colorMap.put(\u0026#34;RED\u0026#34;, \u0026#34;红\u0026#34;); colorMap.put(\u0026#34;ORANGE\u0026#34;, \u0026#34;橙\u0026#34;); colorMap.put(\u0026#34;YELLOW\u0026#34;, \u0026#34;黄\u0026#34;); // ...  }).start(); } public Map\u0026lt;String, String\u0026gt; getColorMap() { return colorMap; } public static void main(String[] args) { WrongInit wrongInit = new WrongInit(); System.out.println(wrongInit.getColorMap().get(\u0026#34;RED\u0026#34;)); } } 这段代码并不会输出 红，而是会抛出 NullPointerException。这是因为 colorMap 的初始化放在了构造函数内的新线程中，而线程的启动是需要一定的时间的，但是主函数并没有等待就直接获取数据，这个时候 colorMap 可能并没有初始化好。\n正常情况下，colorMap 应该在构造函数内被初始化好了才可让外界访问，以上代码就出现了 colorMap 的逃逸。对象逃逸会带来问题，因为：只有当对象的构造函数返回时，对象的状态才是可预测的（predictable）、一致的（consistent）。在错误的时间或地点发布或初始化可能导致线程安全问题。\n活跃性问题 在线程的安全性和活跃性之间通常存在着某种平衡。我们使用加锁机制来确保线程安全，但如果过度的使用加锁，则可能导致 顺序死锁（Lock-ordering Deadlock）。同样，我们使用线程池和信号量来限制对资源的使用，但这些限制行为可能导致 资源死锁（Resource Deadlock）。Java 应用程序无法从死锁中恢复过来（恢复应用程序的唯一方式就是终止并重启它，并希望它不要再发生），因此在设计时一定要排除那些可能导致死锁出现的条件。\n活跃性问题与前面提到的运行结果错误有着很大不同，当出现活跃性问题时，程序可能始终都得不到结果。常见的活跃性问题有三种，分别是：死锁、饥饿和活锁。\n死锁 死锁（Deadlock） 是指两个线程之间相互等待对方占有的资源（锁），但同时又互不相让，都想自己先执行。\n顺序死锁 以下代码可能导致顺序死锁：\npublic class LeftRightDeadLock { private final Object left = new Object(); private final Object right = new Object(); public void leftRight() { synchronized (left) { synchronized (right) { // doSomething();  } } } public void rightLeft() { synchronized (right) { synchronized (left) { // doSomethingElse();  } } } public static void main(String[] args) { LeftRightDeadLock lrdl = new LeftRightDeadLock(); new Thread(lrdl::leftRight).start(); new Thread(lrdl::rightLeft).start(); } } 如下图所示：线程 A 调用的方法会尝试先获得 left 的锁再尝试获得 right 的锁，而线程 B 刚好相反，并且这两个线程的操作是交替执行，那么它们就会发生死锁。\n以上发生死锁的原因是：两个线程试图以不同的顺序获得相同的锁。如果彼此按照相同的顺序来请求锁，那么就不会出现循环的加锁依赖性，也就不会产生死锁。换句话说：如果每个需要锁 L 和 M 的线程都以 相同的顺序 来获取锁 L 和 M ，就不会发生死锁。\n资源死锁 正如当多个线程相互持有彼此正在等待的锁而又不释放自己持有的锁时会发生死锁一样，当它们发生在相同的资源集合上等待时，也会发生死锁。\n假设有两个资源池 L 和 M ，若在线程 A 持有 L 中的 S 并等待 M 中的 T 的同时，线程 B 持有 M 中的 T 并等待 L 中的 S ，则会出现死锁。这有点类似于顺序死锁中提到的交替执行导致死锁出现的情况。\n另一种基于资源的死锁形式就是线程饥饿死锁（Thread-Starvation Deadlock）：只要资源池中的任务需要无限等待一些必须由池中的其它任务才能提供的资源或条件（例如一个任务等待另一个任务的执行结果），那么除非资源池足够大，否则将发生线程饥饿死锁。\n例如：在单线程的 Executor 中，如果一个任务将另一个任务提交到同一个 Executor，并且等待这个被提交的任务的执行结果，那么通常会引发死锁。因为第二个任务将停留在工作队列中，并等待第一个任务完成，而第一个任务又无法完成，因为它在等待第二个任务的完成。这就是线程饥饿死锁。\n饥饿 饥饿（Starvation） 是指线程由于无法访问它所需要的资源而不能继续执行。比如，Java 应用程序对线程的优先级设置不当，导致部分线程无法获取 CPU 时间片。Java 中的线程是有优先级的。Java 中线程优先级分为 1 到 10，1 最低，10 最高，优先级越高的线程获得 CPU 时间片的概率越大。如果我们把某个线程的优先级设置为 1，这是最低的优先级，在这种情况下，这个线程就有可能始终分配不到 CPU 资源，而导致长时间无法运行。\n活锁 活锁（Livelock） 不会阻塞线程，但也不能继续执行，因为线程将不断重复执行相同的操作，而且总会失败。这就像两个过于礼貌的人在半路上面对面地相遇：他们彼此都给对方让路，然后又在另一条路上相遇了，并且他们就这样反复地避让下去。可以在重试机制中引入随机性来解决活跃性问题，这样能减少冲突。\n线程安全问题出现的场景 线程安全问题最常出现在对共享变量或共享资源的访问中。前面提到的计数问题就是一个典型的例子。\n有时，我们的操作是依赖时序的，而并发场景下可能出现执行顺序与我们预想不一致的情况。比如：\nif (x == 1) { x = 3 * x; } 这段代码涉及先检查后计算的组合操作。但这个组合操作并不是原子操作，中间可能被打断。\n我们使用其他类时，如果对方没有声明自己是线程安全的，那么这种情况下对其进行并发操作，就有可能会出现线程安全问题。\n参考资料  Brian Goetz, Tim Peierls, Joshua Bloch, Joseph Bowbeer, David Holmes, and Doug Lea. Java Concurrency in Practice. Addison-Wesley Professional, 2006.  ","href":"/java/concurrency/thread_safety/","title":"线程安全"},{"content":"","href":"/categories/%E5%B9%B6%E5%8F%91/","title":"并发"},{"content":"","href":"/tags/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85/","title":"生产者消费者"},{"content":"生产者/消费者模式（Producer/Consumer Pattern）是一种非常常见的程序设计模式，广泛用于消息队列、解耦等场景中。简单来说，就是有一个共享的数据结构连接着生产者与消费者，生产者负责生产数据并将数据添加到共享数据结构中，而消费者负责从共享数据结构中取走数据并消费。如果生产者的生产速度特别快，而消费者的消费速度又特别慢，就会出现“产能过剩”的情况，反之则是“产能不足”。\n上面提到的共享数据结构其实扮演了缓冲区的角色，它平衡了生产者与消费者之间的能力。当缓冲区满时，生产者被阻塞，但当消费者拿走数据空出位置之后，消费者就会通知生产者进行生产；而当缓冲区空时，消费者被阻塞，等待数据到来，在生产者将生产的数据放到共享数据结构之后，就会通知消费者去消费。\n一般情况下，这个共享数据结构的空间是有限的，因此生产者-消费者问题又称有界缓冲区问题（bounded-buffer problem)）。\n在 Java 中，实现生产者/消费者模式的方法又很多种，常见的有基于阻塞队列（BlockingQueue）的、也有基于 Condition 的，还有基于 wait/notify 的。\n使用阻塞队列实现生产者/消费者模式 这时，阻塞队列承担的就是共享数据结构的功能。\npublic class BlockingQueueProducerConsumer { public static void main(String[] args) { BlockingQueue\u0026lt;Object\u0026gt; buffer = new ArrayBlockingQueue\u0026lt;\u0026gt;(10); Runnable producer = () -\u0026gt; { while (true) { try { buffer.put(new Object()); } catch (InterruptedException e) { e.printStackTrace(); } } }; Runnable consumer = () -\u0026gt; { while (true) { try { buffer.take(); } catch (InterruptedException e) { e.printStackTrace(); } } }; new Thread(producer).start(); new Thread(producer).start(); new Thread(consumer).start(); new Thread(consumer).start(); } } 上面这段代码先是创建了一个用于生产者/消费者交流的缓冲区，随后分别开启两个生产者线程和两个消费者线程，生产者不断向 buffer 添加数据，而消费者不断从 buffer 消费数据。虽然这段代码看上去不涉及任何阻塞等待与唤醒，因为阻塞等待与唤醒操作是 ArrayBlockingQueue 内部实现的。\n使用 Condition 实现生产者/消费者模式 以下代码利用 Condition 实现生产者/消费者模式，其背后的思想与 BlockingQueue 非常相似：\npublic class ConditionProducerConsumer { static class MyBlockingQueue { private Queue buffer; private int max = 16; private ReentrantLock lock = new ReentrantLock(); private Condition notEmpty = lock.newCondition(); // 队列没空  private Condition notFull = lock.newCondition(); // 队列没满  public MyBlockingQueue(int size) { this.buffer = new LinkedList(); this.max = size; } public void put(Object o) throws InterruptedException { lock.lock(); try { while (buffer.size() == max) { notFull.await(); // 自动释放lock并等待空间  } buffer.add(o); notEmpty.signalAll(); // 唤醒消费者  } finally { lock.unlock(); } } public Object take() throws InterruptedException { lock.lock(); try { while (buffer.size() == 0) { notEmpty.await(); // 自动释放lock并等待数据  } Object item = buffer.remove(); notFull.signalAll(); // 唤醒生产者  return item; } finally { lock.unlock(); } } } public static void main(String[] args) { MyBlockingQueue buffer = new MyBlockingQueue(10); Runnable producer = () -\u0026gt; { for (int i = 0; i \u0026lt; 100; i++) { try { buffer.put(new Object()); } catch (InterruptedException e) { e.printStackTrace(); } } }; Runnable consumer = () -\u0026gt; { for (int i = 0; i \u0026lt; 100; i++) { try { buffer.take(); } catch (InterruptedException e) { e.printStackTrace(); } } }; new Thread(producer).start(); new Thread(consumer).start(); } } 首先代码中实现了一个简化版的阻塞队列，它充当的是缓冲区的角色。随后，定义了一个 ReentrantLock 类型的锁，并在它的基础上创建了两个 Condition：\n notEmpty：表示缓冲区中有数据 notFull：表示缓冲区中有空闲空间  最后，是 put() 和 take() 这两个方法，分别用于往缓冲区中放数据和从缓冲区中取数据。由于生产者/消费者模式通常发生在多线程的场景下，所以我们需要保障线程安全。为此，put() 方法首先获取锁，然后在 while 循环中检测缓冲区的空闲情况，如果缓冲区满了，则调用 notFull.await() 阻塞生产者并释放锁，如果没有满，则向缓冲区中添加数据并使用 notEmpty.signalAll() 唤醒所有正在等待的消费者。最后的 finally 保证了锁的正确释放。\ntake() 方法类似，它先通过 while 检查缓冲区是否为空，若为空则让消费者等待，否则从缓冲区中取走数据并通知生产者有空闲位置，最后释放锁。\n使用 wait/notify 实现生产者/消费者模式 虽然使用方法不同，但它的原理与 Condition 非常类似：\npublic class WaitNotifyProducerConsumer { static class MyBlockingQueue { private Queue buffer; private int max = 16; public MyBlockingQueue(int size) { this.buffer = new LinkedList(); this.max = size; } public synchronized void put(Object o) throws InterruptedException { while (buffer.size() == max) { wait(); // 自动释放lock并等待空间  } buffer.add(o); notifyAll(); // 唤醒消费者  } public synchronized Object take() throws InterruptedException { while (buffer.size() == 0) { wait(); // 自动释放lock并等待数据  } Object item = buffer.remove(); notifyAll(); // 唤醒生产者  return item; } } public static void main(String[] args) { MyBlockingQueue buffer = new MyBlockingQueue(10); Runnable producer = () -\u0026gt; { for (int i = 0; i \u0026lt; 100; i++) { try { buffer.put(new Object()); } catch (InterruptedException e) { e.printStackTrace(); } } }; Runnable consumer = () -\u0026gt; { for (int i = 0; i \u0026lt; 100; i++) { try { buffer.take(); } catch (InterruptedException e) { e.printStackTrace(); } } }; new Thread(producer).start(); new Thread(consumer).start(); } } ","href":"/java/concurrency/producer_consumer_pattern/","title":"生产者消费者模式的几种实现方案"},{"content":"","href":"/tags/gc/","title":"GC"},{"content":"The Truth About Garbage Collection 这篇文章写得挺好的，本文的很多内容也是基于这篇文章而来。\nJava 是一门面向对象的编程语言，在程序的运行过程中，不断有新的对象被创建出来，也不断有对象被回收。JVM 中的对象从创建到回收，通常会经历以下大多数状态：\n Created In use (strongly reachable) Invisible Unreachable Collected Finalized Deallocated  Created 对象的创建通常会经历以下几个步骤：\n 为对象分配空间 开始构造对象 调用父类的构造函数 初始化实例与实例变量 执行构造函数的剩余部分  这些操作的具体代价取决于 JVM 的实现，以及构造类的过程是如何实现的。对象被创建后，如果它被赋给某给变量（有变量引用了这个对象），它就会直接进入 In Use 状态。\n其实以上五个步骤可以分为两个大的步骤——实例化（Instantiation）和初始化（Initialization）。为了便于理解，我举一个例子。先定义一个类 Bird，它有一个 name 属性：\npublic class Bird { private final String name; public Bird(String name) {this.name = name;} } 当我们想创建一个 Bird 对象时，我们会怎么做？最简单最直接的方法当然是使用 new 关键字啦。比如：\nBird eagle = new Bird(\u0026#34;eagle\u0026#34;); 这行代码包含三个部分：\n 声明（Declaration）：Bird eagle 声明了一个类型为 Bird 的变量，变量名为 eagle。 实例化（Instantiation）：Java 使用 new 关键字创建新对象。new 先为新对象分配内存，然后返回那块内存的引用，这个过程就是对象的实例化。 初始化（Initialization）：new 会调用构造器 Bird(\u0026quot;eagle\u0026quot;)，构造器会初始化前面创建的新对象。  In Use 若对象被至少一个强引用持有，它就处于使用中（In Use）状态。JDK 1.1.x 中的所有引用都是强引用，JDK 1.2 则引入了三种其它的引用：软引用（soft reference）、弱引用（weak reference）和虚引用（phantom reference），三种引用的强度依次减弱。下面这段代码展示了创建对象并将其赋值给一些变量的过程：\npublic class CatTest { static Vector catList = new Vector(); static void makeCat() { Object cat = new Cat(); catList.addElement(cat); } public static void main(String[] arg) { makeCat(); // do more stuff  } } 下图显示了 makeCat 方法返回前，JVM 中对象的引用关系。此时，有两个引用指向了 Cat 对象：\n当 makeCat 方法返回后，它的栈帧与方法内声明的所有临时变量都会被移除。如此一来，指向 Cat 对象的引用就只有静态变量 catList 了，它通过 Vector 间接指向 Cat 对象。\nInvisible 即使指向对象的强引用依然存在，但如果这些强引用对程序来说是 不可访问的，对象就会进入不可见状态（invisible）。并不是所有的对象都会进入这个状态，下面代码片段展示了一个不可见对象：\npublic void run() { try { Object foo = new Object(); foo.doSomething(); } catch (Exception e) { // whatever  } while (true) { // loop forever  // do stuff  } } 以上代码创建了 foo 这个对象，当 try 语句块执行完成时，foo 就变得不可访问了（foo 只可在try 语句块内被访问，JLS11 中的 Scope of a Declaration 部分对局部变量的作用域进行了详细说明）。在 run 方法返回之前，foo 都是强引用，并且是 GC Root，所以不能被回收，因此可能导致内存泄漏。在这种情况下，我们必须显式地将 foo 设置为 null，才能保证垃圾回收。\nUnreachable 当不存在从 GC Root 到对象的强引用（链）时，对象就处于不可达（unreachable）状态。处于不可达状态的对象就可以被回收了，但这并不意味着对象会被立即回收，JVM 可以在任何需要的时候进行回收操作。\n循环引用并不一定会导致内存泄漏，举个例子：\npublic void buildDog() { Dog newDog = new Dog(); Tail newTail = new Tail(); newDog.tail = newTail; newTail.dog = newDog; } 下图展示了 buildDog 方法返回前对象的引用图。方法返回前，newDog 和 newTail 都是栈的局部变量，并且互相引用，它们都是 GC Root：\n下图展示了 buildDog 方法返回后的对象引用图。这时，由于栈帧弹出，Dog 和 Tail 虽然相互引用，但是它们当中任何一个对 GC Roots 来说都是不可达的，所以它们可以被回收。\nCollected 当 GC 在识别出对象处于不可达状态，并做好了对该对象的内存空间进行重新分配的准备之后，对象就进入被收集（Collected）状态。如果对象没有 finalize 方法，它就会直接进入终结（Finalized）状态。若对象有 finalize 方法（具有 finalizer），并且没有执行过，finalize 方法会被执行（finalize 方法只会被执行一次）。\n注：finalize 方法在 JDK9 中已被弃用，因为它可能带来性能问题，导致死锁、资源泄漏等。\nFinalized 若对象具有 finalize 方法，如果对象在 finalize 方法被执行后仍然是不可达的，对象的状态就是已终结（Finalized）。处于终结状态的对象正等待被回收。虽然 finalizer 的执行时机由 JVM 决定，但是有一点可以肯定：finalizer 会延长对象的生命周期，甚至让对象起死回生。\n周志明老师的《深入理解Java虚拟机（第3版）》中就给出了一段对象自我拯救的代码：\n/** * 此代码演示了两点： * 1.对象可以在被GC时自我拯救。 * 2.这种自救的机会只有一次，因为一个对象的finalize()方法最多只会被系统自动调用一次 * @author zzm */ public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() { System.out.println(\u0026#34;yes, i am still alive :)\u0026#34;); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\u0026#34;finalize mehtod executed!\u0026#34;); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws Throwable { SAVE_HOOK = new FinalizeEscapeGC(); //对象第一次成功拯救自己  SAVE_HOOK = null; System.gc(); // 因为Finalizer方法优先级很低，暂停0.5秒，以等待它  Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\u0026#34;no, i am dead :(\u0026#34;); } // 下面这段代码与上面的完全相同，但是这次自救却失败了  SAVE_HOOK = null; System.gc(); // 因为Finalizer方法优先级很低，暂停0.5秒，以等待它  Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\u0026#34;no, i am dead :(\u0026#34;); } } } 运行结果：\nfinalize mehtod executed ! yes,i am still alive : ) no,i am dead : ( 由于 finalize 方法只会被系统自动调用一次，所以对象第一次自救成功，第二次却失败了。JLS11 的 Finalization of Class Instances 部分对 Finalization 进行了详细的介绍，但考虑到它已经被 JDK 弃用，我们还是跳过吧。\nDeallocated 释放（Deallocated）状态是 GC 过程最后一步中对象的状态。在经过以上六个阶段之后，如果对象依然不可达，那么它就可以被释放了。何时以及如何释放对象所占内存依然是由 JVM 决定的。\n小结 最后，借网上的一张图来结束本文，感谢原作。\n本文其实只涉及对象生命周期的一个宏观步骤，实际的对象的一生会更加复杂。对象是类的实例，对于一个新生对象，JVM 需要找到对象所属的类，如果类没有加载，还需要先进行类加载，然后为对象分配内存空间，初始化对象的属性为零值，对对象进行必要的设置……经过这一系列步骤，对象才得以诞生。随后，为了使用对象，我们需要进行对象的访问定位。用完之后，对象等待被垃圾回收，为其它新生对象留出宝贵的内存空间。对象的一生可不简单。\n参考资料  The Truth About Garbage Collection. 周志明. 深入理解Java虚拟机（第3版）. 机械工业出版社, 2019. Creating Objects.  ","href":"/java/jvm/java_object_lifecycle/","title":"Java 对象的一生"},{"content":"","href":"/tags/jvm/","title":"JVM"},{"content":"JDK 1.2 之后，Java 将引用分为强引用（Strongly Reference）、软引用（Soft Reference）、弱引用（Weak Reference）和虚引用（Phantom Reference）四种，这四种引用的强度依次减弱，它们与 Java 的对象回收有着很大的关系。\n A reference object encapsulates a reference to some other object so that the reference itself may be examined and manipulated like any other object. Three types of reference objects are provided, each weaker than the last: soft, weak, and phantom. Each type corresponds to a different level of reachability.\n 这段话已经描述得很清楚了：Java 中有三种引用对象（reference object），它们封装了一些其它的对象，从而让我们可以像操作其它对象一样操作引用本身，不同引用对象的可达性不同。\n除开与 Finalization 有关的类，下图展示了 java.lang.ref 包中的类结构：\nReference 对象用于维持对其它对象的引用，但 GC 仍然可以回收这些被引用的其它对象。当 GC 决定回收某个引用对象关联的对象时，它会将对应的引用对象放入与之关联的引用队列（ReferenceQueue）中，这样我们就可以得到对象被回收的通知了。\n对象可达性 在进行垃圾回收之前，第一件事情就是找出垃圾。引用计数算法和可达性分析算法是两种常见的判断垃圾的方法，JVM 是通过分析对象的可达性来判断是否应该将其回收的。堆中的对象是可以相互引用的，根据对象类型的不同，引用的直接表现形式也不同。对于普通对象而言，引用就是对象的字段，而对于数组而言，引用就是数组的元素。此外，还有一些隐藏的引用，例如：每个对象实例都包含一个指向其类型（即实例对应的 Class）的引用，而每个 Class 又包含一个指向加载它的 ClassLoader 的引用。\n我们可以将对象之间的引用关系看作是一张有向图，图中的节点就是对象，而边就是对象间的引用。从源对象出发，若存在一条通往目标对象的路径，则目标对象对源对象来说是可达的，反之目标对象对源对象而言就是不可达的。\nGC Roots 可达性分析算法的基本思路是通过一系列被称为“GC Roots”的根对象作为起始节点集，从这些结点开始，根据引用关系向下搜索，搜索路径即为引用链（Reference Chain），如果没有任何路径可以到达 某个对象，则说明这个对象不再被使用，即为垃圾。\n在 Java 中，GC Root 是一个可以 从堆外访问 的对象，通常包括以下对象：\n JVM 栈中引用的对象，比如线程方法的参数、局部变量、临时变量等； 方法区中静态属性引用的对象，比如 Java 类的引用类型的静态变量； 方法区中常量引用的对象，比如字符串常量池中的引用； Native 方法栈中 JNI 引用的对象； JVM 内部的引用，比如基本数据类型对应的 Class 对象、一些常驻的异常对象（NullPointerException、OutOfMemeryError 等）、系统类加载器； 所有被同步锁（synchronized）持有的对象； 反映 JVM 内部情况的 JMXBean、JVMTI 中注册的回调、Native 代码缓存等；  Eclipse 的内存分析工具列举出了各种具体的 GC Root，感兴趣的同学可以进一步了解。\n对象的五种可达性 Java 中的对象可达性有五种不同的程度：强可达（strongly reachable）、软可达（softly reachable）、弱可达（weakly reachable）、虚可达（phantom reachable）和不可达（unreachable）。以下关于可达性的描述，取自 java.lang.ref 包的文档：\n Going from strongest to weakest, the different levels of reachability reflect the life cycle of an object. They are operationally defined as follows:\n  An object is strongly reachable if it can be reached by some thread without traversing any reference objects. A newly-created object is strongly reachable by the thread that created it.\n  An object is softly reachable if it is not strongly reachable but can be reached by traversing a soft reference.\n  An object is weakly reachable if it is neither strongly nor softly reachable but can be reached by traversing a weak reference. When the weak references to a weakly-reachable object are cleared, the object becomes eligible for finalization.\n  An object is phantom reachable if it is neither strongly, softly, nor weakly reachable, it has been finalized, and some phantom reference refers to it.\n  Finally, an object is unreachable, and therefore eligible for reclamation, when it is not reachable in any of the above ways.\n   强引用 强引用指程序代码中普遍存在的引用赋值（比如Object strongReference = new Object()这种关系）。在任何情况下，只要强引用的关系还在，垃圾收集器就不会回收被引用的对象。当内存不足时，JVM 宁愿抛出 OutOfMemorryError，也不会回收具有强引用的对象。当一个强引用对象不再使用时，若要让 GC 回收它，我们需要让它变得不可达（实际上，GC 也只回收不可达对象）。\nvoid test() { Object strongReference = new Object(); // 强引用  strongReference = null; // 对象可以被 GC 回收了 } 在上面这段代码中，new Object() 会在堆中创建一个对象，而指向这个对象的是栈中的引用 strongReference。当方法调用完成，方法栈退出，strongReference 不复存在，这个对象就可以被回收了。若 strongReference 是全局变量，则需要手动将其设置为 null，去除其对对象的引用，对象才能被回收。\n软引用 被软引用关联对象即软可达对象，它们是一些还有用但非必须的对象，在系统将要发生内存溢出之前，会被二次回收，如果这次回收之后的内存依然不够，JVM 才会抛出 OutOfMemoryError。JVM 保证在抛出 OutOfMemoryError 之前回收所有被软引用关联的软可达对象。JDK 1.2 之后提供了 SoftReference 类来表示软引用。\n Soft references are most often used to implement memory-sensitive caches.\n 创建软引用对象的方法有两种。\n第一种是创建不与任何引用队列关联的软引用对象：\nObject referent = new Object(); SoftReference\u0026lt;Object\u0026gt; softReference = new SoftReference\u0026lt;\u0026gt;(referent); 我们也可以使用带 ReferenceQueue 的构造函数，如果软引用所引用的对象被回收，JVM 就会自动将这个软引用加入到与之关联的引用队列中：\nReferenceQueue\u0026lt;Object\u0026gt; referenceQueue = new ReferenceQueue\u0026lt;\u0026gt;(); SoftReference\u0026lt;Object\u0026gt; softReference = new SoftReference\u0026lt;\u0026gt;(obj, referenceQueue); Reference 类提供的 get 和 clear 方法分别可以用来获取和重置对应的引用：\nObject referent = softReference.get(); softReference.clear(); Object referent2 = softReference.get(); // null 实际上，由于对象可能被 GC 回收，所以在进行下一步操作时，我们需要对 get 方法返回的 referent 进行检查：\nObject referent = softReference.get(); if (referent != null) { // 对象还没有被 GC 回收  // 用作缓存时，可直接返回对象 } else { // 对象已经被 GC 回收  // 用作缓存时，需要重新实例化引用对象 } 弱引用 被弱引用关联的对象即弱可达对象，它描述的也是非必须对象，其强度比软引用更弱。被弱引用关联的对象，只能生存到下一次垃圾回收。当垃圾收集器开始工作，无论当前内存是否不足，都会回收掉被弱引用关联的对象。JDK 1.2 之后提供了 WeakReference 类来表示弱引用。\n弱引用与软引用之间最大的区别在于：被弱引用指向的对象只在两次 GC 之间存活，而被软引用指向的对象在 JVM 内存紧张的时候才被回收，它是可以经历多次 GC 的。\n Weak references are most often used to implement canonicalizing mappings.\n 创建弱引用对象的方法也有两种，不关联任何引用队列的和关联引用队列的：\nObject referent = new Object(); ReferenceQueue referenceQueue = new ReferenceQueue(); WeakReference weakReference1 = new WeakReference\u0026lt;\u0026gt;(referent); WeakReference weakReference2 = new WeakReference\u0026lt;\u0026gt;(referent, referenceQueue); 弱引用的其它使用与软引用类似，不再赘述。\nJDK 提供了一个基于弱引用实现的 HashMap 集合—— WeakHashMap，其中的 Entry 继承了 WeakReference，Entry 中使用弱引用指向 Key，使用强引用指向 Value。当没有强引用指向 Key 的时候，Key 可以被 GC 回收。当再次操作 WeakHashMap 的时候，就会遍历关联的引用队列，从 WeakHashMap 中清理掉相应的 Entry。\n虚引用 虚引用是最弱的一种引用关系，又称“幽灵引用”或“幻影引用”。一个对象是否有虚引用，完全不会对其生存时间构成影响，也无法通过虚引用来获取一个对象的实例。JDK 1.2 之后提供了 PhantomReference 类来表示虚引用。虚引用主要用来跟踪对象被 GC 回收的活动。 虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列一起使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。\nObject referent = new Object(); ReferenceQueue referenceQueue = new ReferenceQueue(); // 创建虚引用时，必须和一个引用队列相关联 PhantomReference phantomReference = new PhantomReference\u0026lt;\u0026gt;(referent, referenceQueue); 弱引用的其它使用与软引用类似，不再赘述。\n参考资料  周志明. 深入理解Java虚拟机（第3版）. 机械工业出版社, 2019. Package java.lang.ref.  ","href":"/java/java_lang/references_and_reachability_in_java/","title":"Java 中的引用与对象可达性"},{"content":"","href":"/tags/jdbc/","title":"JDBC"},{"content":"JDBC（Java DataBase Connectivity）是 Java 程序与关系型数据库交互的统一 API，它由两部分 API 组成：\n 面向 Java 开发者的 Java API，这一部分 API 独立于各个数据库产品的接口规范，是标准又统一的 Java　API。 面向数据库驱动程序开发者的 API，由数据库厂商实现，用于连接具体的数据库产品。  使用 JDBC 操作数据库的核心步骤 在实际开发 Java 程序时，我们可以通过 JDBC 连接到数据库，完成各种数据库操作。以下就是执行 SELECT 语句时发生的 JDBC 操作：\n 注册数据库驱动类，给出数据库连接信息（数据库地址、用户名、密码等） 创建 Connection 连接到数据库（调用 DriverManager.getConnection() 方法） 创建 Statement 对象（调用 Connection 的 createStatement() 或 prepareStatement() 方法） 通过 Statement 对象执行 SQL，得到 ResultSet 对象（查询结果集） 从 ResultSet 中读取数据 关闭 ResultSet、Statement 及 Connection 对象  数据库连接池 为什么要使用数据库连接池？数据库连接是整个服务中比较珍贵的资源之一，因为建立数据库连接涉及鉴权、握手等一系列网络操作。使用池化技术缓存数据库连接带来的好处还有很多，例如：\n 实现连接重用，从而提高系统的响应速度 控制数据库连接数量上限，防止连接过多造成数据库假死 统一连接管理，避免连接泄漏  连接池的连接数量上限一定要根据实际情况仔细选取。如果设置得过大，可能导致数据库因连接过多而假死或崩溃，从而影响服务的可用性。如果设置得过小，则可能无法让数据库达到最佳性能，造成资源浪费。\nORM 框架 ORM 框架的核心功能是：根据配置（一般是配置文件或者 Java 注解）实现对象模型（Java 程序）与关系模型（数据库）之间的映射。\n常见的 ORM 框架 Hibernate Hibernate 是一个老牌的 ORM 框架，在 Java 生态中久负盛名。它支持多种异构数据的持久化，支持全文搜索（Hibernate Search）和数据校验（validation），还提供了 NoSQL 的解决方案（Hibernate OGM）。\n使用 Hibernate ORM 进行 Java 开发时，可以使用映射文件或注解定义 Java 类与数据库表之间的映射关系，使用到的映射关系文件后缀为 .hbm.xml。hbm.xml 映射文件将数据库表与 Java 对象进行关联，让数据库表中的每一行记录都可以被转换为一个对应的 Java 对象。这样一来，Java 开发者只需要使用面向对象的思维就可完成数据库的设计。\n除了完成对象模型与关系模型的双向映射外，Hibernate 还可以帮助我们屏蔽不同数据库之间 SQL 的差异（通过数据库方言实现）。在开发过程中，我们只需要使用 Hibernate 提供的 API 就可以完成数据库操作，并不需要直接编写 SQL 语句，因为 Hibernate 封装了数据库层面的全部操作。Hibernate 提供的 Criteria API 是一套面向对象的 API，使用它操作数据库时，我们只需要关注这套 API 本身以及返回的 Java 对象，并不需要考虑数据库底层实现的差异。\n除了 Criteria API 之外，Hibernate 还提供了一套面向对象的查询语言——HQL（Hibernate Query Language），它的语句结构与 SQL 语句的结构十分类似，二者的本质区别在于：前者是面向对象的，后者是面向关系型数据库的。在使用 HQL 时，也不需要考虑底层数据库的差异，Hibernate 会自动将 HQL 转换为合法的 SQL 语句。\n不管是 Criteria API，还是 HQL，最终二者都会转化为 SQL，由于 SQL 是生成出来的，所以复杂情况下生成的 SQL 可能会非常难于理解。\nHibernate 默认提供了一级缓存（默认开启）和二级缓存（需要配置开启），支持延迟加载，可以避免无效查询。\n不过，Hibernate 无法在面向对象模型中找到数据库中所有概念的映射，例如索引、函数、存储过程等。\nJPA JPA（Java Persistence API） 是 JDK5.0 后提出的 Java 持久化规范，目的在于整合市面上已有的 ORM 框架。JPA 本身只是一个规范，没有提供具体的实现，Hibernate、OpenJPA、EclipseLink 等都是 JPA 规范的具体实现。JPA 由三个核心部分组成：ORM 映射元数据、操作实体对象的 API 和面向对象的查询语言 JPQL，三者的作用和 Hibernate 中的 hbm.xml、Criteria API 和 HQL 非常类似。\n虽然很多 ORM 框架都实现了 JPA 规范，但是它们各自在 JPA 的基础上又有所修改，这就导致我们在使用 JPA 的时候，无法进行底层 ORM 框架的无缝切换，但是 Spring Data JPA 可以帮我们实现无缝切换。\nMyBatis 另一个流行的 ORM 框架是 MyBatis，它的一个重要功能是帮助 Java 开发者封装重复性的 JDBC 代码——通过 Mapper 映射配置文件及相关注解，将 ResultSet 结果映射为 Java 对象。很多人将 Hibernate 与 Mybatis 进行比较，认为前者是一个全自动的 ORM 框架，后者是一个半自动的 ORM 框架。Mybatis 并没有实现 JPA，但是相对 Hibernate 以及各类 JPA 框架的实现来说，它更加灵活、轻量级和可控。\n此外，Mybatis 还提供了一个非常强大的功能——动态 SQL。这一特性可以帮助我们更好地处理可能动态变化的复杂查询条件，减少出错率。\n实际工作中对持久层框架的选择 从性能角度来看，Hibernate、Spring Data JPA 对 SQL 语句的掌握、SQL 手工调优、多表连接查询等方面不如 MyBatis 直接使用原生 SQL 语句那么方便、高效。\n从可移植性角度来看，Hibernate 帮助我们屏蔽了底层数据库方言的差异，Spring Data JPA 帮助我们屏蔽了 ORM 的差异，而 MyBatis 由于直接手工编写原生 SQL，会与具体的数据库完全绑定。\n从开发效率来看，Hibernate、Spring Data JPA 处理中小型项目的效率略高于 MyBatis，但这个主要要看需求和开发者的技术栈。\n参考资料  Mybatis. Hibernate. JDBC 4.2 Specification. Spring Data JPA.  ","href":"/java/jdbc/jdbc/","title":"JDBC"},{"content":"","href":"/tags/mybatis/","title":"MyBatis"},{"content":"之所以放在 Notebook 中，是因为本文的绝大部分内容来自某网课，做笔记备忘，便于以后碰到相关问题更快地定位和排查问题。\nMyBatis 三层架构 MyBatis 的整体架构分为三层，分别是：基础支撑层、逻辑处理层 和 接口层。\n基础支撑层 基础支撑层为整个 MyBatis 框架提供最基础的功能，分为九个基础模块，每个模块都具有各自不同的能力。\n类型转换模块  在使用 JDBC 操作数据库时，我们面临着三个不同层次的数据类型差异——数据库自己的数据类型、JDBC 定义的数据类型和 Java 数据类型。JDBC 在中间，作为规范标准，它可以屏蔽底层数据的差异，但 JDBC 类型与 Java 类型并不是一一对应的，所以需要进行适当的类型转换。\n JDBC Spec 4.2 中定义了 JDBC 类型到 Java 类型的映射关系、Java 类型到 JDBC 类型的映射关系等等。\n类型转换模块实现了 MyBatis 中定义的 JDBC 类型与 Java 类型之间的相互转换：\n 当 MyBatis 在将 SQL 模板与用户传入的参数相绑定（在我们使用 PreparedStatement 执行 SQL 之前，需要手动调用 setInt()、setString() 等方法绑定参数）时，类型转换模块会将 Java 类型数据转换为 JDBC 类型数据。 当取得查询结果后，类型转换模块会将 ResultSet 中的 JDBC 类型数据转换为 Java 类型数据。  此外，类型转换模块还实现了别名的功能。我们可以在 mybatis-config.xml 配置文件中使用 \u0026lt;typeAlias\u0026gt; 标签为 Java 类的完整名称定义相应的别名。然后在后续编写 SQL 语句、定义 \u0026lt;resultMap\u0026gt; 的时候，就可以直接使用这些别名替代相应的 Java 类名，这样就非常易于代码的编写和维护。TypeAliasRegistry 是维护别名配置的核心实现，它提供了别名注册、别名查询等基本功能。\nTypeHandler MyBatis 中的类型转换模块包含众多类型转换器，这些类型转换器都有一个父接口——TypeHandler。整个转换器的层次结构如下（为了简单，图中省略了不少类型转换器）：\n其中，BaseTypeHandler 不仅实现了一些 TypeHandler 的公共逻辑，还实现了抽象类 TypeReference。\n除了这些默认的 TypeHandler 之外，我们还可以在 mybatis-config.xml 中使用标签配置自定义的 TypeHandler 实现，也可以在定义 Mapper.xml 的时候指定 typeHandler 属性。不管是哪种方式，MyBatis 都会在初始化的过程中，获取所有已知的 TypeHandler，创建对应的实例并注册到 TypeHandlerRegistry 中，由 TypeHandlerRegistry 统一管理所有的 TypeHandler 实例。\n日志模块 MyBatis 提供的日志模块可以很方便地集成各种第三方 Java 日志框架，比如 Log4j、Log4j2、slf4j、java.util.logging 等。由于这些日志框架来源于不同的开源组织，给用户暴露的接口也不尽相同。所以 MyBatis 使用了 适配器模式（设计模式读书笔记之适配器模式）对第三方日志框架接口进行了统一。\nMyBatis 自己定义了一个 Log 接口，然后使用适配器模式针对不同的日志框架进行了适配，将第三方日志框架的日志接口转化为了它自己的 Log 接口，这样就成功集成了第三方日志框架的日志打印功能。\nLog 接口及相关适配器实现均位于 org.apache.ibatis.logging 包中。整体结构如下：\n其中，LoggFactory 负责创建 Log 对象，这个工厂类中有一段静态代码块，会依次加载各个第三方日志框架的适配器。\nBinding 模块 在使用 MyBatis 时，我们无须编写 Mapper 接口的具体实现，Binding 模块会自动生成 Mapper 接口的动态代理对象，通过代理对象可以执行关联 Mapper.xml 文件（或 Mapper 接口中的注解）中的数据库操作。Binding 模块的核心类如下：\nMapperRegistry（在 MyBatis 初始化过程中构造） 主要负责统一维护 Mapper 接口以及这些 Mapper 对应的代理对象工厂 MapperProxyFactory。MapperProxyFactory 通过 JDK 动态代理的方式创建 Mapper 接口的代理对象 MapperProxy，MapperProxy 封装核心代理逻辑，将拦截到的目标方法委托给 MapperMethod 处理。\nMapperProxy MapperProxy 是生成 Mapper 接口代理对象的关键，它实现了 InvocationHandler 接口。Mapper 代理对象的执行入口正是 MapperProxy 的 invoke() 方法。该方法内部会针对所有的非 Object 方法调用 cachedInvoker() 方法获取对应的 MapperMethodInvoker 对象，并调用其 invoke() 方法执行代理逻辑以及目标方法。\n在 cachedInvoker() 方法中，首先会查询 methodCache 缓存（类型为 Map\u0026lt;Method, MapperMethodInvoker\u0026gt;），如果查询的方法为 default 方法，则会根据当前使用的 JDK 版本，获取对应的 MethodHandle（相关文档：MethodHandle） 并封装成 DefaultMethodInvoker 对象写入缓存备用；如果查询的方法是非 default 方法，则创建 PlainMethodInvoker 对象写入缓存备用。\nDefaultMethodInvoker 与 PlainMethodInvoker 的内部实现稍有不同，前者通过 MethodHandle 完成调用，而后者通过 MapperMethod 完成调用。\nMapperMethod MapperMethod 记录了 Mapper 接口中的对应方法，也是最终执行 SQL 语句的地方（execute()方法），execute() 方法会根据要执行的 SQL 语句的具体类型执行 SqlSession 的相应方法完成数据库操作。\n在 MapperMethod 中，command 字段（SqlCommand 类型，它是 MapperMethod 的一个内部类）维护了关联 SQL 语句的相关信息。先简单看一下 SqlCommand 类：\npublic static class SqlCommand { private final String name; // 关联 SQL 语句的唯一标识  private final SqlCommandType type; // 关联 SQL 语句的操作类型  // ... } MyBatis 中的 SQL 语句有五种不同的操作类型：\npublic enum SqlCommandType { UNKNOWN, INSERT, UPDATE, DELETE, SELECT, FLUSH } MapperMethod 中的 method 字段（MethodSignature 类型）则是维护了 Mapper 接口中方法的相关信息。它依赖 ParamNameResolver 这个解析方法参数列表的工具类。这里有必要单独说一下这个工具类。\nParamNameResolver 中有一个 name 字段记录了各个参数参数在参数列表中的位置以及参数名称，其中 key 是参数在参数列表中的位置索引， value 为参数的名称。\nprivate final SortedMap\u0026lt;Integer, String\u0026gt; names; 我们可以通过 @Param 注解指定一个参数名称，如果没有指定，则默认使用参数列表中的变量名称作为其名称，这与 ParamNameResolver 的 useActualParamName 有关，它是一个全局配置。如果将其配置为 false，则使用参数的下标索引作为其名称。此外，names 集合会跳过类型为 RowBounds 和 ResultHandler 类型的参数，如果使用下标作为索引作为参数名称的话，这时就会在 names 集合中出现 KV 不一致的情况。例如：\naMethod(@Param(\u0026#34;M\u0026#34;) int a, @Param(\u0026#34;N\u0026#34;) int b) -\u0026gt; {{0, \u0026#34;M\u0026#34;}, {1, \u0026#34;N\u0026#34;}} aMethod(int a, int b) -\u0026gt; {{0, \u0026#34;0\u0026#34;}, {1, \u0026#34;1\u0026#34;}} aMethod(int a, RowBounds rb, int b) -\u0026gt; {{0, \u0026#34;0\u0026#34;}, {2, \u0026#34;1\u0026#34;}} 资源加载模块 资源加载模块主要是对类加载器进行封装，确定类加载器的使用顺序，并提供了加载类文件以及其他资源文件的功能。\n数据源模块 数据源是持久层框架中最核心的组件之一，MyBatis 不仅提供了一套默认的数据源实现，还能够很方便地集成第三方数据源。在 Java 中，数据源这个抽象由 javax.sql.DataSource 接口表示，MyBatis 自带的数据源实现就实现了该接口。MyBatis 默认实现了两种类型的数据源——UnpooledDataSource 和 PooledDataSource。\n此外，对于这两种不同的数据源实现，MyBatis 还使用工厂方法模式（设计模式读书笔记之工厂方法模式）提供了相应的工厂类：\n使用工厂方法模式的好处在于，如果需要扩展新的数据源，只需要添加相应的 DataSourceFactory 接口实现类即可。这里的 DataSource 即工厂方法模式中的 Product。DataSourceFactory 定义了两个方法：setProperties() 用于配置数据源属性，getDataSource() 用于返回数据源对象。从上图可以看出，PooledDataSourceFactory 并没有直接实现 DatasourceFactory 接口，而是直接继承了 UnpooledDataSourceFactory 类。\n缓存管理模块 缓存是优化数据库性能的常用手段之一，正确使用缓存可以将一部分数据库请求拦截在缓存层，减少数据库的压力，提升系统性能。\nMyBatis 就提供了许多不同策略的缓存，这些缓存分为两个级别：一级缓存和二级缓存，它们都实现了 Cache 接口。\nCache 接口定义了 MyBatis 缓存最基本、最核心的行为，其中的核心方法主要是 putObject()、getObject() 和 removeObject()，分别用来添加、查询和删除缓存数据。MyBatis 在实现缓存模块时采用了装饰器模式（设计模式读书笔记之装饰器模式），其中 PerpetualCache 扮演的正是装饰器模式中的 ConcretComponent 的角色，它实现了 Cache 接口缓存的基本能力。除 PerpetualCache 以外的其它 Cache 接口实现都是装饰器，扮演的是装饰器模式中的 ConcreteDecorator 的角色，不同装饰器提供了不同的功能扩展。\n例如，BlockingCache 在原有缓存实现之上添加了阻塞线程的特性，FifoCache 添加了先进先出的特性，LruCache 添加了最近最少适用的淘汰机制等等。\n解析器模块 MyBatis 中需要解析的配置文件分为两部分：一个是 mybatis-config.xml 配置文件，另一个是 Mapper.xml 配置文件。这两个文件都是由 MyBatis 的解析器模块进行解析的，其中主要是依赖 XPath 实现 XML 配置文件以及各类表达式的高效解析。\n事务管理模块 MyBatis 对数据库中的事务进行了一层简单的抽象，提供了简单易用的事务接口和实现。在 MyBatis 中，数据库事务被抽象为 Transaction 接口，它定义了提交事务、回滚事务以及获取底层数据库连接的方法。TransactionFactory 则是用于创建 Transaction 对象的工厂接口。\n反射工具模块 MyBatis 在 Java 反射的基础上进行了封装，为上层使用方提供了更加灵活、方便的 API 接口。反射工具箱的具体代码位于 org.apache.ibatis.reflection 包中。Reflector 是整个模块的基础，在使用反射模块操作一个 Class 之前，MyBatis 会将 Class 封装成一个 Reflector，为了提高反射执行的效率，Reflector 中缓存了 Class 的元数据信息。\nReflector 将类的属性与方法等信息记录在自己的核心字段中，它的核心字段如下所示：\nReflector 的构造函数以 Class 为参数，在构造方法内部会解析传入的 Class 对象，填充以上字段。\n同样，为了提高 Reflector 的初始化速度，MyBatis 提供了 ReflectorFactory 工厂接口。该接口的默认实现 DefaultReflectorFactory 在内存中通过 ConcurrentHashMap\u0026lt;Class\u0026lt;?\u0026gt;, Reflector\u0026gt;（即 reflectorMap）对 Reflector 对象进行了缓存，该接口的核心方法 findForClass(Class\u0026lt;?\u0026gt; type) 就会根据传入 Class 查找 reflectorMap 缓存，若查找成功，则直接返回，否则创建相应的 Reflector 对象并放入缓存，以便下次使用。\nObjectFactory 是 MyBatis 中的对象反射工厂接口，它提供了两个不同的 create 方法，我们可以用它来创建指定类型的对象。DefaultObjectFactory 是此接口的默认实现，底层会利用反射根据入参列表选择合适的构造函数实例化对象。我们可以在 mybatis-config.xml 中配置自定义的 ObjectFactory 接口的扩展实现类，达到自定义功能扩展的目的。\n反射模块内还提供了几个用于解析属性的工具类：\n PropertyTokenizer：负责解析由 . 和 [] 构成的表达式，支持对嵌套多层表达式的处理 PropertyCopier 用于属性拷贝 PropertyNamer 可以将方法名转换为属性名，亦能检测一个方法是否为 setter 或 getter 方法  此外，反射模块还有 MetaClass，它封装了 Class 的元数据信息，以及 ObjectWrapper，它封装了对象的元数据信息。\n核心处理层 MyBatis 的核心实现，涉及 MyBatis 的初始化以及执行一条 SQL 语句的全流程。\n配置解析 MyBatis 中有三处可以添加配置信息的地方：mybatis-config.xml 配置文件、Mapper.xml 配置文件，以及 Mapper 接口中的注解信息。MyBatis 会在初始化的时候加载这些配置信息，并在解析完成之后，将解析得到的配置到对象保存到 Configuration 对象中。\n配置解析过程中用到了设计模式中的建造者模式（设计模式读书笔记之建造者模式）。\nmybatis-config.xml 解析 MyBatis 初始化的第一个步骤就是加载和解析 mybatis-config.xml 这个全局配置文件。入口位于 XMLConfigBuilder 这个 Builder 对象，它由 SqlSessionFactoryBuilder.build() 方法创建。\nXMLConfigBuilder 会将 mybatis-config.xml 解析为对应的 Configuration 全局配置对象，然后 SqlSessionFactoryBuilder 会根据得到的 Configuration 对象创建一个 DefaultSqlSessionFactory 供上层使用。\nXMLConfigBuiler 继承自 BaseBuilder 抽象类，该类的继承关系如下：\nBaseBuilder 提供了三方面的能力：\n 关联 Configuration 对象 解析别名 解析 TypeHandler  mybatis-config.xml 文件的解析是由 XMLConfigBuilder.parse() 方法触发的，其中的 parseConfiguration() 方法定义了解析 mybatis-config.xml 的完整流程。其核心步骤如下：\nMapper.xml 解析 mybatis-config.xml 中可以定义多个 \u0026lt;mapper\u0026gt; 标签，指定 Mapper 配置文件的位置。MyBatis 会为每个 Mapepr.xml 映射文件都创建一个 XMLMapperBuilder 实例，从而完成相关解析工作。\nSQL 解析与 scripting 模块 动态 SQL 是 MyBatis 最大的亮点，这个模块负责动态生成 SQL。具体来说，它会根据运行时用户传入的实参解析动态 SQL 中的标签，并形成 SQL 模板，然后处理 SQL 模板中的占位符，用实参填充占位符，得到真正可执行的 SQL 语句。\nscripting 模块是 MyBatis 中动态生成 SQL 的核心模块，它会根据用户传入的实参解析动态 SQL 中的标签，形成 SQL 模板，然后用运行时的实参填充模板中的占位符，得到真正可以在数据库中执行的 SQL 语句。\nDynamicContext MyBatis 解析一条动态 SQL 语句的整个流程可能会非常长，其中涉及多层方法的调用、方法的递归、复杂的循环等，其中产生的中间结果需要有一个地方进行存储，那就是 DynamicContext 上下文对象。\nDynamicContext 有两个核心属性：一个是 sqlBuilder 字段（StringJoiner 类型），用来记录解析之后的 SQL 语句；另一个是 bindings 字段（ContextMap 类型），用来记录上下文中的一些 KV 信息。ContextMap 是 DynamicContext 定义的一个内部类，用来记录运行时用户传入的、用来替换“#{}”占位符的实参。\nSqlNode MyBatis 在处理动态 SQL 语句的时候，会将动态 SQL 标签解析为 SqlNode 对象，多个 SqlNode 对象通过组合模式（设计模式读书笔记之组合模式）组成树形结构供上层使用。SqlNode 接口充当的就是组合模式中的 Component 的角色，它的实现类非常多，很多都对应着一个动态 SQL 标签（组合模式中的 Leaf 角色），少数实现类扮演着组合模式中的 Composite 角色（比如 MixedSqlNode）。\n StaticTextSqlNode用于表示非动态的 SQL 片段，其中维护了一个 text 字段（String 类型），用于记录非动态 SQL 片段的文本内容。 MixedSqlNode 在整个 SqlNode 树中充当了树枝节点，也就是扮演了组合模式中 Composite 的角色，其中维护的 List\u0026lt;SqlNode\u0026gt; 集合了记录 MixedSqlNode 下所有的子 SqlNode 对象。 TextSqlNode 抽象了包含 “${}”占位符的动态 SQL 片段，它通过一个 text 字段（String 类型）来记录包含“${}”占位符的 SQL 文本内容。 IfSqlNode 对应了动态 SQL 语句中的 \u0026lt;if\u0026gt; 标签，MyBatis 的 标签的 test 属性可以指定一个表达式，当表达式成立时，\u0026lt;if\u0026gt; 标签内的 SQL 片段才会出现在完整的 SQL 语句中。 TrimSqlNode 对应 MyBatis 动态 SQL 语句中的 \u0026lt;trim\u0026gt; 标签。在使用 \u0026lt;trim\u0026gt; 标签的时候，我们可以指定 prefix 和 suffix 属性添加前缀和后缀，也可以指定 prefixesToOverrides 和 suffixesToOverrides 属性来删除多个前缀和后缀（使用“|”分割不同字符串）。 ForeachSqlNode 对应 MyBatis 动态 SQL 语句中的 \u0026lt;foreach\u0026gt; 标签。我们可以在动态 SQL 语句中使用 \u0026lt;foreach\u0026gt; 标签对一个集合进行迭代。在迭代过程中，可以通过 index 属性值指定的变量作为元素的下标索引（迭代 Map 集合的话，就是 Key 值），使用 item 属性值指定的变量作为集合元素（迭代 Map 集合的话，就是 Value 值）。另外，我们还可以通过 open 和 close 属性在迭代开始前和结束后添加相应的字符串，也可以使用 separator 属性自定义分隔符。 ChooseSqlNode 对应 MyBatis 动态 SQL 语句中的 \u0026lt;choose\u0026gt; 标签。其内部的\u0026lt;when\u0026gt; 标签会被解析成 IfSqlNode 对象，\u0026lt;otherwise\u0026gt; 标签会被解析成 MixedSqlNode 对象。 VarDeclSqlNode 抽象了 \u0026lt;bind\u0026gt; 标签，其核心功能是将一个 OGNL 表达式的值绑定到一个指定的变量名上，并记录到 DynamicContext 中。  SqlSourceBuilder 动态 SQL 语句在被解析成 SqlNode 对象后，会经由 SqlSourceBuilder 进一步处理。SqlSourceBuilder 的核心操作主要有两个：\n 解析“#{}”占位符中携带的各种属性。 将 SQL 语句中的“#{}”占位符替换成“?”占位符，替换之后的 SQL 语句就可以提交给数据库进行编译了。  SqlSourceBuilder 完成了“#{}”占位符的解析和替换之后，会将最终的 SQL 语句以及得到的 ParameterMapping 集合封装成一个 StaticSqlSource 对象并返回。\nSqlSource 动态 SQL 经过 SqlNode 和 SqlSourceBuilder 之后，最终会由 SqlSource 进行最后的处理。SqlSource 接口中只定义了一个 getBoundSql() 方法，它控制着动态 SQL 语句解析的整个流程。它会根据从 Mapper.xml 映射文件（或注解）解析到的 SQL 语句以及执行 SQL 时传入的实参，返回一条可执行的 SQL。接口继承关系如下：\nDynamicSqlSource DynamicSqlSource 主要负责解析动态 SQL 语句以及“#{}”占位符。\n动态 SQL 的判断标准：如果某个 SQL 片段包含了未解析的“${}”占位符或动态 SQL 标签，则为动态 SQL 语句。但是，只包含“#{}”占位符的 SQL 并不是动态 SQL。\nRawSqlSource 与 DynamicSqlSource 不同，RawSqlSourc 处理的是非动态 SQL 语句，它解析 SQL 语句的时机是在初始化流程中。\nStaticSqlSource 无论是 DynamicSqlSource 还是 RawSqlSource，底层都依赖 SqlSourceBuilder 解析之后得到的 StaticSqlSource 对象。StaticSqlSource 中维护了解析之后的 SQL 语句以及“#{}”占位符的属性信息。\nSQL 执行 在 MyBatis 中，执行一条 SQL 的核心组件有 Executor、StatementHandler、ParameterHandler 和 ResultSetHandler。其中，Executor 会调用事务管理模块实现事务的相关通知，同时会通过缓存模块的一级缓存和二级缓存。SQL 语句的真正执行由 StatementHandler 实现，它会依赖 ParameterHandler 进行 SQL 模板的数据绑定，然后传到数据库执行，从数据库拿到 ResultSet。最后，ResultSetHandler 会将 ResultSet 映射为 Java 对象返回给调用方。下图展示了一条 SQL 执行的大致过程：\nExecutor MyBatis 中有多个 Executor 接口的实现类，如下图所示：\nBaseExecutor BaseExecutor 使用模板方法模式实现了 Executor 接口中的方法，其中，不变的部分是事务管理和缓存管理两部分的内容，由 BaseExecutor 实现；变化的部分则是具体的数据库操作，由 BaseExecutor 子类实现，涉及 doUpdate()、doQuery()、doQueryCursor() 和 doFlushStatement() 这四个方法。\nSimpleExecutor SimpleExecutor 是 Executor 接口最简单的实现。\nReuseExecutor 重用 Statement 对象是一种常见的优化手段，不仅可以减少 SQL 预编译开销，还能降低 Statement 对象的创建和销毁频率，这在一定程度上可以提升系统性能。ReuseExecutor 就实现了重用 Statement 的优化。\nReuseExecutor 中的 do*() 方法实现与 SimpleExecutor 实现完全一样，两者唯一的区别在于其中依赖的 prepareStatement() 方法：SimpleExecutor 每次都会创建全新的 Statement 对象，ReuseExecutor 则是先尝试查询 statementMap 缓存，如果缓存命中，则会重用其中的 Statement 对象。\nBatchExecutor 批处理是 JDBC 编程中的一种常见优化手段。不过，有一点需要特别注意：每次向数据库发送的 SQL 语句的条数是有上限的，如果批量执行的时候超过这个上限值，数据库就会抛出异常，拒绝执行这一批 SQL 语句，所以我们需要控制批量发送 SQL 语句的条数和频率。BatchExecutor 是用于实现批处理的 Executor 实现类。\nCachingExecutor CachingExecutor 是一个 Executor 装饰器实现，会在其他 Executor 的基础之上添加二级缓存的相关功能。\n插件 我们可以通过自定义的插件来扩展 MyBatis，或者改变 MyBatis 的默认行为。插件模块中最核心的接口就是 Interceptor 接口，所有 MyBatis 插件都必须要实现的这个接口。\n接口层 接口层中的接口都是 MyBatis 最常使用的一些接口，比如 SqlSession 和 SqlSessionFactory 等，其中最核心的接口是 SqlSession，我们可以通过它获取 Mapper 代理、执行 SQL 语句、控制事务开关等。\nSqlSession SqlSession 是 MyBatis 对外提供的一个 API 接口，整个 MyBatis 接口层也是围绕 SqlSession 接口展开的，SqlSession 接口中定义了下面几类方法。\n  select*() 方法：用来执行查询操作的方法，SqlSession 会将结果集映射成不同类型的结果对象，例如，selectOne() 方法返回单个 Java 对象，selectList()、selectMap() 方法返回集合对象。\n  insert()、update()、delete() 方法：用来执行 DML 语句。\n  commit()、rollback() 方法：用来控制事务。\n  getMapper()、getConnection()、getConfiguration() 方法：分别用来获取接口对应的 Mapper 对象、底层的数据库连接和全局的 Configuration 配置对象。\n  MyBatis 提供了两个 SqlSession 接口的实现类，还提供了 SqlSessionFactory 来创建 SqlSession 对象。相关类的关系如下：\n其中，DefaultSqlSession 是 SqlSession 的默认实现，它维护了一个 Executor 对象，用来完成数据库操作以及事务管理。\nDefaultSqlSessionFactory 是创建 DefaultSqlSession 的具体实现。\nSqlSessionManager 同时实现了 SqlSession 和 SqlSessionFactory 两个接口，也就是说，它同时具备操作数据库的能力和创建 SqlSession 的能力。\n","href":"/notebook/mybatis/","title":"Mybatis"},{"content":"","href":"/categories/notebook/","title":"Notebook"},{"content":"","href":"/tags/ostep/","title":"OSTEP"},{"content":"地址空间 操作系统为用户提供了一个易于使用的物理内存抽象，这个抽象叫做 地址空间（address space）。在系统中，地址空间是运行的程序看到的内存。\n一个进程的地址空间包含运行的程序的所有内存状态。当程序运行时，利用栈（stack）保存当前的函数调用信息，分配空间给局部变量、传递参数和返回值。最后，堆（heap）用于管理动态分配的、由用户管理的内存。当然，还有其它的东西，比如静态初始化变量。不过我们现在可以假设地址空间只有三部分：代码、栈和堆。\n上图展示了一个小型地址空间（只有 16KB）的例子。其中程序代码位于地址空间的顶部。在程序运行时，地址空间的堆（顶部）和栈（底部）两个区域可能动态增长或收缩。这种两端放置的方法只是一种约定，实际上可以按照任意方法放置。\n当我们描述地址空间时，我们描述的是操作系统提供给运行程序的抽象。以上面 16KB 的地址空间为例，程序实际上可能处于物理内存中的任意位置，并不一定在 0-16KB之间。地址空间描述的是虚拟内存，它与物理内存是不同的。\n 关键问题：如何虚拟化内存？操作系统如何在单一的物理内存上为多个进程构建出一个私有的、可能无限大的地址空间的抽象？\n 隔离是建立可靠系统的关键原则。如果两个实体相互隔离，则一个实体的失败不会影星到另一个实体。操作系统尽力让进程之间彼此隔离，从而防止相互伤害。\n虚拟化内存的目标 虚拟内存系统有几个重要的目标：\n 透明（transparency）：运行的程序并不会感知到内存被虚拟化的事实，程序的行为就好像它拥有自己的私有物理内存。背后是操作系统的辛勤工作，操作系统让不同的任务复用内存，从而提供这个假象。 效率（efficiency）：操作系统应该让虚拟化尽可能地高效。为此，可能需要像 TLB 这样的硬件支持。 保护（protection）：操作系统应当确保进程和自己受到保护，不受其它进程影响。保护让我们能够在进程之间提供隔离的特性，避免恶意进程的破坏。保护让我们能够在进程之间提供隔离机制，每个进程都应该在自己独立的环境中运行，避免其它出错或恶意进程的影响。   C 程序里面可以打印指针，但是我们看到的值是虚拟地址而不是物理地址（我们看到的所有地址都不是真的）。虚拟地址只是提供地址如何在内存中分布的假象，只有操作系统（和硬件）知道物理地址。这也反映出了用户程序和操作系统看到东西的不同。\n 地址转换 前面提到过，运行中的程序看到的是虚拟地址，而数据是位于物理地址中的。为了读取正确的数据，需要进行虚拟地址到物理地址的转换。\n基于硬件的地址转换（hardware-based address translation）简称地址转换（address translation），它是一种通用技术。每当程序进行内存访问（取指令、读写数据）时，地址转换将指令中的虚拟地址（virtual address）转换为实际的物理地址（physical address）。因此，在每次内存引用时，硬件都会进行地址转换，将应用程序的内存引用重定位到内存中的实际位置。\n仅靠硬件不足以实现虚拟内存，硬件只是提供了底层机制来提高地址转换的效率，实现虚拟内存还需要操作系统在关键位置介入。因此，操作系统必须能够管理内存，记录内存的使用情况。\n 介入（interposiiton） 是一种很常见又很有用的技术。在虚拟内存中，硬件可以介入到每次内存访问中，将进程提供的虚拟地址转换为数据实际所在的物理地址。\n 动态（基于硬件）重定位 基于硬件的地址转换在早期的时候很简单：每个 CPU 需要两个硬件寄存器——基址（base）寄存器 和 界限（bound）寄存器，界限寄存器有时又称限制（limit）寄存器。二者的合作能保证地址空间在物理内存中的任何位置，同时确保进程只能访问自己的地址空间。\n当我们编写和编译程序时，假设地址空间从零开始。但是，当程序真正执行时，操作系统会决定其在物理内存中的实际地址，并将基址寄存器设置为这个值。当进程运行时，硬件会通过以下方式计算出物理地址并发给内存系统：\n$$ physical\\ address = virtual\\ address + base $$\n地址转换技术指的正是将虚拟地址转换为物理地址。硬件取得进程认为它要访问的地址，将其转换为数据实际位于的物理地址，由于这种重定位是在运行时发生的，所以这种技术一般被称为动态重定位（dynamic relocation）。\n基址寄存器负责支持重定位，而界限寄存器就负责提供内存保护，确保进程产生的地址都在进程应该处于的界限之中。界限寄存器通常有两种工作方式：\n 记录地址空间的大小，在硬件将虚拟地址与基址寄存器内容求和前检查这个界限。 记录地址空间结束的物理地址，在硬件将虚拟地址转换成物理地址之后去检查这个界限。  下面是一个例子。假设一个进程拥有 4KB 大小的地址空间，它被加载到从 16KB 开始的物理内存中，一些地址转换结果如下表所示：\n   虚拟地址  物理地址     0 → 16KB   1KB → 17KB   3000 → 19384   4400 → 错误（越界）    操作系统的职责 为了支持动态重定位，操作系统需要处理一些问题：\n 内存管理  为新进程分配内存 从终止进程回收内存 管理内存使用情况   基址/界限管理  必须在上下文切换时正确设置基址/界限寄存器   异常处理  当异常发生时执行必要的处理逻辑    分段 如果简单地将进程的整个地址空间放入物理内存，栈和堆之间的空间即使没有被使用，也会占用实际的物理内存。因此，简单地通过基址寄存器和界限寄存器实现的虚拟内存非常浪费。\n分段（segmentation）实际上是为了支持更大的地址空间。其思想是：让地址空间内的每个逻辑段都有一对基址和界限寄存器。一个段（segment）只是地址空间内的一个连续定长的区域。在典型的地址空间里有3种不同的逻辑段：代码段、栈和堆。分段可以让操作系统将不同的段放到不同的物理内存区域，从而避免虚拟地址空间中未使用的部分占用物理内存。\n段错误是指在支持分段的机器上发生了非法的内存访问。\n当硬件在地址转换时使用段寄存器时，它如何知道虚拟地址引用了那个段，以及段内偏移量呢？一种常见的方式是用虚拟地址的前几位来标识不同的段，剩余位用来标识偏移量。这种方式显式地给出段的位置，硬件可以通过隐式方式获知。例如，如果地址由程序计数器产生，那么地址位于代码段，如果地址基于栈或者基址指针，那么地址位于栈段，其它情况则位于堆段。\n空闲空间管理 外部碎片（external fragmentation）：空闲空间被分割成不同大小的小块，成为碎片，后续的内存分配请求会由于找不到一块足够大的连续空闲空间而失败，即使这时总的空闲空间超出了请求的大小。 内部碎片（internal fragmentation）：如果分配程序分配给程序的内存块超过了请求的大小，那么超出大小的部分（未被使用）就会称为内部碎片，即空间的浪费出现在已分配单元的内部。\n空闲链表（free list）是一种管理空闲空间的数据结构，该结构包含了所管理内存区域中所有空闲块的引用。\n常见的内存分配策略有：最优匹配、最差匹配、首次匹配、下次匹配等\n分离空闲链表：如果某个程序经常申请一种或集中大小的内存空间，那就用一个独立的列表来管理这些大小的对象，其它大小的请求交给通用的内存分配程序。例如 Solaris 的内存分配程序厚块分配程序（slab allocator）。\n分页 在内存管理方面，操作系统通常有两种方法：\n 分段：将空间分割成不同长度的分片，就像虚拟内存管理中的分段。但是，将空间切成不同长度的碎片后，空间本身会碎片化，随着时间的推移，分配内存会变得比较困难。 分页：将空间分割成固定长度的分片。每个分片为一个单元，即一页，称之为页帧（page frame）。  页表 为了记录地址空间的每个虚拟页在物理内存中所处的位置，操作系统通常会为每个进程保存一个被称为页表（page table）的数据结构。页表的主要作用是为每个虚拟页面保存地址转换信息，从而让我们知道每个页在物理内存中的位置。\n虚拟地址由两部分组成：虚拟页面号（virtual page number，VPN）和页内偏移量（offset）。\n在进行地址转换时，先根据 VPN 检索页表，找出虚拟页号对应的物理帧号（PFN）或物理页号（PPN）。通过用 PFN 替换 VPN 转换虚拟地址，页内偏移保持不变（虚拟页和物理页大小相等），过程如下：\n页表的保存 页表可以变得非常大，一个 20 位的 VPN 意味着操作系统必须为每个进程管理 1M （大约一百万）个地址转换。假设每个页表条目（PTE）占 4B，则每个页表就需要 4MB 的存储空间。实际上，操作系统将每个进程的页表存储在内存中，由于很多操作系统内存本身也可以被虚拟化，所以页表也可以存储在操作系统的虚拟内存中（甚至可以被交换到磁盘上）。\n分页的问题 普通的分页存在两个问题：\n 页表太大，导致访问慢。 页表太大，导致消耗的内存太多。  TLB TLB 是为了解决分页速度慢的问题。\n基于分页的虚拟内存可能会带来比较高的性能开销，因为需要将地址空间切分为固定大小的页，并记录这些页的地址映射信息。由于这些信息一般存储在物理内存中，所以在进行地址转换时，分页逻辑需要一次额外的内存访问。每次获取指令、加载或保存数据，都需要多读一次内存才能得到转换信息，这回使得总体的慢难以接受。为了加速地址转换，操作系统需要硬件的帮助，也就是所谓的 地址转换旁路缓冲（translation-lookaside buffer，TLB）。\n对于每次内存访问，硬件都会先检查 TLB，若其中包含期望的地址转换，则不需要访问页表就能完成转换。页表中包含全部的转换映射，直接走 TLB 而不查询页表会很快，TLB 因此能够带来巨大的性能提升。\nTLB 和其他缓存类似，前提都是在一般情况下，转换映射会在缓存中（即TLB命中）。若是如此，只需要增加少量开销就可以达到非常快的访问速度，因为 TLB 就在处理器附近。如果 TLB 未命中，就会产生很大的分页开销，必须访问页表获取转换映射，导致额外的内存访问。如果经常这样，程序的运行就会显著变慢。因此，我们希望尽可能地避免 TLB 不命中。\n既然缓存这么快，为什么不把它做得更大呢？原因是如果想要快速地缓存，它就必须小，因为光速和其它物理限制会起作用。大的缓存注定慢，因此无法实现目的。所以我们应该关注如何利用好缓存来提高性能。\n随机存取存储器（Random-Access Memory，RAM）暗示你访问 RAM 的任意部分都一样快。虽然 一般这样想 RAM 没错，但因为 TLB 这样的硬件/操作系统功能，访问某些内存页的开销较大，尤其是 没有被 TLB 缓存的页。因此，最好记住这个实现的窍门：RAM 不总是 RAM。有时候随机访问地址空 间，尤其是 TLB 没有缓存的页，可能导致严重的性能损失。\n较小的页表  关键问题：简单的线性页表太大，占用过多内存，如何让它更小？\n 一种方法是使用更大的页，页的数量少了，页表因此变小。但是，更大的页容易出现内部碎片，造成内存的浪费。\n另一种方法是混合使用分段和分页方法，每个分段一个页表。但这避不开分段可能导致的外部碎片问题。\n还有一种方法是采用多级页表。多级页表是时间与空间的折中。\n在反向页表中，只有一个页表，其中的项代表系统的每个物理页，而不是进程的页表。页表项告诉我们哪个进程正在使用此页，以及该进程的哪个虚拟页映射到了此物理页。\n交换  关键问题：操作系统如何利用大而慢的设备，透明地提供巨大虚拟地址空间的假象？\n 交换空间 为了提供巨大虚拟地址空间的假象，我们可以在硬盘上开辟一部分空间（交换空间，swap space）用于物理页的移入和移出。在必要的时候将内存中的页面交换出去，在需要的时候又从其中交换回来。\n需要注意的是，交换空间不是唯一的硬盘交换目的地。\n交换时机 操作系统一般会预留一小部分空闲内存。大多数操作系统都会设置高水位线（High Watermark，HW）和低水位线（Low Watermark，LW），从而帮助决定何时从内存中清楚页。原理很简单：当操作系统发现有少于 LW 个页可用时，后台负责释放内存的线程就会开始运行，直到有 HW 个可用物理页。\n页置换策略 内存其实只包含了系统中所有页的子集，所以我们可以将其视为系统中虚拟内存页的缓存。从磁盘获取页就是缓存未命中，从内存中找到待访问的页就是缓存命中。\n最优替换策略：替换内存中在最远将来才会被访问到的页，这样能达到最低的缓存命中率。\nFIFO：先入先出替换策略。\n随机策略：在内存满的时候随机选择一个页进行替换。\nLRU（Least-Recently-Used）：替换最近使用最少的页面。\nLFU（Least-Frequently-Used）：替换最近嘴部经常使用的页面。\n抖动（trashing）：当内存被超额请求时，操作系统将会不断地进行换页。\n","href":"/notebook/reading_notes/ostep/virtualization-memory/","title":"内存虚拟化"},{"content":"进程 进程即运行中的程序。程序本身是没有生命周期的，它只是存在磁盘上的一些指令（也可能是一些静态数据）。事实表明，人们通常希望同时运行多个程序。\n 关键问题：如何提供有许多 CPU 的假象？\n 操作系统通过 CPU 虚拟化提供有许多 CPU 的假象，允许一个进程运行一段时间，然后切换到其它进程，这就是时分共享（time sharing）技术。时分共享的潜在缺点就是性能损失，因为如果 CPU 必须共享，那么每个进程的运行就会慢一点。\n要实现好 CPU 虚拟化，操作系统需要一些底层机制（mechanism）和高级策略（policy）。底层机制是一些底层方法或协议，实现了所需的功能，比如上下文切换（context switch）。而高级策略则是在操作系统内做出某种决定的算法，比如调度策略。\n操作系统为正在运行的程序提供的抽象，就是所谓的进程（process）。要理解构成进程的是什么，我们必须理解进程的机器状态（machine state）：程序在运行时可以读取或更新的内容。内存是进程的机器状态的一个重要组成部分，指令位于内存中，程序读写的数据也在内存中；进程的机器状态的另一部分是寄存器；由于程序经常访问持久存储设备，此类 I/O 信息可能包含当前打开的文件列表。\n进程的创建  操作系统如何将程序转化为进程？\n 要将程序转化为进程，操作系统需要完成以下工作：\n 将代码和所有静态数据加载到内存中，加载到进程的地址空间中。 为程序的运行时栈分配一些内存。C 语言中的栈用于存放局部变量、函数参数和返回地址。 操作系统也可能为程序的堆分配一些内存。C 语言中的堆用于显式请求的动态分配数据。 执行其它初始化任务，特别是与 I/O 相关的任务。 启动程序。OS 将 CPU 的控制转转交给新创建的进程，程序开始运行。  进程状态 早期的计算机系统中，进程可以处于以下三种状态之一：\n 运行（running）：进程正在处理器上运行，这意味着进程正在执行指令。 就绪（ready）。进程已经准备好，但由于某些原因，操作系统选择不在此时运行该进程。 阻塞（blocked）：进程等待特定事件发生时才会准备运行，比如等待 I/O 完成。  进程的数据结构 操作系统也是一个程序，和其它程序一样，它维护了一些关键的数据结构，用来跟踪各种相关的信息。例如，下面是 xv6 内核中的进程结构：\n// the registers xv6 will save and restore // to stop and subsequently restart a process struct context { int eip; int esp; int ebx; int ecx; int edx; int esi; int edi; int ebp; }; // the different states a process can be in enum proc_state { UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE }; // the information xv6 tracks about each process // including its register context and state struct proc { char *mem; // Start of process memory  uint sz; // Size of process memory  char *kstack; // Bottom of kernel stack for this process  enum proc_state state; // Process state  int pid; // Process ID  struct proc *parent; // Parent process  void *chan; // If !zero, sleeping on chan  int killed; // If !zero, has been killed  struct file *ofile[NOFILE]; // Open files  struct inode *cwd; // Current directory  struct context context; // Switch here to run process  struct trapframe *tf; // Trap frame for the current interrupt }; Limited Direct Execution 操作系统通过虚拟化 CPU 让多个任务共享物理 CPU，但是在构建这样的虚拟化时存在一些挑战：其一是性能，其二是控制权。控制权对于操作系统尤为重要，因为操作系统负责资源管理。操作系统应以高性能的方式虚拟化 CPU，同时保持对系统的控制。\n 关键问题：如何高效、可控地虚拟化 CPU？\n 受限直接执行（Limited Direct Execution）是操作系统开发人员想出的一种技术，目的是让程序尽可能快地运行。“受限”表示操作系统需要具备控制权，进程会受到一定的限制，“直接执行”就表示在 CPU 上运行程序。\n受限制的操作 直接执行有一个明显的优势——快速，但是我们需要考虑当进程希望执行某种受限操作时（比如磁盘 I/O）该怎么办？\n 关键问题：如何执行受限制的操作？进程必须能够执行I/O等受限操作，但又不能完全让进程控制系统。\n 用户模式与内核模式 操作系统与硬件协作，硬件提供不同的执行模式。在 用户模式（user mode） 下，应用程序不能访问全部的硬件资源，而在 内核模式（kernel mode） 下，操作系统可以访问机器的全部资源。\n在用户模式下，运行的代码会受到限制。在内核模式下，运行的代码可以做任何事，操作系统（或内核）就运行在这个模式下。\n系统调用允许内核向用户程序暴露某些关键功能，要执行系统调用，程序必须执行特殊的 陷阱（trap） 指令。该指令同时跳入内核并将特权级别提升到内核模式。系统调用执行完成后，操作系统调用一个特殊的 从陷阱返回（return-from-trap） 指令，返回到发起调用的用户程序中，同时将特权级别降低，回到用户模式。\n在执行陷阱时，硬件必须确保保存足够的调用者寄存器，以便在操作系统发出从陷阱返回指令时能正确地返回。那么陷阱如何知道在 OS 内运行哪些代码呢？内核通过在启动时设置 陷阱表（trap table） 来呈现需要在 OS 内运行的代码。操作系统启动时做的有一件事情就是告诉硬件在发生某些异常事件时要运行哪些代码。\n下面的时间线对首先直接运行协议进行了总结。整个过程假设每个进程都有一个内核栈，在进入内核时保存寄存器内容，在离开内核时恢复寄存器内容。\nLDE 协议有两个阶段。第一个阶段（在系统引导时），内核初始化陷阱表，并且 CPU 记住它的位置以供随后使用。内核通过特权指令来执行此操作（所有特权指令均以粗体突出显示）。第二个阶段（运行进程时），在使用从陷阱返回指令开始执行进程之前，内核设置了一些内容（例如，在进程列表中分配一个节点，分配内存）。这会将 CPU 切换到用户模式并开始运行该进程。当进程希望发出系统调用时，它会重新陷入操作系统，然后再次通过从陷阱返回，将控制权还给进程。该进程然后完成它的工作，并从 main()返回。这通常会返回到一些存根代码，它将正确退出该程序（例如，通过调用 exit()系统调用，这将陷入 OS 中）。此时，OS 清理干净，任务完成了。\n进程切换  关键问题：操作系统如何重获 CPU 的控制权，以便它可以在进程间切换？\n 协作方式 过去某些系统采用了一种被称为协作（cooperative）的方式，在这种方式下，操作系统相信进程会合理运行，运行时间过程的进程会定期放弃 CPU（通过系统调用），以便操作系统运行其它任务。\n通常情况下，操作系统必须处理系统中的不当行为，如果某些进程执行了非法操作，操作系统需要重新控制 CPU。在协作方式中，某个进程若开启无限循环，并且不进行 yield 系统调用，那么操作系统则无法做任何事情。唯一的解决方式就是重启。\n非协作方式 通过 时钟中断（timer interrupt），操作系统可以以非协作的方式获取 CPU 的控制权。时钟设备定时产生中断，产生中断时，当前运行的程序停止，操作系统预先配置的中断处理程序会运行。此时，操作系统重新获得 CPU 控制权，就可以做任何事情了，比如停止当前进程并启动另一个进程。\n在发生中断时，硬件需要为当前正在运行的程序保存足够的状态，以便从陷阱返回指令能够正确恢复该程序。\n上下文切换（context switch） ：操作系统要做的就是为当前正在执行的进程保存一些寄存器的值（例如，到它的内核栈），并为即将执行的进程恢复一些寄存器的值（从它的内核栈）。这样一来，操作系统就可以确保最后执行从陷阱返回指令时，不是返回到之前运行的进程，而是继续执行另一个进程。\n下面是基于时钟的 LDE 协议：\n进程调度  关键问题：如何开发调度策略？什么是关键假设？哪些指标非常重要？\n 确定工作负载（workload）是构建调度策略的关键部分。工作负载了解得越多，你的调度策略就能越优化。此外，我们还需要一些调度指标，用来比较不同的调度策略。一个基本的调度指标是 周转时间（turnround time）。\n任务的周转时间即任务完成时间减去任务到达系统的时间：\n$$ T_{周转时间} = T_{完成时间} - T_{到达时间} $$\n周转时间是一个性能指标。还有一个有趣的指标—— 公平性（fairness）。性能和公平性在调度系统中往往是矛盾的。\n先进先出 先进先出（First In First Out，FIFO）调度是一种基本的算法，有时候又称为先来先服务（First Come First Service，FCFS）。FIFO 的调度思想是：先运行最先达到系统的任务，然后是第二到达的，以此类推。\n假设 A、B、C 三个任务几乎同时到达系统（A 比 B 早一点点，B 比 C 早一点点），若采用 FIFO 调度策略，则调度器会先执行 A，然后执行 B，最后执行 C。从图 FIFO 1 可以看出：A 在 10s 完成，B 在 20 秒完成，C 在 30 秒完成。那么这三个任务的平均周转时间就是 $\\frac {10 + 20 + 30}{30} = 20 $。但是，若三个任务的运行情况如图 FIFO 2 所示的话，三个任务的平均周转时间就是 $\\frac {100 + 110 + 120}{3} = 110$。\n第二种情况通常被称为 护航效应（convoy effect），即一些耗时短较少的潜在资源消耗者排在重量级的资源消费者之后。这对于耗时较短的任务来说，并不友好。\n最短任务优先 最短任务优先（Shortest Job First， SJF）是另一种调度算法，解决了 FIFO 算法中耗时较短的任务等待时间可能过长的问题。SJF 的调度思想是：先运行最短的任务，然后是词短的任务，以此类推。\n假设 A、B、C 同时到达（图 SJF 1），按照 SJF 原则，系统会先运行 A，然后运行 B，最后运行 C。因此，SJF 可以将 110s 的平均周转时间降低到 $\\frac {10 + 20 + 120}{3} = 50$。事实上，当所有任务同时到达系统时，SJF 被证明是一个最优的调度算法。但是 SJF 算法也存在问题，假设 A 先到达，10s 之后 B、C 也到达（图 SJF 2）了，三个任务的平均周转时间则是 $\\frac {100 + (110 - 10) + (120 - 10)}{3} = 103.33$，仍然出现了护航效应。\n最短完成时间优先 SJF 调度算法是非抢占式（non-preemptive），而最短完成时间优先（Shortest Time-to-Completion First，STCF）或抢占式最短作业优先（Preemptive Shortest Job First，PSJF）是 SJF 的抢占式版本。每当新任务进入系统时，调度程序就会计算剩余任务和新任务之间，哪个任务的剩余时间最短，然后调度该任务。\n还是上一个例子，采用 STCF 算法，平均周转时间大大提高：$\\frac {120 + (20 - 10) + (30 - 10)}{3} = 50$\n轮转 周转时间关心的是任务完成的快慢，适合批处理系统。而在分时系统中，用户就在系统前面，期望系统具有良好的交互性。因此，新的调度指标——响应时间（response time） 出现了。响应时间指的是从任务到达系统到系统首次运行它的时间，即\n$$T_{响应时间} = T_{首次运行} - T_{到达时间} $$\n在 STCF 的例子中，周转时间很好，但是响应时间就没那么好了，如果用户在系统响应前不得不等待 10s,那就太糟糕了！现在思考新的问题，如何构建对响应时间敏感的程序？\n有一种简单的调度算法，叫做轮转（Round-Robin，RR）。它的基本思想很简单：RR 在一个时间片内运行一个任务，然后切换至运行队列中的下一个任务，反复执行，直到所有任务完成。时间片（time slice）有时又称调度量子（scheduling quantum），需要注意的是：时间片的长度必须是时钟中断周期的整数倍。\n假设 A、B、C 三个任务同时到达系统，并且都希望运行 10s。SJF 调度程序必须在运行完当前任务之后才可运行下一个任务（图 RR 1），而 1s 的时间片可以让 RR 调度程序调度各任务快速地循环工作。SJF 的平均响应时间为：$\\frac {0 + 10 + 20}{3} = 10$，而 RR 的平均响应时间为：$\\frac {0 + 1 + 2}{3} = 1$。\n可以发现，时间片长度 对 RR 至关重要。时间片越短，RR 在响应时间上的表现就越好。然而，时间片也不是越短越好：频繁的上下文切换将影响系统整体性能。因此，系统设计者需要衡量时间片的长度，设置一个足够长的值，以便在摊销（amortize）上下文切换成本的同时保持系统的响应性。\n多级反馈队列 多级反馈队列（Multi-level Feedback Queue，MLFQ）是一种应用于兼容时分共享系统（Compatible Time-Sharing System）的调度算法。它有两个目标：其一是优化周转时间，其二是给用户良好的交互体验（降低响应时间）。\n 关键问题：没有工作长度的先验知识，如何设计一个能同时减少响应时间和周转时间的调度程序。\n 方法是 从历史中学习。MLFQ 就是用历史经验预测未来的一个典型例子。\nMLFQ 中有许多独立的队列，每个队列有不同的优先级。任何时刻，一个任务只能存在于一个队列中。MLFQ 总是优先执行较高优先级的任务（在较高级队列中的任务）。当然，一个队列中可能会有多个任务（这些任务的优先级相同），这时可以对它们采用轮转调度。因此，MLFQ 调度策略的关键在于 如何设置优先级。\n基本规则 MLFQ 的基本规则：\n 规则 1：If Priority(A) \u0026gt; Priority(B), A runs (B doesn\u0026rsquo;t). 规则 2：If Priority(A) = Priority(B), A \u0026amp; B run in RR. 规则 3：When a job enters the system, it is placed at the highest priority (the topmost queue). 规则 4a：If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue). 规则 4b：If a job gives up the CPU before the time slice is up, it stays at the same priority level. 规则 5：After some time period S, move all the jobs in the system to the topmost queue.  规则 1 和 规则 2 是最基本的两条规则，给出了两个任务在给定优先级下的执行顺序，会出现低优先级的任务饥饿的情况。\n规则 3、4a、4b 给出了改变优先级的规则。它能够根据负载情况调整任务的优先级，使得耗时任务之间可以公平地共享 CPU，并且短任务或交互性任务也能得到很好的响应时间，但是仍然存在饥饿的问题——过多的交互任务会导致耗时任务饥饿。\n规则 5 定时改变优先级，消除了饥饿，保证了任务的执行。但是引入的时间段 S 又带来了新的问题——S 该如何设置？太长会让耗时任务会饥饿，太短会让交互型任务得不到合适的 CPU 时间。\n一个更好的计时方式是修改规则 4a、4b。让调度程序记录一个进程在某一个队列中消耗的总时间，而不是在调度时重新计时。只要进程用完了自己的配额，就把它降到低一优先级的队列中去。因此，重写规则 4a、4b 为规则 4：\n 规则 4：Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).  与其它调度策略的不同 MLFQ 不同于 FCFS、SJF、STCF，它不需要对任务的运作方式有先验知识，而是通过观察任务的运行情况来给出对应的优先级。\nMLFQ 可以满足各种任务需求：对运行时间短的交互型任务，它可以获得类似于 SJF/STCF 的较不错的全局性能，同时对于耗时的 CPU 密集型负载也可以公平地稳步向前。\n比例份额 比例份额（proportional-share）调度程序有时也称公平份额（fair-share）调度程序，其核心思想为：调度程序的最终目标是确保每个任务获得一定比例的 CPU 时间，而不是优化周转时间和响应时间。\n彩票调度 彩票调度（lottery scheduling）是比例份额调度程序的一个优秀例子。基本思想为：每隔一段时间就举行一次彩票抽奖，以确定接下来应该运行哪个进程。越是应该频繁运行的进程，越是应该拥有更多赢得彩票的机会。\n彩票数（ticket） 是彩票调度背后的基本概念，它代表了进程占有某个资源的份额。一个资源拥有的彩票数占总彩票数的百分比，就是它占有的资源的份额。通过不断地定时抽取彩票，彩票调度从 概率 上获得这种份额比例。\n彩票调度最精彩的地方在于利用了 随机性。随机性至少有三点优势：\n 随机方法常常可以避免各种奇怪的边界情况。 随机方法很轻量，几乎不需要记录任何状态。 随机方法很快，只要能产生随机数，就能做出决策。  彩票调度还提供了一些机制，它们以不同的方式来调度彩票：\n 彩票货币（ticket currency）允许拥有一组彩票的用户以自己喜欢的方式将彩票分给不同的任务。之后操作系统再自动将这种货币兑换为正确的全局彩票。 彩票转让（ticket transfer）。通过转让，一个进程可以临时将自己的彩票交给另一个进程，加速它的执行。 彩票膨胀（ticket inflation）。利用膨胀，一个进程可以临时提升或降低自己拥有的彩票数量。一般用于进程间相互信任的环境。  由于随机性，彩票调度在任务执行时间很短时，公平度非常差。只有当任务执行的时间片非常多时，才能得出想要的结果。这也是基于概率的算法的一个通病。\n步长调度 步长调度（stride scheduling）解决了彩票调度的不确定性问题，它是一个确定性的公平分配算法。在步长调度中，每个任务都有自己的步长，这个值与彩票数量成反比。\n对于每一个进程，每次运行后，让进程的计数器（称为行程（pass）值）增加它的步长那么多的增量，记录进程执行的总体进度。之后，调度程序使用进程的步长及行程值来确定调度哪个进程。基本思想很简单：当需要进行调度时，选择目前行程值最小的进程，运行它并在运行完之后将该进程的行程值增加一个步长。\nLinux 的完全公平调度器 Linux 中的完全公平调度器（Completely Fair Scheduler，CFS）不仅实现了公平份额调度，还具有非常高的效率和可扩展性。CFS 的目标是：在所有相互竞争的进程之间公平地均匀分配 CPU。Linux 是通过 virtual time (vruntime) 技术来实现的。\n每个进程运行时都会累计 vruntime 值。最常见的是，每个进程的 vruntime 都以相同的速率累积。当需要进行调度时，CFS 会选取具有最小 vruntime 值的进程运行。这里有一个需要考虑的点——调度器如何知道该在何时停止当前运行的进程并运行下一个进程？如果 CFS 切换得过于频繁，公平性增加而性能降低（更多的上下文切换），反之则公平性降低而性能增加（更少的上下文切换）。\nCFS 具体是通过一些控制参数来管理调度过程的。第一个参数是 sched_latency，CFS 用它来决定一个进程在切换前应当运行的时长，通常为 48ms。CFS 用进程的数量 n 除 sched_latency 得到单个进程的时间片，从而确保完全公平。但是，如果进程过多怎么办？时间片会因此变得非常小吗？上下文切换会更加频繁吗？并不会，CFS 为了解决这个问题，给了我们另一个参数 min_granularity，这个值通常为 6ms。CFS 永远不会将时间片的大小设置为小于这个值的数。\nCFS 还支持控制进程的优先级，允许用户赋予某些进程更高的 CPU 份额。这并不是通过彩票数实现的，而是通过 Linux 中经典的进程 nice 值实现的。对于一个进程而言，其 nice 值可以是 [-20 , +19] 之间的任一整数，默认为 0。正数表示更低的优先级，而负数表示更高的优先级。CFS 会将 nice 值映射为权重：\nstatic const int prio_to_weight[40] = { /* -20 */ 88761, 71755, 56483, 46273, 36291, /* -15 */ 29154, 23254, 18705, 14949, 11916, /* -10 */ 9548, 7620, 6100, 4904, 3906, /* -5 */ 3121, 2501, 1991, 1586, 1277, /* 0 */ 1024, 820, 655, 526, 423, /* 5 */ 335, 272, 215, 172, 137, /* 10 */ 110, 87, 70, 56, 45, /* 15 */ 36, 29, 23, 18, 15, }; 有了权重之后，我们就可以计算出每个进程的时间片\n$$ time \\underline{\\ } slice_k = \\frac {weight_k}{\\sum_{i=0}^{n-1}(weight_i)} \\cdot sched \\underline{\\ } latency $$\n此外，vruntime 的计算方式也有所改变：\n$$ vruntime_i = vruntime_i + \\frac {weight_0}{weight_i} \\cdot runtime_i $$\n多处理器调度  关键问题：操作系统应该如何在多 CPU 上调度任务？会遇到什么新问题？已有技术是否依然适用？\n 缓存 多处理器与单 CPU 之间最大的区别在于 硬件对缓存（cache）的使用，以及多处理器之间共享数据的方式：\n缓存是基于 局部性（locality） 的。局部性分为两种：\n 时间局部性：当一个数据被访问后，她很有可能在不久的将来再次被访问。 空间局部性：当程序访问某个地址的数据时，很有可能会紧接着访问该地址周围的数据。  缓存可以有效地提升数据的访问速度，从而加快程序的运行速度。CPU 会先尝试从缓存中查找数据，若缓存中没有需要的数据，则访问内存。由于数据更新的存在，所以在在多处理器中，会出现 缓存一致性（cache coherence）问题。\n通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性。在基于总线的系统中，一种方式就是使用总线窥探（bus snooping）。每个缓存都通过监听链接到所有缓存和内存的总线，来发现内存访问。如果 CPU 发现它放在缓存中的数据被更新了，就会作废（invalidate）本地的副本（从缓存中移除）或者更新它（修改会新的值）。\n虽然缓存在一致性方面做了很多工作，但是应用程序还是需要关心共享数据的访问。跨 CPU 访问（尤其是写入）共享数据或数据结构时，需要使用 互斥原语（比如锁），才能保证正确性。一些无锁的数据结构也能达到这个目的，但是它们非常复杂，使用得不是太多。同步操作对性能有影响，随着 CPU 数量的增加，访问同步共享的数据结构会变得很慢，因为需要做大量的同步操作。\n多处理器调度中还需要考虑 缓存亲和度（cache affinity）：当进程在某个 CPU 上运行时，该 CPU 的缓存内会维护很多状态。若下次该进程还在同一个 CPU 上运行，它会由于缓存中的数据而执行得非常快。相反，在不同的 CPU 上执行同一个进程时，由于需要重新加载数据，所以会变得很慢。因此，最好是尽可能让同一个进程在同一个 CPU 上执行。\n多队列调度 有些系统采用了多队列的调度方案，比如每个 CPU 一个队列，称之为多队列多处理器调度（Multi-Queue Multiprocessor Scheduling，MQMS）。\n在 MQMS 中，基本调度框架包含多个调度队列，每个队列可以使用不同的调度规则。当任务进入系统后，系统会按照一些启发性规则将其放入到某个调度队列。这样一来，每个 CPU 之间的调度相互独立，不需要同步。\nMQMS 天生具有良好的缓存亲和度。所有的工作都保持在固定的 CPU 上，因而可以很好地利用缓存数据。但这又存在一个新的问题——负载不均（load imbalance）。\n为了实现负载均衡。我们可以让任务跨 CPU 移动，这种技术被称为迁移（migration）。有很多种迁移模式，不过最棘手的部分就是如何决定发起迁移？有一个被称为 工作窃取（work stealing） 的技术，工作量较少的（源）队列不定期“偷看”其它（目标）队列的任务情况，如果目标队列更加拥挤，就从目标队列“窃取”一个或多个任务，实现负载均衡。在使用这种方法时，需要留意：对其它队列的检查不能太频繁，否则会带来较高的开销，但检查间隔又不能太长，否则可能带来严重的负载不均。所以需要找到合适的阈值。\nLinux 上的多处理器调度 有趣的时，Linux 社区一直没有就构建多处理器调度成都达成共识。一直以来，存在三种不同的调度程序：\n O(1) 调度程序 完全公平调度程序（CFS） BF 调度程序（BFS） O(1) 和 CFS 采用多队列，而 BFS 采用单队列。  有关 BFS 的更多信息：http://ck.kolivas.org/patches/bfs/bfs-faq.txt\n","href":"/notebook/reading_notes/ostep/virtualization-cpu/","title":"CPU 虚拟化"},{"content":"程序运行时会发生什么？\n一个正在运行的程序会做一件非常简单的事情：执行指令。CPU 从内存中取出（fetch）一条指令，对其进行译码（decode），然后执行（execute）它，保存执行结果，紧接着又去取指令，译码，执行指令……，如此周而复始，反复循环，使得计算机能够自动地工作。除非遇到停机指令（程序执行完成），否则这个循环将一直进行下去。\n虚拟化  关键问题：如何将资源虚拟化？\n 操作系统负责确保既易使用又高效地运行，为此，它将物理资源（处理器、内存或磁盘）转换为更通用、更强大且更易于使用的虚拟形式，因此，我们有时也将操作系统称为虚拟机。为了让用户可以告诉操作系统要做什么，操作系统提供了一些 API（或者说系统调用）供程序调用。由于操作系统提供这些调用来运行程序、访问内存和设备、进行其它操作，所以我们有时也会说操作系统为应用程序提供了一个 标准库（standard library）。\n操作系统早期被称为 super visor 或 master control program。因为虚拟化让许多程序运行（从而共享 CPU），让许多程序可以同时访问自己的指令和数据（从而共享内存），让许多程序访问设备（从而共享磁盘等），所以操作系统又被称为 资源管理器（resource manager）。每个 CPU、内存和磁盘都是系统的资源，因此操作系统扮演的主要角色就是管理这些资源，做到高效公平，或实现其它预定目标。\n在硬件的帮助下，操作系统负责提供系统拥有非常多的虚拟 CPU 的假象。将单个 CPU（或其中一小部分）转换成为看似无限数量的 CPU，从而让许多程序看似在同时运行，这就是所谓的 虚拟化 CPU。\n内存是一个字节数组。要读取内存，必须指定一个地址，才能访问存储在那里的数据。要写入或更新内存，还必须执行要向给定地址写入的数据。每个进程访问自己的私有虚拟地址空间（有时也直接称为地址空间），操作系统以某种方式将虚拟地址空间映射到机器的物理内存上去。一个正在运行的程序中的内存引用不会影响到其他进程（或操作系统本身）的地址空间。对于正在运行的程序，它完全拥有自己的物理内存。这就是 虚拟化内存。\n并发  关键问题：如何构建正确的并发程序？\n 持久化  关键问题：如何持久化地存储数据？\n 在系统内存中，数据容易丢失，因为像 DRAM 这样的设备以易失（volatile）的方式存储数据。如果断电或系统崩溃，内存中的所有数据都会丢失。因此，我们需要硬件和软件来持久地存储数据。这样的存储对于所有系统都很重要，因为用户非常关心他们的数据。\n操作系统中管理磁盘的软件通常为称为文件系统，它负责可靠高效地将用户创建的任何文件存储在系统的磁盘上。不想操作系统为 CPU 和内存提供的抽象，操作系统不会为每个应用程序创建专用的虚拟硬盘，相反，它假设用户经常需要 共享 文件中的信息。\n操作系统的设计目标 操作系统取得 CPU、内存或磁盘等物理资源（resources），并对它们进行虚拟化（virtualize）。它处理与并发（concurrency）有关的麻烦事儿。它持久地（persistently）存储文件，从而使它们长期安全。我们希望建立这样一个系统，所以要有一些目标，以帮助我们集中设计和实现，并在必要时进行折中。找到合适的折中是建立系统的关键。以下是一些目标：\n 建立一些抽象，让系统方便易用。 提供高性能（最小化操作系统的开销）。 在应用程序之间以及在 OS 与应用程序之间提供保护。保护是操作系统和核心原理之一（即隔离），让进程彼此隔离是保护的关键。  参考资料  CPU的工作过程.  ","href":"/notebook/reading_notes/ostep/introduction_to_operating_systems/","title":"Introduction to Operating Systems"},{"content":"","href":"/tags/%E6%90%9C%E7%B4%A2/","title":"搜索"},{"content":" 人生中 99% 的问题早已有答案，你只要搜索就好。只要你去搜索，就能一直站在巨人的肩膀上去寻求新突破，做出微创新。 ——朱丹\n 前言 朱丹老师的《超级搜索术 : 帮你找到99%问题的答案》是一本讲搜索技巧的书，也是一本介绍学习技巧的书，内容不难，却很实用。大学期间，笔者为了获得虫部落的邀请码，玩过多期搜索游戏，趣味十足。那时候，我的技巧还比较散，从未像这本书中这么成体系，读完本书倒也有了一种豁然开朗的感觉。\n信息素养：对信息进行搜索、获取、判断和利用，从而了解陌生事物，解决切身问题。其过程包括信息获取、信息输入、信息加工与输出等环节。\n信息素养的四大能力：识别需求、获取信息、甄别信息和运用信息。\n搜索解决问题的闭环行动框架（1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;1\u0026hellip;）：\n 分析问题、界定需求 选择搜索工具 构造检索公式 筛选检索结果 优化检索策略  \u0026hellip;\n识别需求 找对问题——6W2H模型 找对问题的目的在于精准挖掘需求。6W2H 指的是为什么（Why）、是什么（What）、什么时候（When）、在哪里（Where）、是谁（Who）、哪一个（Which）、怎么做（How）和成本如何（How much）。\n分解问题——柚子模型 分解问题的目的在于找到真正的执行方案。柚子有三层，先剥去黄色外皮，再剥白色内皮，看见第三层果肉，才可食之。复杂问题下的信息挖掘类似，挖第一层和第二层的时候还不足以解决实际问题，当挖到第三层的时候，就意味着我们可以制定方案开始行动了。\n资源获取 搜索指令 正确使用搜索指令有利于过滤垃圾信息，更快更准确地获取我们想要的结果。下面是一些常见的搜索指令：\n   关键词 作用 用法     site 将搜索结果限定在某个网站中 关键词+空格+site+英文冒号+搜索范围所限定的网站   filetype 将搜索结果限定为某种文件类型 关键词+空格+filetype+英文冒号+文件格式   时间1..时间2 限定搜索结果的时间范围 关键词+空格+20xx年+两个英文句号+20xx年   intitle 限定搜索标题中所包含的关键词 关键词+空格+intitle+英文冒号+需要限定的关键词   inurl 限定搜索结果的网址中应包含的字段 inurl:xxx。它的使用范围比 site 要广，可替代 site 使用   - 让搜索结果中不包含某些词 关键词-过滤词    此外还有一些高级指令，比如基于与或非的布尔表达式等。\n通过搜索拓展人脉 常见渠道：微博、在行、知乎、搜索引擎、邮件、天眼查、垂直领域平台、微信QQ等社交平台、在线文档、社群等。\n常见方式：私信、邮箱、微信号、领英账号、手机号、公开提问、企业法人等。\n通过搜索获取知识 跳槽：看准网看公司评价、面试经验、工资水平和大体职位，通过公司官网或公众号确认信息跟踪动态，人脉搜索助力。\n考研：考研论坛，计算机有王道论坛。\n新领域：百度百科、维基百科、淘宝、百度经验、wikiHow 等。\n生活经验：支付宝微信的便民板块等。\n深度资源搜索 网盘搜索、磁力搜索。\n做学术有百度学术、必应学术、中国知网、万方等。\n搜索引擎原理 简单来说，搜索引擎的工作原理大致可分为以下四步：\n 利用爬虫爬取数据 分析数据 建立索引 查询结果  搜索引擎大致可以分为三类：\n 全文搜索引擎：即我们一般意义上认为的搜索引擎 元搜索引擎：它在接收用户查询之后，同时在多个搜索引擎上搜索，然后将结果返给用户 垂直搜索引擎：专注于某一领域的搜索引擎  资源变现 这一部分的核心在于“找对资源，少花钱”。\n教育资源 寻找优质教育资源：inurl:edu 可以找出教育机构。\n自我充电：中国大学MOOC、大学生自学网、网易公开课、Bilibili等。\n行业技术论坛：开发者——Github、人力资源——中国人力资源开发网、医学论坛——丁香园、考公务员——小木虫论坛等。\n行业报告：咨询行业——麦肯锡、BCG、贝恩、艾瑞网，数据平台行业研究报告——腾讯企鹅智库、阿里研究院、CBNData第一财经商业数据中心、中国社会科学院等。\n医疗资源 常见病查询：丁香医生。\n平日用药：丁香园的用药助手、《世界卫生组织基本药物标准清单》。\n药品专利搜索：国家知识产权局专利检索系统。\n专业医疗健康信息获取：医学导航、丁香园。\n大数据辅助决策 可靠的数据来源：国家统计局官网、中国经济信息网、Wind。\n指数研究：百度指数、阿里指数、微信指数、微指数、猫眼票房指数、爱奇艺指数等。\n购物 历史比价：慢慢买等历史比较和全网比价网站。\n领券：省钱达人、券达人\n折扣信息：自媒体官网。\n核心思想是：判断东西是否便宜，然后如何才能更便宜。\n深度阅读 读了这么多书，还过不好一生，那么读书又有何用？其实这个问题存在的关键在于，很多人的阅读没有和现实经验结合起来。正如著名漫画家 Hugh MacLeod 画的示意图一样：\n图中的小点点表示我们读过的书，或者看过的文章。如果不把这些点串联起来，那么读再多的书，知识都是一盘散沙。相互连接的知识，才能成为有用的经验。所以，关键不是我们阅读的方式，也不在于我们做笔记的方式，而是 如何用抽象的知识，来解决现实中的问题。\n朱丹老师认为做到这一点的核心在于：联系实际，把阅读当作高效的知识检索来操作。完整的知识检索分为四步：搜索——输入——处理——输出，其对应到读书上面就是 提问——阅读——思考——应用（笔记）。\n有了好的阅读方法和书源，加上搜索能力，就可以给自己搭建一个阅读闭环，具体操作包括：寻找好书单——搜索书籍——高效阅读——学会用知识解决问题。\n信息处理 大脑信息分级 信息就是资源。为了快速学习和认知升级，更新信息量，我们需要具备三大能力：\n 快速理解的能力 深入思考的能力 强大的学习能力  借助各种搜索技术、电子工具以及科学的学习方法，我们可以建立高效的信息处理流。不用太在乎自己能记得多少，而是要用更加高级的方法来整合信息、深入思考。换句话说，更高质量的信息来源、更强大的信息处理方法和更精准的思考方式，三者配合，可以帮助我们构建一个高速信息处理系统。系统分为智囊库、外脑和内脑三个层次。核心要素：用搜索获取更多信息，用工具高效掌控知识，用科学的方法提升脑力。\n智囊库：高效的搜索能力 通过搜索打开信息渠道，获取海量信息，使用合适的工具和方法存放初步筛选但还没完全消化的信息。\n外脑：有效整理名片、笔记、书籍等信息 我们的大脑应该只用来思考、理清问题的关键部分。很多琐碎的信息并不需要装进大脑，找个地方放着就好了。打造一个高效外脑的方式：\n 一套高效整齐的信息归纳方法，用来整理名片、文章等容易引起混乱的信息 成体系、有逻辑的电脑文档整理、归纳、迭代体系，用来辅助工作升职、技能提升等 科学、高效的笔记系统，用来整合关键信息，对接内脑的吸收和思考  内脑：集中思考 按照信息输入的流程，我们可以建立一个高效的信息流通通道，通道中的每个步骤都对信息进行整理、删减和归档，最后只需要保留精华部分进入我们的大脑。我们应该用内脑做一些更加深入的工作：\n 系统地梳理知识体系，不要让信息乱作一团 思考提问、寻找解决问题的方法 做决定或创造  整合应用 将搜索到的信息与我们现有知识结合起来 模块化思维 是处理大量知识一个技巧，即从大量知识中提炼出核心要点，将其压缩成一个独立的知识模块，便于快速处理和保存。运用模块化思维的两个步骤：简化归类和总结压缩。\n经济学中有一个基础概念叫做边际收益，它指的是每多拥有一个商品，所增加的收益是递减的。但是对知识和技能的学习而言，学习的边际收益却是递增的。我们可以利用 知识黏合 技巧让知识获取最大的边际收益，知识黏合指的是：相似或相近的知识被快速掌握后，可以形成更大的知识块、知识网。\nSQ3R 阅读法 是美国教育家 Francis Robinson 为了提升军人的学习能力而开发出来的阅读思考模式，后来广泛用于学校教育和成人教育等领域。SQ3R 具体指 Survey（纵览）、Question（提问）、Reading（阅读）、Recite（复述）和 Review（复习）这五个阶段，这是一个不错的学习工具。\n花最少的力气，获得最牢固的知识 分散学习：对于一个大任务，如果把它拆分成一组小任务，则更有可能完成原任务。比如记单词、练听力就非常适合利用碎片化时间学习。利用这种记忆，能保持高涨的记忆兴趣，使大脑处在清醒的记忆状态中，事半功倍。\n间隔性重复 是一种分散学习策略，指的是将大的学习任务拆分为小段，小段之间留出充足的间隔时间供大脑休整，每个一段时间记忆一次。这其实就是一个反复让知识写入大脑的过程。\n番茄工作法：设定时间单位——记录任务完成情况及临时任务——总结。\n联想迁移 通过串联梳理获得知识链，运行比喻联想将知识与生活中的事物配对，通过分类整合将知识打包，构造情境边学边想怎么用。\n通过迁移拓展快速掌握各种技能 我国著名心理学家冯忠良把操作技能分为四个阶段：\n 第一阶段：确定学习对象 第二阶段：快速模仿学习对象的演示和操作 第三阶段：操作整合 第四阶段：熟练练习。将模仿和操作整合不断进行重复、归纳和总结，并注意举一反三  操作技能习得要点：\n 加强正面连接：关注成功，避免失败 建立奖励机制 即使监控，调整难度  信息输出，建立个人品牌 挖掘个人品牌的 5 个切入点，它们也可作为人生规划的参考：\n 价值追求。大到你想成为什么样的人、你希望通过什么样的事情获得认可、你期待中的美好世界长啥样、你如何让这个世界更美好，小到你不自觉关注什么样的事情、你的兴趣爱好、让你从心底高兴的事情等，它们都可以帮你找出自己的价值追求。 潜力挖掘。对于普通人，以下几种能力值得思考和培养：写作能力、审美能力、语言表达能力、信息力。 品牌定位。先弄清楚市场需求，找出价值放大器。 成长规划。不断学习新技能、积累人脉。 给自己写一个故事。故事是最容易影响人的方法，有一个构成好故事的基本公式：好故事 = 愿望 + 行动 + 情感  并不是有了品牌就完事儿，我们还需要借助网络平台进行个人品牌的经营。常见的平台有：领英、公众号、知乎、微博、社群。\n","href":"/notebook/reading_notes/%E8%B6%85%E7%BA%A7%E6%90%9C%E7%B4%A2%E6%9C%AF/","title":"超级搜索术"},{"content":"1.1 Introduction The nature of computers is described in Stanford\u0026rsquo;s introductory course as:\nThe fundamental equation of computers is: computer = powerful + stupid.\nComputers are very powerful, looking at volumes of data very quickly. Computers can perform billions of operations per second, where each operation is pretty simple.\nComputers are also shockingly stupid and fragile. The operations that they can do are extremely rigid, simple, and mechanical. The computer lacks anything like real insight .. it’s nothing like the HAL 9000 from the movies. If nothing else, you should not be intimidated by the computer as if it’s some sort of brain. It’s very mechanical underneath it all.\nProgramming is about a person using their real insight to build something useful, constructed out of these teeny, simple little operations that the computer can do.\n—Francisco Cai and Nick Parlante, Stanford CS101\n调试技巧 Learning to interpret errors and diagnose the cause of unexpected errors is called debugging. Some guiding principles of debugging are:\n Test incrementally: Every well-written program is composed of small, modular components that can be tested individually. Test everything you write as soon as possible to catch errors early and gain confidence in your components. Isolate errors: An error in the output of a compound program, expression, or statement can typically be attributed to a particular modular component. When trying to diagnose a problem, trace the error to the smallest fragment of code you can before trying to correct it. Check your assumptions: Interpreters do carry out your instructions to the letter \u0026mdash; no more and no less. Their output is unexpected when the behavior of some code does not match what the programmer believes (or assumes) that behavior to be. Know your assumptions, then focus your debugging effort on verifying that your assumptions actually hold. Consult others: You are not alone! If you don’t understand an error message, ask a friend, instructor, or search engine. If you have isolated an error, but can’t figure out how to correct it, ask someone else to take a look. A lot of valuable programming knowledge is shared in the context of team problem solving.  1.2 The Elements of Programming 编程语言 A programming language is more than just a means for instructing a computer to perform tasks. The language also serves as a framework within which we organize our ideas about processes. Programs serve to communicate those ideas among the members of a programming community. Thus, programs must be written for people to read, and only incidentally for machines to execute.\n函数与数据 In programming, we deal with two kinds of elements: functions and data. Informally, data is stuff that we want to manipulate, and functions describe the rules for manipulating the data.\n环境 A critical aspect of a programming language is the means it provides for using names to refer to computational objects. If a value has been given a name, we say that the name binds to the value.The possibility of binding names to values and later retrieving those values by name means that the interpreter must maintain some sort of memory that keeps track of the names, values, and bindings. This memory is called an environment. Names can also be bound to functions.\nEnvironments provide the context in which evaluation takes place, which plays an important role in our understanding of program execution.\n函数 Pure functions. Functions have some input (their arguments) and return some output (the result of applying them). Pure functions have the property that applying them has no effects beyond returning a value.\nNon-pure functions. In addition to returning a value, applying a non-pure function can generate side effects, which make some change to the state of the interpreter or computer. A common side effect is to generate additional output beyond the return value.\nSignatures. Functions differ in the number of arguments that they are allowed to take. A description of the arguments that a function can take is called the function’s signature.\n1.6 Higher-Order Functions Functions that manipulate functions are called higher-order functions.\n","href":"/notebook/reading_notes/sicp_in_python/building_abstractions_with_functions/","title":"Building Abstractions with Functions"},{"content":"","href":"/tags/sicp/","title":"SICP"},{"content":"","href":"/tags/%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF/","title":"多处理器编程的艺术"},{"content":"任何互斥协议都会面临一个问题：当无法获得锁时，该怎么做？有两种方案：一种是继续进行尝试，这种锁称为 自旋锁（spin lock），反复检测锁的这个过程被称为自旋（spining）或忙等待（busy waiting），当平均获取锁所花时间较短时，非常适合使用自旋锁。另外一种方案是挂起自己，请求操作系统将另一个线程调度到当前的处理器上，这种方式被称为 阻塞（blocking），由于线程的上下文切换产生的开销很大，所以阻塞适合在获取锁所花时间较长的场景中使用。\nTest-And-Set Locks java.util.concurrent包中的AtomicBoolean类提供了用新值替换旧值并将旧值返回的getAndSet(boolean newValue)方法，传统的testAndSet()指令就相当于是对getAndSet(true)的一次调用。TASLock类描述了一个基于testAndSet()指令的锁算法：\npublic class TASLock implements Lock { AtomicBoolean state = new AtomicBoolean(false); public void lock() { while (state.getAndSet(true)) {} } public void unlock() { state.set(false); } } 为了说明TASLock的缺陷，考虑一种基于总线进行通信的多处理器架构：处理器和存储控制器都可以在总线上广播，但是一个时刻只能有一个处理器（或存储控制器）在总线上广播。所有的处理器（或存储控制器）都可以监听。每个处理器都有一个高速缓存（cache），当处理器读取某个地址的数据时，首先会检查该地址及其所存储的数据是否已在它的cache中。如果cache命中，就直接读取，否者需要在其它处理器的cache或内存中查找这个数据。为了在其它处理器的cache中找到需要的数据，处理器会在总线上广播这个地址，如果监听总线的处理器中有一个在自己的cache中发现了这个地址，就会广播该地址来进行响应。如果所有的处理器中都没有发现这个地址，则会去读取内存中该地址所对应的值。\n在TASLock中，每个getAndSet()调用相当于总线上的一次广播，由于所有的线程都需要通过总线和内存来进行通信，所以getAndSet()会延迟所有的线程（包括那些没有等待这个锁的线程）。更加糟糕的是，getAndSet()强制其它处理器丢弃自己cache内锁的副本，因此每个自旋的线程几乎每次都会遭遇cache缺失，然后必须从总线获取新的但未改变的值。当持有锁的线程尝试释放锁时，也有可能会因总线被其它自旋的线程独占而被延迟。\nTTASLock是一种改进后的算法，它没有直接调用getAndSet()，而是由线程反复地查询锁地状态直到锁是空闲的（即直到get()返回false），这种技术被称为：Test-Test-And-Set:\npublic class TTASLock implements Lock { AtomicBoolean state = new AtomicBoolean(fasle); public void lock() { while (true) { while (state.get()) {} if (!state.getAndSet(true)) { return; } } } public void unlock() { state.set(false); } } TTASLock的性能比TASLock要好，当锁被线程A持有后，线程B在第一次检查锁的状态时会发生cache缺失，从而阻塞并等待值被载入它的cache中。只要A不释放锁，B就会一直读取同一个值并且每次都命中cache，这样就不会产生总线流量，也不会降低其它线程的内存访问速度。此外，释放锁的线程也不会被正在该锁上自旋的线程后延。\n竞争（Contention） 当多个线程同时尝试获取一个锁时，就会发生竞争。同时争抢同一个锁的线程越多，竞争就越激烈，为了减轻竞争程度，可以让获取锁的线程主动“沉默”一段时间，然后再尝试获取锁。那么线程应该“沉默”多久呢？有一个好的准则：尝试获取锁的失败次数越多，竞争可能就越激烈，“沉默”的时间就应该越长。\n队列锁（Queue Locks） 基于Test-And-Set的锁存在两个问题：\n Cache-coherence Traffic：所有在同一个共享位置上自旋的线程在成功访问锁之后都会带来cache-coherence traffic。 Critical Section Underutilization：所有的线程都会出现不必要的延迟，这回导致临界区资源利用不足。  可以将线程组织为一个队列来克服这些缺点。在队列中，每个线程通过检测前一个线程是否完成来判断是否轮到自己，这样一来，每个线程在不同的存储单元上自旋，降低了cache-coherence traffic。此外，每个线程不需要主动判断何时访问临界区，因为队列中的前一个线程会通知它，这提高了临界区资源的利用率。\n基于数组的队列锁 以下是一个基于数组的简单队列锁ALock：\npublic class ALock implements Lock { ThreadLocal\u0026lt;Integer\u0026gt; slotIndex = new ThreadLocal\u0026lt;Integer\u0026gt;(){ protected Integer initialValue() { return 0; } }; AtomicInteger tail; boolean[] flag; int size; public ALock(int capacity) { size = capacity; tail = new AtomicInteger(0); flag = new boolean[capacity]; flag[0] = true; } public void lock() { int slot = tail.getAndIncrement() % size; slotIndex.set(slot); while (!flag[slot]) {}; } public void unlock() { int slot = slotIndex.get(); flag[slot] = false; flag[(slot + 1) % size] = true; } } 在ALock中，tail域被所有线程共享，其初始值为0。为了获得锁，每个线程原子的增加tail域，所得结果值称为槽，槽将作为布尔数组flag的索引。若flag[slot]=true，那么槽为slot的线程就有权获得锁。初始状态下，flag[0]=true。为了获得锁，线程不断自旋，直到它的槽在flag数组中变为true。释放锁时，线程将它的槽在flag数组中对应的位置设置为false，并将下一个槽位在flag数组中的对应位置设置为true。\nCLH队列锁 ALock保证了先来先服务的公平性，但空间利用率却不怎么好。它要求事先知道并发线程的最大数量n，并且为每一个锁都分配一个大小为n的数组。因此，同步L个不同的对象需要O(Ln)的空间，即使一个线程一次只访问一把锁也是这样。\nCLHLock是另一种不同类型的队列锁，它提高了空间利用率：\npublic class CLHLock implements Lock { AtomicReference\u0026lt;QNode\u0026gt; tail; ThreadLocal\u0026lt;QNode\u0026gt; prev; ThreadLocal\u0026lt;QNode\u0026gt; node; public CLHLock() { tail = new AtomicReference\u0026lt;QNode\u0026gt;(new QNode()); prev = new ThreadLocal\u0026lt;QNode\u0026gt;() { protected QNode initialValue() { return new QNode(); } }; node = new ThreadLocal\u0026lt;QNode\u0026gt;() { protected QNode initialValue() { return null; } }; } public void lock() { QNode qNode = node.get(); qNode.locked = true; QNode prevNode = tail.getAndSet(qNode); prev.set(prevNode); while (prevNode.locked) {} } public void unlock() { QNode qNode = node.get(); qNode.locked = false; node.set(prev.get()); } } 类中的QNode对象的布尔型locked域中记录了对应线程的状态，若为true，则表示相应的线程要么已经获得锁，要么正在等待锁，若为false，则表示相应的线程已经释放锁。锁本身被表示成一个由QNode对象构成的虚拟链表。这里的链表是 隐式的：每个线程通过Threadlocal类型的prev变量引用其前驱，共享变量tail指向的是最新加入队列的节点。\n若要获得锁，线程将其QNode的locked域设置为true，表示该线程不准备释放锁。线程在tail域上执行getAndSet()并使自己的node成为队列中的tail，与此同时，线程还会获得一个指向前驱QNode的引用。最后，线程会监视其前驱，在其前驱的QNode的locked域上自旋，等待锁被释放。\n若要释放锁，线程将其node的locked域设为false，这样线程的后继就能够获得锁了。然后回收前驱的QNode，等待下次使用。\n如果有L个锁，且每个线程每次最多访问一个锁，与ALock所需要的O(Ln)空间相比，CLHLock只需要O(L+n)的空间。\nMCS队列锁 MCSLock的锁也被表示为QNode对象的链表，与CLHLock不同的是，MCSLock中的链表不是隐式的：整个链表通过QNode对象里的next域连接。\npublic class MCSLock implements Lock { AtomicReference\u0026lt;QNode\u0026gt; tail; ThreadLocal\u0026lt;QNode\u0026gt; node; public MCSLock() { tail = new AtomicReference\u0026lt;QNode\u0026gt;(null); node = new ThreadLocal\u0026lt;QNode\u0026gt;() { protected QNode initialValue() { return null; } }; } public void lock() { QNode qNode = node.get(); QNode prev = tail.getAndSet(qNode); if (prev != null) { qNode.locked = true; prev.next = qNode; while (qNode.locked) {} } } public void unlock() { QNode qNode = node.get(); if (qNode.next == null) { if (tail.compareAndSet(qNode, null)) return; while (qNode.next == null) {} } qNode.next.locked = false; node.next = null; } class QNode { boolean locked = false; QNode next = null; } } 若要获得锁，线程将自己的QNode添加到链表的尾部。若队列原先不为空，则将其前驱QNode的next域设置为当前线程自己的QNode，然后在自己QNode的locked域上自旋等待，直到前驱将该域设为false为止。\n释放锁时，线程会先检查QNode的next域是否为空，如果是，则表示当前没有其它线程正在争用该锁，或当前存在一个争用该锁的线程，但这个线程运行得很慢。为了区分这两种情况，令qNode为当前线程的QNode，对tail域调用compareAndSet(qNode, null)，如果调用成功，则表示没有其它线程正在试图获得锁，方法会tail域会设置为null后直接返回。否则，说明是第二种情况，方法会自旋等待，直至结束。在任何一种情况下，一旦出现了后继，就将后继节点的locked域设置为false，表明当前锁是空闲的。\nMCSLock具有CLHLock的优点，空间复杂度也为O(L+n)。和CLHLock相比，MCSLock在释放锁时也需要自旋，指令条数也更多。\n","href":"/notebook/reading_notes/the_art_of_multiprocessor_programming/spinlock/","title":"自旋锁"},{"content":"加速比 : 对于某一项工作，加速比指的是一个处理器完成该工作所用的时间与采用n个处理器并行完成该工作所用时间的比值。\nAmdahl定律给出了n个处理器协同完成一项工作时可获得的最大加速比 S，其中 **p**表示的是该工作中可以被并行执行的部分。假设完成该工作需要 **1**个单位的时间，那么串行部分所需要的时间为 1-p，而并行部分需要的时间为 p/n。因此，采用并行处理之后，完成整个工作所需要的时间为：\n$$ 1 - p + \\frac{p}{n} $$\n进一步可以得出加速比：\n$$ S = \\frac{1}{1 - p + \\frac{p}{n}} $$\nAmdahl定律告诉我们：完成复杂工作可获得的加速比是有限的，并且受限于这个工作中必须被串行执行的部分。\n对于一个给定的问题以及一台具有10个处理器的机器，由Amdahl定律可知，即使其中的90%可以并行，最终也只能获得5倍的加速比，而不是10倍：\n$$ S = \\frac{1}{1 - p + \\frac{p}{n}} = \\frac{1}{1 - 0.9 + \\frac{0.9}{10}} = \\frac{1}{1-0.9+0.09} \\approx 5 $$\n因此，要获得更高的加速比，就要尽可能地使得串行部分达到最大程度得并行。在并发程序设计里，我们也应该在保证程序正确地情况下尽可能地减少串行部分（ 同步代码块所包含指令数量）。\n","href":"/notebook/reading_notes/the_art_of_multiprocessor_programming/amdahls_law/","title":"Amdahl定律"},{"content":"","href":"/tags/tomcat/","title":"Tomcat"},{"content":" 这里记录了将Tomcat源码导入IDEA并启动容器的过程。\n 准备环境 为了将Tomcat的源码导入IDEA，我们需要先准备好JDK、Intellij IDEA和Apache Ant。不同版本的Tomcat对JDK和Ant的版本要求有所不同，具体的版本要求可以在Tomcat官网的文档里找到，例如：Tomcat 9对工具的要求。\n在安装完JDK和Ant之后，我们还需要设置两个环境变量：\n JAVA_HOME：指向JDK的安装目录。 ANT_HOME：指向Ant的安装目录。  为了使用java和ant命令，我还需要将%JAVA_HOME%\\bin和%ANT_HOME%\\bin添加到Windows系统的PATH环境变量里去，或者将${JAVA_HOME}/bin和${ANT_HOME}/bin添加到Linux系统的PATH环境变量里去。\n获取Tomcat源码 我所用的源码是直接从Github下载的，对应的是master分支。此外，还可以到Tomcat官网下载特定版本的源码。\n构建Tomcat 配置构建参数 将Tomcat源代码目录中的build.properties.default复制一份为build.properties并根据实际情况进行修改。在构建Tomcat的过程中，会有很多的依赖库被下载。默认情况下，这些依赖库将会被下载到：${user.home}/tomcat-build-libs中，这个位置可以通过修改配置项base.path的值来改变。我没有对这个文件进行修改，一切都是用的默认值。\n构建Tomcat 首先切换到Tomcat源代码的根目录。我们可以以默认的方式构建Tomcat，只需要执行ant命令即可。但是这样构建完之后直接导入IDEA是会有问题的。为了将代码导入IDEA，我们需要执行ant -buildfile build.xml ide-intellij。这告诉Ant将ide-intellij作为此次构建的目标。我们不妨来看一下相关内容：\n\u0026lt;target name=\u0026#34;ide-intellij\u0026#34; depends=\u0026#34;download-compile, download-test-compile\u0026#34; description=\u0026#34;Creates project directory .idea for IntelliJ IDEA\u0026#34;\u0026gt; \u0026lt;copy todir=\u0026#34;${tomcat.home}/.idea\u0026#34;\u0026gt; \u0026lt;fileset dir=\u0026#34;${tomcat.home}/res/ide-support/idea\u0026#34;/\u0026gt; \u0026lt;/copy\u0026gt; \u0026lt;echo\u0026gt;IntelliJ IDEA project directory created. Please create PATH VARIABLES for ANT_HOME = ${ant.home} TOMCAT_BUILD_LIBS = ${base.path} \u0026lt;/echo\u0026gt; \u0026lt;/target\u0026gt; 命令执行完之后，终端会提醒我们创建ANT_HOME和TOMCAT_BUILD_LIBS这两个PATH变量。\n将源码导入IDEA 按照上面的步骤构建完Tomcat之后，我们就可以直接将Tomcat源码导入IDEA了。\n若不设置ANT_HOME和TOMCAT_BUILD_LIBS，直接构建工程的结果就是各种找不到依赖。打开Project Structure -\u0026gt; Project Settings -\u0026gt; Modules -\u0026gt; tomcat -\u0026gt; Dependencies你会发现这里罗列的依赖项都是红色的(IDEA找不到这些依赖)。\n打开File -\u0026gt; Settings -\u0026gt; Appearance \u0026amp; Behavior -\u0026gt; Path Variables。添加ANT_HOME和TOMCAT_BUILD_LIBS两条变量，值分别设置为ant.home和base.path的实际值。然后重新构建就OK了。\n启动Tomcat Tomcat的启动类为org.apache.catalina.startup.Bootstrap。直接运行是会报错的。\n其实在执行ant命令之后，源代码根目录下面的output的文件夹下会出现一个名为build的文件夹。它的路径就是运行 Bootstrap 类所需要的catalina.home参数。例如，我的机器上build的路径为C:/Code/Apache/tomcat/output/build。所以将-Dcatalina.home=\u0026quot;C:/Code/Apache/tomcat/output/build\u0026quot;添加到VM options中之后，就可以正常启动Tomcat了，然后使用浏览器访问http://127.0.0.1:8080就能看到那只猫了。\n不过你会发现，后台会出现一堆乱码。解决思路可以参考：编译Tomcat9源码及tomcat乱码问题解决。他使用-Dfile.encoding=UTF8 -Duser.language=en -Duser.region=US这三个参数绕过了乱码的问题。\n","href":"/posts/getting_started/import_tomcat_source_code_into_intellij/","title":"将Tomcat源码导入IDEA"},{"content":"","href":"/categories/%E6%96%B0%E6%89%8B%E4%B8%8A%E8%B7%AF/","title":"新手上路"},{"content":"安全环境 威胁 信息系统的安全性目标通常可以分解为三个部分:\n 机密性(confidentiality)：是机密的数据处于保密状态。系统应该保证数据不被未经授权的用户访问。 完整性(integrity)：未经授权的用户无法在未经数据拥有着授权的情况下修改任何数据。这里的数据修改不仅包括修改数据本身，还包括删除或添加错误数据。 可用性(availability)：没有人可以扰乱系统使之不可用。  这三个部分通常被称为CIA。它们受到的威胁各不相同：\n   目标 威胁     数据机密性 数据暴露   数据完整性 数据篡改   系统可用性 拒绝服务    操作系统安全 一般而言，攻击可以分为被动攻击和主动攻击。被动攻击试图窃取信息，而主动攻击会使计算机行为异常。加密(cryptography) 以某种方式对消息或文件进行转码，使攻击者在没有密钥的情况下很难恢复出原内容。加固(hardening) 是指在程序中加入保护机制以加大攻击者破坏程序的难度。\n可信计算基 可信系统(trusted systems) 在形式上申明了安全要求并满足了这些安全要求。每一个可信系统的核心是最小的可信计算基(Trusted Computing Base, TCB)，它包含了实施所有安全规则所必须的硬件和软件。\n保护机制 保护域 计算机系统内有许多需要保护的“对象”，这些对象可以是硬件，也可以是软件。权限 是进行某个操作的许可。为了讨论不同的保护机制，需要引入域的概念。域(domain) 是(对象，权限)对的集合，每一个(对象，权限)对都制定了一个对象和一些可在其上进行的操作的子集。\n需求决定对象被分配到的域，一个基本的原则就是最少权限原则(Principle of Least Authority, POLA)。一般而言，当每个域中只含有完成其工作所需要的 最少 的对象和权限时，安全性最好。\n为了跟踪对象所属的域，我们可以建立一个矩阵，矩阵的行代表域，列代表对象，然后在矩阵中的每个元素中列出对象在该域中所包含的权限。例如，下面的表中展示了三个域和8个对象：\n    File1 File2 File3 File4 File5 File6 Printer1 Plotter2     Domain 1 R RW         Domain 2   R RWX RW  W    Domain 3      RWX W W    在实际应用中，很少采用上述矩阵的方式来存储权限信息，因为矩阵过大并且过于稀疏，非常浪费空间。但还有两种可行方法，它们通过按行或按列来存储这个矩阵并只保存非空元素。\n访问控制列表 先来看按列存储，为每个对象关联一个(有序)列表，列表里面包含了所有可访问该对象的所有域以及这些域访问该对象的方法。这个列表就是访问控制列表(Access Control List, ACL)。例如：下图展示了三个进程A、B、C，每个进程都属于不同的域。F1、F2、F3为三个文件，每个文件都有一个相关联的ACL。若假设一个域相当于一个用户，那么我们就有用户A、B、C，在安全性语言里，用户通常被叫做主体(subject或principal)，而它们所拥有的为对象(object)(比如文件)。\n很多系统中也支持 组(group) 的概念，组可以有自己的名字并包含在ACL中。在某些系统中，每个进程除了UID外，还有GID。这类系统中的ACL可能长这样：UID1, GID1: rights1; UID2, GID2: rights2; \u0026hellip;\n这样，当出现访问对象的请求时，必须同时检查调用者的UID和GID。如果它们都出现在ACL中，所列出的权限就是可行的，否则访问会被拒绝。组也可以看作是**角色(role)**的概念。\n权能字 现在来看按行存储的方式。这个时候与每个域(或进程)关联的是可访问的对象列表，以及进程可在对象上执行的操作。这样一来，每个进程就具有一个权能字列表(capability list or C-list)，里表中的每个单独的元素就是一个权能字(capobility)。下图展示了A、B、C三个进程的权能字，每一个权能字赋予所有者在特定对象上的权限：\n很明显，操作系统必须防止用户篡改权能字列表。已知的保护方法有三种:\n 建立带标记的体系结构。在这种硬件设计中，每个内存字必须拥有额外的位(或标记位)来判断该字是否包含了权能字。标记位不能被算术、比较等指令使用，仅可以被在内核态下运行的程序修改。 在操作系统内保存权能字列表，随后根据权能字在列表中的位置引用权能字。 把权能字列表放在用户空间中，并用加密的方法进行管理，这样用户就不能篡改它们。这种方法比较适合分布式操作系统。  密码学原理 加密的目的是通过某种手段将 明文(plaintext) 加密成 密文(ciphertext)，只有被授权的人才知道如何将密文恢复为原来的明文。而对于无关的人来说，密文就是一段无法理解的编码。\n在算法中使用的加密参数叫做密钥(key)。如果用P表示明文， $K_E$ 表示加密密钥，C表示密文，E表示加密算法，那么加密就可以定义为：$C = E(P, K_E)$ 。荷兰密码学家Kerckhoffs于19世纪提出了Kerckhoffs原则，认为：加密算法本身应该完全公开，而加密的安全性由独立于加密算法之外的密钥决定。\n同样地，若用D表示解密算法， $K_D$ 表示解密密钥，则 $P = D(C, K_D)$ 。加密与解密运算地关系如下：\n私钥加密技术 或 对称加密技术 使用同一个密钥进行加密和解密，而 公钥加密技术 中地加密密钥和解密密钥是不同的。\n认证 每一个安全的计算机系统一定会要求所有的用户在登录的时候进行身份认证。最简单的认证方式就是通过密码认证，此外还有像智能卡这样的物理认证以及像指纹和虹膜这样的生物识别认证。\n软件漏洞 入侵用户计算机的主要方法之一就是利用系统中运行的软件的漏洞，使其做一些违背开发者本意的事情。缓冲区溢出攻击是一种非常常见的利用软件漏洞发起攻击的方式。\n恶意软件 恶意软件(malware) 是一个统称，包括木马、病毒、蠕虫一起其它能够产生恶意行为的软件。\n特洛伊木马 很多攻击者会编写一些有用的程序，并将恶意代码嵌入其中，然后提供免费下载。当人们下载并安装这些程序之后，恶意代码也就存在于用户的计算机上了，这种攻击方式叫做木马攻击(Torjan horse attack)。\n病毒 病毒(virus) 是一种特殊的程序，它可以通过将自己植入其它应用程序中来进行“繁殖”，就像生物界中真正的病毒一样。常见的病毒有七种：\n 共事者病毒(companion virus)并不真正感染程序，但当程序执行时它也执行。 可执行程序病毒(executable program virus)会感染可执行程序。其中最简单的一种叫做覆盖病毒(overwriting virus)，还有寄生病毒(parasitic virus)以及空腔病毒(cavity virus)。 内存驻留病毒(memory-resident virus)总是驻留在内存中。 引导扇区病毒(boot sector virus)可以覆盖主引导记录或者引导扇区，并能造成灾难性的后果。 设备驱动病毒(device driver virus)会感染设备驱动程序。 宏病毒(macro virus)利用宏文件执行恶意操作。 源代码病毒(source code virus)的移植性最强，当程序运行时，病毒也会被调用。  蠕虫 蠕虫(worm) 是一个能够自我复制的程序，能够在迅速完成自我复制，然后迅速传染到所有的机器。\n","href":"/notebook/reading_notes/modern_operating_systems/security/","title":"Security"},{"content":"","href":"/tags/%E7%8E%B0%E4%BB%A3%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","title":"现代操作系统"},{"content":"","href":"/tags/os/","title":"OS"},{"content":"从计算机诞生之日起，人们就无休止地追求着更强大地计算能力。过去地解决方案是使时钟走得更快，但是现在时钟速度已经很难再进一步提升了。计算机虽然可以变得很小，但是会遇到散热问题：计算机运行得越快，产生的热量就越多，而计算机越小散热越难。总而言之，对于计算机的时钟来说：从1MHz到1GHz需要的是更好的芯片制造工艺，而从1GHz到1THz需要的是完全不同的方法。\n获得更强计算能力的另一种方法是大量使用并行计算机。这些机器都拥有很多CPU，虽然每个CPU的速度都和普通的CPU一样，但是它们总体的计算能力比单个CPU要强大得多。\n电子部件之间的所有通信的本质就是它们之间发送具有良好结构定义的位串，其差别在于所涉及的时间范围、距离范围和逻辑组织。上图展示了三种不同的并行计算机系统模型：\n (a)图展示了一个极端的例子，系统中的所有CPU通过共享内存通信，每个CPU都可以读写整个物理存储器，访问一个内存字通常需要1~10ns。 在(b)图所示的系统中，许多CPU-内存通过高速互联网络连接在一起，这种系统被称为消息传递型计算机。每个内存都局部对应一个CPU，并且只能被该CPU访问。这些CPU通过互联网络发送多字信息通信，一条短消息通常可在10~50μs内发出，但是内存访问时间仍然比(a)中系统要长。 (c)中所有的计算机系统都通过一个广域网连接，形成一个分布式系统。每台计算机都有自己的内存，计算机之间通过消息传递进行通信，消息传递的时间通常为10~100ms。如此长的延迟会造成使用这类松耦合系统的方式和(b)中的紧耦合方式不同。  三种类型的系统在通信延迟上各不相同，分别有三个数量级的差别。\n多处理机(MULTIPROCESSORS) **共享内存多处理机(shared-memory multiprocessor)**是一种多个CPU以完全共享地方式访问同一个公用RAM的计算机系统。运行在任何一个CPU上的程序都能看到一个普通的虚拟地址空间。\n多处理机硬件 所有的多处理机中的每个CPU都可以访问全部的共享内存。若机器上读取每个内存字(memory word)的速度都是一样快的，则称这种机器为UMA(Uniform Memory Access)，否则称这种机器为NUMA(Nonuniform Memory Access)。\nUMA 基于总线的UMA 下图展示了三种基于总线的UMA:\n最简单的多处理机是基于单总线的(图(a))。多个CPU以及一个或多个内存模块通过公用的总线进行通信。当一个CPU需要读取一个内存字时，它会先检查总线是否空闲。如果总线忙，它就等待直到总线空闲。当总线空闲时，CPU把所需字的地址放到总线上，发出控制信号，然后等待内存将它所需的字放到总线上。这种设计存在问题，当CPU数量不多时，总线的使用权是很好管理的，一旦CPU数量过多，总线的管理就变得异常复杂。此外，总线成为了系统的瓶颈，多数CPU在大部分时间内是空闲的。\n为了解决这一问题，可以为每个CPU添加一个高速缓存(图(b))。CPU可以直接从高速缓存中读取数据，总线流量大大减少。一般而言，高速缓存的读取单位不再是单个字，而是一个高速缓存行(cache line)。为了防止数据出现不一致现象，需要执行缓存一致性协议。\n图(c)中的每个CPU不仅有一个高速缓存，还有一个私有内存。CPU通过专门的私有总线访问这个私有内存。为了利用这一配置，编译器应该把所有程序的代码、字符串、常量以及其它只读数据放入私有内存，而将程序的共享变量放入共享内存。在多数情况下，这种配置可以极大的减少总线流量，但是这需要编译器的积极配合。\n其它UMA 除了基于总线的UMA，还有基于交叉开关和多级交换网络的UMA。\n连接n个CPU和k个内存的最简单的电路就是交叉开关(crossbar switch)。将n个CPU和k个内存排列成一个n×k的矩阵，矩阵中的每个位置都是一个交叉点(crosspoint)。交叉点是一个小的电子开关，其开关状态取决于对应的CPU和内存是否需要连接。交叉开关是一个非阻塞网络，即不会因为有些交叉点被占据而拒绝连接。当两个CPU同时访问同一个内存时，还是可能出现竞争的。不过通过将内存分为k个单元，竞争的概率降低到了1/k。交叉开关的缺点是：交叉点的数量会以n×k的方式增长，对于大规模的系统并不适用。\n**多级交换网络(multistage switching networks)**是另外一种完全不同的设计，由2×2的交叉开关构建。对于一个2×2的开关(两输入和两输出)，到达任意一个输入的消息可以被交换到任意一个输出上。omega网络就是典型的多级交换网络。\nNUMA 单总线的UMA多处理机通常只限于拥有数十个CPU的系统，交叉开关和多级交换网络需要大量(昂贵)的硬件，并且也不能太大。为了支持100个及更多的CPU，必须做一些改变，这就导致了NUMA的出现。NUMA和UMA一样，都为所有的CPU提供了蛋依的地址空间，但是在NUMA中，对局部内存模块的访问要比远端内存模块的访问要快。虽然为UMA编写的程序可以不经修改就在NUMA机器上运行，但是其效率会比在UMA上差。\n所有的NUMA机器都具备一下三种关键特性，他们也是NUMA与其它多处理机的主要区别：\n 具有对所有CPU都可见的单个地址空间。 通过LOAD和STORE指令访问远端内存。 对远端内存的访问慢于局部内存。  若机器必须访问远端内存(因为需要的数据没有被缓存)，系统被称为NC-NUMA (Non Cache-coherent NUMA)。当cache处于一致状态时，系统被称为CC-NUMA (Cache-Coherent NUMA)。\n多核芯片 随着芯片制造技术的发展，晶体管的体积越来越小，从而有可能将越来越多的晶体管放入一个芯片中。随后的问题是：“如何利用这些晶体管？”，其中的一个选择时给芯片添加cache。当cache的大小达到一定程度时，继续增加cache的大小虽然有可能提高命中率，却不能显著提升应用的性能。另一个选择是将两个或更多的CPU(通常称为核(core))放到同一个芯片上。在多核(multicore)芯片中，cache仍然是至关重要的，并且遍布整个芯片。多核芯片通常被称为片级多处理机(Chip-level MultiProcessors, CMP)。\n众核芯片 “多核”只是简单的表示核的数量多于一个，但是当核的数量持续增加时，它们就就有了另一个名称——众核。**众核(manycore)**芯片是指包括几十、几百甚至成千上万个核心的多核处理器。\n超大量核带来了一个问题：用来保持缓存一致性的机制会变得异常的复杂。很多工程师担心硬件上保持缓存一致性的开销会非常高，以至于这些新增的核并不能带来多大的性能提升，因为处理器一直忙于维护缓存状态的一致性。更加糟糕的是，保持缓存目录的一致性还将消耗大量的内存，这就是著名的一致性堡垒(coherency wall)。\nGPU是目前最常见的众核处理器，它拥有专用的内存和成千上万个微小的核。\n多处理机操作系统 每个CPU都有自己的操作系统 组织一个多处理机操作系统的最简单办法可能就是：静态的将内存划分为和CPU一样多的部分，为每个CPU提供私有内存及私有操作系统副本，以n各CPUn各独立计算机的形式运行。这样的优点是：允许所有的CPU共享操作系统的代码，而且只需要提供数据的私有副本。\n这一机制比有n个分离的计算机要好，因为它允许所有的机器共享同一套磁盘、内存和其它I/O设备。但这一设计存在不少潜在的问题：因为进程不是共享的，所以可能出现一个CPU过载而其它CPU空载；因为物理页也不是共享的，所以可能一个CPU不断地进行页调度而另一个CPU有多余的页；由于操作系统独立的维护近期使用过的磁盘块的cache，可能出现某一修改过的磁盘块同时出现在多个cache中，这回导致不一致的结果。避免这一问题的唯一途径是取消cache，但这样会显著降低性能。\n主从多处理机 下面展示的模型是主从模型(master-slave)，CPU1为主CPU，其它为从CPU：\n在这种模型下，操作系统的一个副本及其数据都都在CPU1上，用户进程运行在其它CPU上。随后所有的系统调用都被重定向到CPU1上，如果CPU1上有剩余的时间片，还可以运行用户进程。\n主从模型解决了分区模型中的多数问题，但是主CPU成为了系统的瓶颈，因为它要处理所有的系统调用。这种模型对小型多处理机是可行的，但不适用于大型多处理机。\n对称多处理机 对称多处理机(Symmetric MultiProcessor, SMP) 消除了上述的不对称性，在内存中有操作系统的一个副本，但任何CPU都可以运行它。当有系统调用时，进行系统调用的CPU陷入内核并进行处理。\n这个模型动态地平衡进程和内存，因为它只有一套操作系统数据表。它还消除了主CPU带来的瓶颈问题。因为没有主CPU，所以需要解决多个CPU可能同时运行操作系统代码的问题。最简单的办法就是在操作系统中使用互斥信号量，如果互斥信号量被锁住，就等待。按照这种方式，任何CPU都可以运行操作系统，但在任一时刻只有那个获得锁的CPU可以运行操作系统，这一方法被称为大内核锁(Big Kernel Lock, BLK)。\n多处理机调度 亲和调度(affinity scheduling)：尽量使一个线程在它前一次运行过的同一个CPU上运行。\n群调度(Gang scheduling)：把一组相关的线程作为一个单位(群, gang)一起调度，使一个群内的所有线程在不同的CPU上同时运行，并且群中的所有线程共同开始和结束其时间片。\n多计算机 多处理机提供了一个简单的通信模型：所有的CPU共享一个共用内存。进程可以向内存写消息，然后消息可以被其它进程读取，可以使用互斥锁、信号量、管程和其它合适的技术进行同步。唯一美中不足的是：大型多处理机构造困难，造价高昂。\n为了解决CPU数量不断增加的问题，人们在 多计算机(multicomputers) 领域进行了很多研究。多计算机的CPU紧耦合在一起，并且不共享内存，每个CPU都有自己的私有内存，CPU之间通过互联网络连接。这些系统拥有各种其它的名称，如机群计算机(cluster computers)、工作站机群(Clusters of Workstations, COWS)。\n多计算机很容易构造，因为其基本部件只是一台配有高性能网卡的PC裸机，不需要键盘、鼠标或显示器。\n多计算机硬件 一个多计算机系统的基本节点包括一个CPU、内存、一个网卡、有时还有一块硬盘。\n互连技术 每个节点都有一块网卡，通过电缆或光纤连接到其它的节点或者交换机上。常见的互连拓扑结构有：星型、环型、网格、双凸面、立方体和四维超立方体等。\n在多计算机中可采用两种交换机制：存储转发包交换和电路交换。\n网络接口 在多计算机中，每个节点都有一块网卡，网卡上一般会有一块用来存储进出包的RAM。通常，在包被传送到第一个交换机之前，这个包必须被复制到网卡的RAM中。这么做的原因是许多互连网络是同步的，这意味着一旦一个包开始传送，比特率就必须以恒定的速率持续进行。如果包在主RAM中，由于内存总线上其它的信息流的存在，所以不能保留送到网络上的流是连续的，而网卡上专用的RAM就解决了这个问题。同样的问题也出现在包的接收过程中。\n底层通信软件 在多计算机中包的过度复制是高性能通信最大的敌人。在最好的情况下，包只需要3次复制：源RAM到源网卡、源网卡到目的网卡、目的网卡到目的RAM，这一过程还必须不涉及任何的存储转发并且网卡是被映射到用户空间的。如果网卡被映射到内核空间而不是用户空间，那么用户进程只能通过发出一个陷入到内核的系统调用来完成包的复制，这样一来，包复制的次数就变成了5次。\n用户层通信软件 在多计算机中，不同CPU上的进程通过互相发送消息来通信。在最简单的情况下，这种消息传送是暴露给用户进程的。换句话说，操作系统提供了一种发送和接收消息的途径，而库过程调用使得这些底层的调用可以被用户进程使用。\n发送和接收 这是最简化的情形，操作系统提供两个库调用给用户进程：一个用户发送消息，另一个用于接收消息。\n阻塞调用和非阻塞调用 阻塞调用有时候称同步调用。当一个进程调用发送方法时，它会阻塞直到消息完全发送出去，然后继续执行发送方法后面的指令。接收过程也类似。\n相对于阻塞调用的另一种方式是非阻塞调用，又称异步调用。由于进程在传输过程中可能重写缓冲区，非阻塞调用必须保证缓冲区的安全。有三种解决方案：\n 让内核复制消息到内核缓冲区，然后让进程继续。 在发送消息完全之后中断发送者，告知其缓冲区又可以使用了。 让缓冲区采用Copy-on-Write的方式。  分布式系统 分布式系统(distributed system) 与多计算机类似：每个节点都有自己的私有内存，整个系统中没有共享的物理内存。但是分布式系统中节点的耦合度比多处理机更低。分布式系统构建在计算机网络的上层，通过在操作系统的顶部加一层叫做 中间件(middleware) 的软件来处理不同硬件与操作系统的差异，为应用程序和用户提供一致的范型。从某种程度上来看，中间件就是一个分布式操作系统。\n基于文档的中间件 万维网(World Wide Web) 可以看作是一个分布式操作系统。Web本质上是一个客户端-服务器系统，其背后得原始范型却非常简单：每个计算机可以持有一个或多个被称为 Web页面(Web page) 的文档，每个页面中有文本、图像、音视频等，还有链接到其它页面的超链接(hyperlink)。用户通过浏览器向服务器请求Web页面，并可以通过页面中的超链接跳转到其它页面。由于超链接的存在，Web本身成为了一个由文档构成的巨大的有向图。每一个Web页面都有一个唯一的地址，称为统一资源定位符(Uniform Resource Locator, URL)，其具体形式为protocol://DNS-name/file-name，常见的协议有HTTP、FTP等，URL唯一指定一个文件。\n当用户在Web浏览器中键入一个URL时，浏览器按照一定的步骤调取所请求的Web页面。例如，当访问http://www.minix3.org/getting-started/index.html时，浏览器按照以下步骤取得所需要的页面：\n 浏览器向DNS询问www.minix3.org的IP地址。 DNS告诉浏览器IP地址为66.147.238.215。 浏览器建立一个到66.147.238.215上端口80的TCP连接。 浏览器发送对文件getting-started/index.html的请求。 www.minix3.org服务器发送文件getting-started/index.html给浏览器。 浏览器展示getting-started/index.html中的文本内容。 同时，浏览器获取并展示页面上的所有图片。 浏览器释放TCP连接。  基于文件系统的中间件 Web背后得思想是使一个分布式系统看起来像一个巨大的超链接集合。另一种处理方式则是使一个分布式系统看起来像一个巨大的文件系统。\n采用文件系统模型的分布式系统意味着只存在一个全局文件系统，用户能够读取其权限集合内的文件。一个进程将数据写入文件，另一个进程从文件中读出数据，进程之间的通信就实现了。\n传输模型 上传/下载模型(upload/download model) 和 远程访问模型(remote-access model) 是不同的两种传输模型。\n在第一种模型中，客户端通过将远程服务器上的文件复制到本地来实现对远程文件的访问。如果只是读文件，读取操作在本地完成之后就完事儿(为了高性能)。而如果是写文件的话，写入操作也在本地进行，当操作完成后，新的文件会被上传回服务器。上传/下载模式的优点是简单，而且一次性传送整个文件比只传输一小块更有效率。由于需要在本地存放整个文件，客户端必须拥有足够的空间。即使只需要文件的一小部分也需要整个文件，这是一种浪费，也是这种模型的缺点。\n在第二种模型中，文件始终位于服务器上，客户端通过向服务器发送相关指令来完成各项操作。\n基于对象的中间件 和前面的一切皆文档或一切皆文件不同，基于对象的中间件的思想是一切皆对象。对象是变量的集合，这些变量与一套被称为方法的访问过程绑定在一起。进程不被允许直接访问这些变量，相反，进程必须通过调用方法来访问它们。\nC++和Java这类编程语言是面向对象的，但这里的对象只是语言级别的对象，而不是运行时刻的对象。CORBA就是一个知名的基于运行时对象的系统。\n基于协作的中间件 分布式系统中还有一个泛型就是所谓的 基于协作的中间件(coordination-based middleware)。Linda就是一个典型的例子，它是一个用于通信和同步的系统。\n发布/订阅(publish/subscribe) 就是受Linda启发而出现的基于写作的模型。模型由大量通过某种方式互联的进程组成，一个进程可以是消息生产者，也有可能是消息消费者，还有可能两者都是。生产者将生产的消息放到一个特定的地方，完成消息的发布，而消费者通过订阅其感兴趣的消息，以获得响应的通知。\n","href":"/notebook/reading_notes/modern_operating_systems/multiple_processor_systems/","title":"多处理机系统"},{"content":" Taken from Design Concepts for Engineers (Fifth Edition) 2020.11.22\n 第1章 工程是什么 作为一名有抱负的工程师，你有很多需要学习的东西。你必须掌握工程的基础知识：数学、物理、化学和生物。你必须学习所学学科的专业课程，例如电路、力学、结构、材料和计算。同时，你必须学习如何通过终生学习来保持技术进步。制造出真正有效的产品的能力是工程师的标志。设计技能是工程师区别于其它基础科学专业人士的能力。\n设计过程是工程专业的本质，因此你必须熟练掌握它，才能够顺利走向成功。\n一个好的工程师通常非常熟悉其他学科和专业，工程师成功的关键就在于接受广泛的多学科的教育。\n在计算机行业中，专门处理大规模软件系统的人称为“系统工程师”。然而，传统的系统工程师则是指设计与实现复杂工程系统的人。系统工程师不仅能够完成项目的初始设计，还能够处理后勤、团队协作和项目监督中的各种问题。\n要成为一个成功的工程师，必须拥有技术、理论和实践能力，还必须善于组织、沟通和写作。工程基础中3个最主要的技能包括知识、经验和直觉。这些才能并不是工程师技能的全部，但是对于一名工程师来说，这是至关重要的。\n设计的效率取决于良好的沟通。\n第2章 设计是什么 分析、设计与复制 工程师要完成的是试图满足一组预先确定的需求，而不是去发现物理现象背后的秘密。分析与设计之间的不同也表现在以下的形式：如果答案是通过类似拼凑碎片组成拼图的方式获得的，那么这项活动很有可能是分析。另一方面，如果有多个解决方案，并且如果决定一个合适的路径需要创造、选择、测试、迭代、评估和重新测试，那么该活动就是设计。分析通常会作为设计过程中的一个步骤，但是设计还要包括创造、选择和测试等其它关键因素。\n复制指的是一个重建已完成设计的过程，它是工程的重要组成部分，是制造业的核心。\n工程师一定要具有远见和创造力。\n设计周期 生成想法 创意是将设计与分析和复制区分开来的突出特征之一。生成想法的最典型方法之一就是头脑风暴。\n头脑风暴的基本规则 当团队决定进行头脑风暴时，该团队应该提前就一系列规则达成一致，创造一个友好的、没有威胁的、鼓励开放思想的环境。每次头脑风暴都可以有其具体的规则，下面几条可以用作指南：\n 不要犹豫。任何好的想法随时可能出现。 没有界限。任何想法永远都不会太荒谬或者太出格。 不要批评。在进入最后的小组讨论阶段之前，不要随意批评一个想法。 不要退缩。在进入最后的小组讨论阶段之前，不要对想法打折扣。 没有限制。想法永远不会嫌多。 没有约束。参与者可以从任何专业领域生成想法。  不要害羞。团队的参与者在提出一个想法的过程中都不会感到局促不安。  优秀的工程师 vs 糟糕的工程师    优秀的工程师 糟糕的工程师     以开放的心态听取新的想法 很少听别人的想法   在选择设计方法之前，考虑各种解决方案 只追求最新的设计方法   仅在测试，修改和重新测试之后才考虑完结项目 一旦出现成功的迹象就认为大功告成；不进行彻底的测试就提交产品   绝对不能通过反复试验来得到一组设计参数 认为纯粹的试错就是工程设计   使用“我需要理解为什么”和“让我们考虑几种可能性”这样的短语 使用诸如“足够好”和“我不明白为什么它不起作用，反正就是这样了”的短语     我意识到我是一个糟糕的工程师，我要努力变得优秀。\n 第3章 项目管理与团队合作技能 一个好的工程师必须知道如何在团队中工作，保持项目进度，保持良好的文档，处理法律问题，并在一个完善的管理计划中工作。\n在团队中工作 有效的团队是指能够一起高效工作的团队。它可以最大潜力的发挥作用，并在其各个成员的特俗能力下共生兴旺。有效团队的一个关键特征就是队友之间良好的支持态度。以下准则是建立有效团队的一种可能方法：\n 明确领导角色。 达成一致的目标。 定义明确的角色。 定义工作流程。 培养良好的人际关系。  管理任务：保持项目正常进行 时间管理对于任何工程项目的成功都是至关重要的。以下是几个常用的时间管理工具：\n 清单。一个简单的清单能够起到对特定项目的相关工程任务的监督作用。 时间表。时间表是维持项目按计划进行的宝贵工具。 甘特图。当一个项目设计并行任务和许多工作人员时，甘特图比时间表更适合进行时间管理。 PERT图。一种用于优化和调度复杂的、相互关联的活动的方法。  文档：项目成功的关键 工程设计绝不是独立进行的，即使是最简单的项目也涉及设计师和最终用户。工程师彼此交流的一种方式仔细地保存记录。作为一名专业的工程师，你有责任收集并保存设计概念、草图、详细图纸、测试结果、重新设计、报告和原理图，无论这些内容是否能够应用于相关项目。文档跟踪是团队其他成员重视或验证你的工作时重要的信息传递工具。同时，文档也是与自己沟通的好方法，许多工程师由于很少保存记录而不能重现设计结果。专业工程师的标志之一就是保持有组织、整洁、最新的专业文档记录。文档编制不应该事后执行，如果一个项目被一个团队成员拖累，但由于文档是同步的，另一个团队成员可以毫不拖延地恢复该项目。\n第4章 工程工具 工程和估计总是齐头并进。当考虑一个新的设计策略时，首先通过粗略计算重要的指标和参数来衡量其可行性是一个非常好的方法。\n第5章 人机界面 人机界面定义了人与工程产品之间的交互方式。一个优秀的产品设计师不仅要关心产品的功能，还要关心产品是如何被使用的。\n经验丰富的工程师知道人机界面是产品开发初期应该解决的第一件事。\n第7章 学会表达、书写和演讲 工作中工程师需要具备的最重要的技能是什么？当被问及这个问题时，几乎每一位雇主都会强调的一点就是沟通技巧。\n准备正式演讲 一些初级工程师普遍犯的错误之一就是认为听众完全熟悉他们所讲的主题。即使听报告的人对所讲的内容一无所知，他仍然用同样简短的方式介绍背景。事实上，对于正式演讲报告来说，应该像准备书面文档的方式来组织。以下几个方面将帮助你准备一个有趣的报告，以满足正式演讲的需要：\n 首先要了解听报告的人员的知识背景，并根据具体情况制定适合的报告内容。例如，根据与会者对报告内容的了解程度和水平调整报告内容。 假设参会人员第一次接触报告的主题。 在参会人员入场前检查好视听设备。 穿着得体。 在最初的几分钟内引起报告的目的。 告诉参会人员为什么由你来作报告。 在报告开始时展示出报告的内容大纲，给出报告将要介绍的内容概述。 在演讲开始时想出一个简单的方式来打破报告开始时的尴尬状态。一个简单的趣事可以帮助观众轻松地与你建立融洽的关系。 演讲方式简单易懂。你可能很容易执着于介绍技术细节，而没有让听众真正获得报告的主要关键点。因此，可以将技术细节的内容留在报告后的讨论环节，通过这种方式，就能够保证听众从中得到他真正感兴趣的技术问题。 保持演讲的简短。根据你被分配的时间长短来准备讲稿，最好只用掉预计时间的50%~60%，你才会有可能准时结束。 自问自答。通过一些视觉辅助的方式作为报告的主要思路线索。切记不要对幻灯片内容逐字逐句地读！ 不要给观众展示公式。听众几乎没有时间去理解它们。偶尔必要时，可以加入少量公式。 以“谢谢，还有其它问题吗？”来结束报告。或者，你也可以通过一张结论性的幻灯片来结束报告内容，让观众知道你的报告已经结束。 在报告结束后的问答阶段，当有参会人员提出问题时，要对其问题进行重复，以使得所有参会人员都能够听到。此外，重申问题也有助于阐明内容，给自己争取到思考的时间。  ","href":"/notebook/reading_notes/design_concepts_for_engineers/","title":"Design concepts for engineers"},{"content":"","href":"/tags/%E5%B7%A5%E7%A8%8B%E6%80%9D%E7%BB%B4/","title":"工程思维"},{"content":"","href":"/categories/os/","title":"OS"},{"content":"虚拟化的主要思想是 虚拟机监视程序(Virtual Machine Monitor, VMM) 在同一物理机上创建出有多台(虚拟)机器的假象，VMM又称虚拟机管理程序(hypervisor)。我们可以将hypervisior分为两类：第一类hypervisor和第二类hypervisor。前者运行在裸机上，而后者主要依赖于底层操作系统提供的服务和抽象。无论采用何种方式，虚拟化技术都允许在单一计算机上运行多个虚拟机，并且这些虚拟机能运行不同的操作系统。\ntype 1 and type 2 hypervisors 有两种类型的hypervisor：\n Type 1 hypervisors (native/bare metal hypervisors). Type 2 hypervisors (hosted hypervisors).  Goldberg对两类虚拟化技术进行了区分。从技术的角度看，一类hypervisor更像是一个操作系统，因为它是唯一一个运行在最高特权模式下的程序。二类hypervisor是一个依赖操作系统分配和调度资源的程序，就像是一个普通的进程，尽管如此，它还是伪装成了一个具有CPU和各种设备的完整计算机。\n参考资料  ANDREW S. TANENBAUM, HERBERT BOS. Modern Operating Systems, 4th Edition. Pearson, 2015.  ","href":"/operating_systems/virtual_machines/","title":"虚拟机"},{"content":"虚拟化技术允许在一台物理机上创建多台虚拟机器。这些虚拟机器本身并不拥有物理资源，但它们却可以像普通的机器一样完成任务，每台虚拟器都不知道其它虚拟机的存在。Hypervisor是位于底层物理资源和虚拟环境之间的一个管理层，它创建彼此相互隔离的虚拟环境并为虚拟环境分配物理资源。Hypervisor又称虚拟机监视程序(Virtual Machine Monitor, VMM)。安装有hypervisor的物理机被称为宿主机(host machine)，而被hypervisor创建并管理的虚拟资源就是 虚拟机(virtual machine) 或 客户机(guest machine)。\n虚拟化的类型 根据虚拟机隔离情况的不同，有三种虚拟化方式：\n 全虚拟化(full virtualization)。 半虚拟化(paravirtualization)。 操作系统虚拟化(operating system virtualization)。  全虚拟化 在全虚拟化的虚拟平台中，Guest OS并不知道自己是一台虚拟机，它会认为自己是运行在物理硬件设备上的Host OS。因为hypervisor将操作系统管理的所有硬件设备逻辑抽象为虚拟设备，然后交给Guest OS使用。实际上是Hypervisor为Guest OS制造了一种假象，让Guest OS认为底层硬件平台归自己所有。\n全虚拟化为每台虚拟机提供了完全隔离的虚拟环境，大多数操作系统都直接在虚拟机中安装，而不需要任何修改。\n半虚拟化 半虚拟化需要在对Guest OS的内核代码进行一定的修改之后，才能使Guest OS在半虚拟化的hypervisor中运行。半虚拟化不需要虚拟机陷入特权指令，因此对系统的侵入更小。Guest OS直到hypervisor的存在，并通过hypercall直接与hypervisor交流。\n操作系统虚拟化 操作系统级别的虚拟化是操作系统的一个特性，内核知道多个用户空间实例的存在。我们称这种虚拟化为容器化(containerzation)，而将这些用户空间的实例称为容器(container)。程序可以在容器内运行，但它们只能访问容器内的内容和使用分配给容器的设备。容器认为自己拥有所有可用资源，实际上它们只能访问分配给容器的那些资源。\nHypervisors 虚拟机需要表现得与真实的机器一样，比如：我们既能像启动真机一样启动它，也能在它上面按照任意的操作系统……\nhypervisor的任务就是高效实现这种假象，它需要在以下三个方面表现良好：\n 安全性(safety)：hypervisor应该具有虚拟化资源的完全控制权。 保真性(fidelity)：虚拟机上的程序应该同其在物理机上运行的表现一致。 高效性(efficiency)：虚拟机中运行的绝大部分代码都不应该受到VMM的干涉。  毫无疑问，在**解释器(interpreter)**中逐条考查并准确执行每一条指令是安全的。有些指令可以直接执行，而其它不能完全执行的指令则需要由解释器进行模拟(使VMM上运行的操作系统认为自己正确的执行了指令)。\n在早期的x86体系结构上，虚拟化一直都是一个问题。每个包含内核态和用户态的CPU都有一个特殊的指令集，其中的指令在内核态与用户态下的执行行为不同，这些指令包括I/O操作和修改MMU设置的指令，称为敏感指令(sensitive instruction)。还有一个指令集，其中的指令在用户态执行时会导致陷入，称为特权指令(privileged instruction)。Popek和Goldberg指出机器可虚拟化的一个必要条件是敏感指令为特权指令的子集，即：若要在用户态做不应该在用户态做的事，硬件必须陷入。但Intel 386不具备这一特性：很多Intel 386敏感指令在用户态执行时要么被忽略，要么具有不同的行为。因此，那时的386及其后继不能被虚拟化，也不能直接支持hypervisor。除了敏感指令在用户态不能陷入以外，386还存在可以在用户态读取敏感状态而不造成陷入的指令。例如：在2005年前的x86处理器上，程序可以通过读取代码段选择符判断自身时运行在内核态还是用户态，若虚拟机中的操作系统这么做并发现自己运行在用户态，就会做出错误的决策。\n从2005年起，Intel和AMD开始在CPU中引入虚拟化支持，使这个问题最终得到解决。这项技术在Intel CPU中被称作VT(Virtualization Technology)，而在AMD CPU中称作SVN(Secure Virtual Machine)。它们的基本思想使创建可运行虚拟机的容器，客户操作系统在容器中启动并持续运行，直到发生异常并陷入hypervisor。这就使得在x86平台实现经典的 陷入模拟(trap-and-emulate) 成为可能。\nHypervisor创建并管理虚拟环境，分为两种：\n Type 1 hypervisors (native/type metal hypervisors). Type 2 hypervisors (hosted hypervisors).  虚拟机一章详细介绍了两类hypervisor。\n参考资料  ANDREW S. TANENBAUM, HERBERT BOS. Modern Operating Systems, 4th Edition. Pearson, 2015.  ","href":"/operating_systems/virtualization/","title":"虚拟化"},{"content":"操作系统中最核心的概念是进程：它是对正在运行的程序的一个抽象。进程通常需要做很多事情，比如游戏运行起来就是一个进程，游戏内部需要渲染图形、响应用户操作、连接网络等，为了保障良好的用户体验，这些功能不能相互阻塞，因此需要多线程；进程需要空间存储指令和数据，因此用到了内存；有时候，进程需要将一部分数据持久化保存，这就用到了文件系统……\n进程 进程（process） 是对正在运行的程序的一个抽象，代表一个正在执行的程序的实例，包括程序计数器、寄存器和变量的当前值。也可以这么说，进程是程序启动后在内存中创建的一个执行副本。\n进程的创建 通常有 4 种事件会导致进程的创建：\n 系统初始化。 正在运行的程序执行了创建进程的系统调用。 用户请求创建一个新进程。 一个批处理作业的初始化。  从技术上看，在所有这些情形中，新进程都是由于一个已经存在的进程执行了一个用于创建进程的系统调用而出现的。\n在 UNIX 系统中，创建新进程使用的是系统调用 fork，而 Windows 中则是CreateProcess 函数。在 UNIX 和 Windows 中，进程创建之后，父进程和子进程有各自不同的地址空间。如果其中某个进程修改了其地址空间内的一个字，其它进程是察觉不到的。在 UNIX 中，子进程的初始地址空间是父进程地址空间的一个副本，但这里其实有涉及到两个不同的地址空间。不可写的内存是共享的。一些 UNIX 系统在父子进程之间共享程序正文，因为程序正文是不可修改的。子进程也可能会共享父进程所有的内存，但这种情况下采用的是 copy-on-write 这种共享方式，一旦有进程想要修改部分内存，就必须先复制这块地址空间，以确保修改发生在私有区域。可写的内存是不可共享的。在 Windows 中，父进程的地址空间和子进程的地址空间从一开始就是不同的。\n进程的终止 进程在创建之后，就开始运行，完成其工作。进程会在某些条件下终止：\n 正常退出（自愿的）。多数进程是由于完成了它们工作而终止，这个时候它们会执行一个系统调用而退出。在 UNIX 中该调用是 exit，Windows 中则是ExitProcess。 出错退出（自愿的）。例如需要的文件不存在。 严重错误（非自愿）。例如执行了非法指令、引用了不可访问的内存等。 被其它进程杀死（非自愿）。某个进程执行一个系统调用通知草走系统杀死某个其它进程。在 UNIX 中，这个调用是 kill，Windows 中，则是 TerminateProcess 函数。  进程的状态 每个进程都是一个独立的实体，有自己的程序计数器和内部状态。进程之间经常相互作用，从而导致进程状态的迁移。\n进程的实现 为了实现进程，操作系统维护着一张 进程表（process table），又称 进程控制块（process control block）。每个进程占用一个进程表项。进程表项包含了进程状态的重要信息，包括程序计数器、堆栈指针、内存分配情况、所打开文件的状态、账号和调度信息，以及其它在进程由运行态转为就绪或阻塞态时必须保存的信息，从而保证该进程随后能再次启动，就像从未被中断过一样。一个进程在执行过程中可能被中断数千次，但每当被中断的进程被重新被执行时，操作系统都会将进程状态恢复到中断发生前进程的状态。\n线程 线程（thread） 具有很多进程的特质，又被称为 轻量级进程（lightweight process）。进程把资源集中在一起（资源管理的单位是进程），而线程则是在 CPU 上被调度执行的实体。一个进程内的多个线程共享同一个地址空间和其它资源。每个进程都有自己独立的地址空间，而一个进程内的多个线程都有完全一样的地址空间，这意味着它们也共享同样的全局变量。由于各个线程都可以访问进程地址空间中的每一个内存地址，所以一个线程可以读、写、甚至清除另一个线程的堆栈，线程之间是没有保护的。\n每个进程的中内容有：地址空间、全局变量、打开的文件、子进程、时钟信息、信号与信号处理程序、账户信息。而每个线程中的内容有：程序计数器、寄存器、堆栈、状态。每个线程都有自己的堆栈，堆栈中的栈帧供各个被调用但是还没有从中返回的过程使用，栈帧保存了相应过程的局部变量以及过程调用完之后使用的返回地址。\n进程间通信 有时候，进程之间需要相互通信。进程间通信需要解决的三个问题：\n 进程如何把信息传递给其它进程。 如何确保两个或更多的进程在关键活动中不会出现交叉。 当进程间出现依赖时，如何保证正确的顺序。  临界区 竞争条件（race condition） 说的是两个或多个进程对共享资源的读写存在竞争，当所有的进程执行完成后，共享资源的值是不可预测的，因为它取决于进程执行的精确顺序。\n避免竞争条件的关键在于阻止多个进程同时读写共享资源。换言之，我们需要的是 互斥（mutual exclusion），即确保一个进程在使用某个共享资源时，其它的进程不能做同样的操作。程序中访问共享资源的那一部分代码称为 临界区域（critical region） 或 临界区（critical section），临界区内共享资源的访问操作必须被同步，否则数据的一致性就得不到保证。\n如果我们精心地安排进程的执行顺序，防止多个进程同时读写临界区，就能避免竞争条件。一个好的解决方案需要满足以下 4 个条件：\n 不会出现两个进程同时在临界区内的情况。 不对 CPU 的速度和数量做任何假设。 运行在临界区外的进程不应该阻塞任何进程。 进程在进入临界区前不会无限等待。  以两个进程为例，我们期望它们的行为是这样的：\n基于忙等待的互斥 接下来会介绍几种基于忙等待实现的互斥方案。在这些方案中，在一个进程在临界区中更新共享内存期间，其它进程不会进入临界区，也不会带来麻烦。\n屏蔽中断 在单处理器系统中，最简单的方法是使每个进程在进入临界区后就立即屏蔽所有中断，并在将要离开临界区之前再次打开中断。因为CPU只有在发生时钟中断或其它中断时才会进行进程切换，屏蔽中断后CPU就不会再进行进程切换了。于是，一旦某个进程屏蔽中断后，它就可以检查和修改共享内存，而不必担心其它进程介入。\n这并不是一个好方案，如果某个进程屏蔽中断后不再打开中断，那么整个系统可能因此而终止。对于多处理器系统，屏蔽中断仅仅对于执行 disable 指令的那个 CPU 有效，其它 CPU 仍继续运行，并可以访问共享内存。\n另一方面，对内核来说，当它在更新变量或列表的几条指令期间将中断屏蔽是很方便的。屏蔽中断对操作系统本身而言是一项很有用的技术，但对于用户进程则不是一种合适的通用互斥机制。\n锁变量 假设有一个共享（锁）变量，其初始值为0。当一个进程想进入其临界区时，它首先查看这把锁，如果锁的值为0，就将其设置为1并进入临界区。若锁的值为1，则一直等待直到锁的值变为0。于是，0表示临界区内无进程，而1表示临界区内有进程。\n但是，这种想法也会出现竞争条件。假设一个进程读出锁变量的值并发现它为0，而恰好在它将锁的值设置为1之前，另一个进程被调度运行，将锁变量的值设置为1。当第一个进程再次运行时，它同样也将锁的值设置为1，则此时有两个进程同时在临界区内。另一方面，我们获取可以在改变锁变量的值之前再次检查锁变量的值，但如果第二个进程恰好在第一个进程检查完之后修改了锁变量的值，同样会出现竞争条件。\n严格轮转法 以下展示了一种用于两个进程互斥的算法。 进程0：\nwhile （TRUE） { while （turn != 0）; // 循环等待  critical_region（）; turn = 1; noncritical_region（）; } 进程1：\nwhile （TRUE） { while （turn != 1）; // 循环等待  critical_region（）; turn = 0; noncritical_region（）; } 线程0和线程1通过检查共享变量turn的取值来决定是否进入临界区。变量turn的初始值为0，表示进程0可以进入临界区。开始时，进程0检查turn，发现为0，于是进入临界区。进程1也发现turn为0，所以在一个循环中不停的测试turn，直到其值变为1。连续测试一个变量直到某个值出现为止，称为忙等待（busy waiting）。由于这种方式会浪费CPU的时间，所以通常应该避免。只有在有理由认为等待时间非常短的情形下，才使用忙等待。用于忙等待的锁，称为自旋锁（spin lock）。\n进程0离开临界区时，将turn设置为1，以便允许进程1进入临界区。假设进程1很快便离开了临界区，它将turn设置为0，这个时候两个进程都处于临界区外。现在进程0再次进入外层循环，它进入临界区又退出，turn的值变为1。此时，两个线程都在临界区外执行。突然，进程0再次完成非临界区的操作而回到循环的开始位置，但这个时候它并不能进入临界区，因为turn当前的值为1，因为此时进程1还在忙于非临界区的操作，进程0只有自旋等待，直到进程1把turn变为0为止。这说明：在一个进程比另一个进程慢了很多的情况下，轮流进入临界区并不是一个好办法。\n该方案要求两个进程严格地轮流进入临界区，避免了所有的竞争条件。由于方案中出现了临界区外的进程阻塞其它进程的情况，违反了条件3，所以不能作为一个很好的备选方案。\nPeterson算法 Peterson于1981年提出了一种新的互斥算法，克服了严格轮转法的缺点：\n#define FALSE 0 #define TRUE 1 #define N 2 // 进程数量  int turn; // 谁可以进入临界区 int interested[N]; void enter_region（int process） { // 进程是0或1  int other = 1 - process; // 另一个进程号  interested[process] = TRUE; // 表示准备进入临界区  turn = process; // 表示谁正在等待进入临界区  while （turn == process \u0026amp;\u0026amp; interested[other] == TRUE）; // 循环等待 } void leave_region（int process） { interested[process] = FALSE; // 表示离开临界区 } 在进入其临界区之前，各个进程使用自己进程号0或1作为enter_region的参数。该调用在需要时将使进程等待，直到能安全地进入临界区。在完成对共享变量的操作之后，进程将调用leave_region离开临界区。若其它进程希望进入临界区，现在就能进入。\n一开始，没有任何进程处于临界区中，现在进程0调用enter_region，它通过设置其数组元素和将turn置为0来标识它希望进入临界区。由于进程1并在临界区内，所以enter_region很快返回，如果进程1现在调用enter_region，进程1将在此处循环等待直到interested[0]变成FASLE，而这只有在进程0调用leave_region退出临界区时才会发生。\n现在考虑两个进程几乎同时调用enter_region的情况，它们将自己的进程号存入turn，但时只有后设置的进程号才会生效，前一个因为被重写而丢失。假设进程1是后存入的，则turn为1。当两个进程都运行到while语句时，进程0将循环0次并进入临界区，而进程1由于while条件不满足而不能进入临界区，它需要一直循环直到进程0退出临界区为止。turn变量可以理解为：现在轮到谁，即谁正在等待进入临界区。\n基于硬件的解决方案 在某些计算机中，特别是那些设计为多处理器的计算机，都有下面一条指令：\nTSL RX, LOCK 称为测试并加锁（test and set lock）。它将一个内存字lock读取到寄存器RX中，然后在该内存地址上存一个非零值。读字和写字操作保证是不可分割的，即该指令结束之前其他处理器均不允许访问该内存字。执行TSL指令的CPU将 锁住内存总线，以禁止其它CPU在该指令结束之前访问内存。\n为了使用TSL指令，需要使用一个共享变量lock来协调对共享内存的访问。当lock为0时，任何进程都可以使用TSL指令将其设置为1并读写共享内存。当操作结束时，进程调用一条普通的MOVE指令将lock的值重新设置为0。\nenter_region: TSL REGISTER,LOCK | 将LOCK中的值复制到REGISTER并将LOCK的值置为1 CMP REGISTER,#0 | REGISTER中的值为0吗？  JNE enter_region | 若非零，说明锁不可用，循环 RET | 返回；进入临界区 leave_region: MOVE LOCK,#0 | 将LOCK的值置为0  RET | 返回 enter_region的第一条指令将lock的旧值复制到寄存器并将lock的新值设置为1，随后将lock的旧值与0相比较，若非0，则说明锁已被使用，则程序回到开始并再次测试。重复这个过程直到lock的值为0，于是过程返回，加锁成功。要清除锁非常简单，程序只需要将0存入lock即可。\n一个可以替代TSL的指令是XCHG，它原子的交换两个位置的内容。以下解法和TSL解法本质上是一样的：\nenter_region: MOVE REGISTER,#1 | 将1存入REGISTER  XCHG REGISTER,LOCK | 交换REGISTER与LOCK的值 CMP REGISTER,#0 | REGISTER中的值为0吗？  JNE enter_region | 若非零，说明锁不可用，循环 RET | 返回；进入临界区 leave_region: MOVE LOCK,#0 | 将LOCK的值置为0  RET | 返回 进程在进入临界区之前先调用enter_region，这将导致忙等待，直到锁空闲为止，随后进程获得锁并返回。进程从临界区返回时调用leave_region，将lock设置为0。与其它基于临界区问题的解法一样，进程必须在正确的时间调用enter_region和leave_region，解法才能奏效。也就是说：临界区能否工作取决于进程是否遵守协定。\n睡眠与唤醒 Peterson算法和基于TSL或XCHG的解决方案都是正确的，但它们都有忙等待的缺点。这些解法的本质是：当一个进程想要进入临界区时，先检查是否允许进入，若不允许，则原地自旋等待，直到允许为止。这种方法不仅浪费CPU时间，而且还可能引发预想不到的问题，例如优先级反转问题。\n现在来考察另一组进程间通信原语，它们在无法进入临界区时将阻塞，而不是忙等待。最简单的就是sleep和wakeup。sleep是一个将引起调用进程阻塞的系统调用，即进程被挂起，直到另一个进程将其唤醒。wakeup只有一个参数，即要被唤醒的进程。\n生产者消费者问题 生产者消费者问题也称叫有限缓冲区问题。两个进程共享一个公共的固定大小的缓冲区，其中一个是生产者，它将消息放入缓冲区；另一个是消费者，它从缓冲区内取出消息。当缓冲区已满，生产者将被阻塞，直到消费者从缓存区中取走数据并将其唤醒。同样的，当消费者试图从缓冲区中取消息而发现缓冲区为空时，消费者就睡眠，直到生产者向缓冲区中放入消息并将其唤醒。\n我们需要一个变量count来追踪缓冲区中消息的数量。如果缓冲区的容量为N，则生产者将首先检查count是否达到了N，若是，则生产者睡眠；否则生产者向缓冲区中放入一条消息并将count值加1。消费者首先测试count是否为0，若是，则睡眠；否则取走一条消息并将count减1。每个进程同时也会检测是否需要唤醒另一个进程。\n# define N 100; // 缓冲区容量 int count = 0; // 缓冲区中消息的数量  void producer（void） { while （TRUE） { int item = product_item（）; // 生产一条消息  if （count == N） sleep（）; // 缓冲区满了，睡眠  put（item）; // 放入一条消息到缓冲区  count++; // 缓冲区中消息数量加1  if （count == 1） wakeup（consumer）; // 若放入消息之前缓冲区是空的，则唤醒消费者  } } void consumer（void） { while （TRUE） { if （count == 0） sleep（）; // 缓冲区是空的，睡眠  int item = take（）; // 取走一条消息  count--; // 缓冲区中消息数量减1  if （count == N - 1） wakeup（producer）; // 如果取走消息前缓冲区是满的，则唤醒生产者  consume（item）; // 消费  } } 这里可能出现竞争条件，原因是对count的访问未加限制。有可能出现以下情况：缓冲区为空，消费者读取到的count值为0。此时调度程序决定挂起消费者并启动生产者，生产者放入一个数据项到缓冲区，count变为1。生产者推断认为由于刚才count为0，所以消费者一定在睡眠，所以调用wakeup唤醒消费者。但是，消费者此时并没有睡眠，所以wakeup信号丢失。当消费者下次运行时，它继续测试之前读到的count值，依旧为挂起之前的值0，于是睡眠。这样下去，生产者迟早会填满整个缓冲区，然后睡眠，最终两个进程都将永远睡眠下去。\n问题的实质在于发给一个未睡眠进程的wakeup信号丢失了。一种快速的弥补方法就是修改规则，加上一个唤醒等待位。当一个wakeup信号发给一个清醒的进程时，将该位置1，随后当进程尝试睡眠时，若唤醒等待位为1，则将其清零，就不睡眠而是保持清醒。\n信号量 信号量（semaphore） 是Dijkstra在1965年提出的一种方法，它使用一个整型变量记录唤醒次数，供以后使用。一个信号量的取值可以是0（表示当前没有睡眠的进程）或者为正值（表示当前有一个或多个进程处于睡眠状态，等待被唤醒）。以下的检查信号量的值、修改信号量的值和睡眠操作均需要保证是原子操作。\n信号量支持两种操作：down和up（分别对应于广义的sleep和wakeup）。对一个信号量执行down操作时，会先检查其值是否大于0，若大于0，则将其减一（用掉一个唤醒信号，即睡眠进程的数量减一）并继续；若信号量的值为0，则进程将睡眠，而且此时down操作未结束。up操作对信号量的值加1，如果有一个或者多个进程在该信号量上睡眠（即先前的down操作还未完成），系统会选取一个进程并允许它完成之前的down1=操作。于是，对一个有进程在其上睡眠的信号量来说，执行一次up操作之后，信号量的值依旧为0，只是在其上睡眠的进程少了一个。\n用信号量解决生产者-消费者问题 为确保信号量能正确工作，需要采用一种原子的方式来实现它。通常将up和down作为系统调用来实现，而且操作系统只需要在执行以下操作时暂时屏蔽中断：测试信号量、更新信号量以及在需要时使某个进程睡眠。\n生产者-消费者问题的解决方案使用了三个信号量：full记录满缓冲槽数量，empty记录空缓冲槽数量，mutex控制对临界区的访问，确保生产者和消费者不会同时进入缓冲区。mutex的初值为1，保证同时只有一个进程可以进入临界区，称作二元信号量（binary semaphore）。\n#define N 100 typedef int semaphore semaphore full = 0; // 慢缓冲槽数量 semaphore empty = N; // 空缓冲槽数量 semaphore mutex = 1; // 控制对临界区的访问  void producer（void） { while （TRUE） { int item = produce_item（）; // 生产一条消息  down（\u0026amp;empty）; // 空槽数量减1  down（\u0026amp;mutex）; // 进入临界区  put（item）; // 将消息放到缓冲区  up（\u0026amp;mutex）; // 离开临界区  up（\u0026amp;full）; // 满槽数量加1  } } void consumer（void） { while （TRUE） { down（\u0026amp;full）; // 满槽数量减1  down（\u0026amp;mutex）; // 进入临界区  int item = take（）; // 从缓冲区取走一条数据  up（\u0026amp;mutex）; // 离开临界区  up（\u0026amp;empty）; // 空槽数量加1  consume（item）; // 消费  } } 以上展示了信号量的两种用法：mutex用于互斥，保证任一时刻只有一个进程读写缓冲区中的变量，而empty和full用于同步，保证某些事件的顺序发生或不发生。\n互斥量 如果不需要信号量的计数能力，有时候可以使用信号量的一个简化版本，称为互斥量（mutex）。互斥量仅适用于管理一些共享资源或一小段代码。它是一个只有可能处于两种状态中的一种的变量：解锁和加锁。\n当一个进程需要访问临界区时，它调用mutex_lock。若mutex为0，表示临界区内无进程，则进入临界区；若mutex为1，表示当前临界区内有进程，调用线程将被阻塞，直到临界区中的进程完成并调用mutex_unlock释放锁。如果多个进程在该互斥量上阻塞，将随机选择一个进程并允许它获得锁。\nmutex_lock: TSL REGISTER,MUTEX | 先将mutex的值复制到GEGISTER，然后将mutex置为1 CMP REGISTER,#0 | 检查REGISTER中的值是否为0  JZE ok | 若为0，表示未上锁，成功获得锁，返回 CALL thread_yield | 若非0，调度另一个线程运行 JMP mutex_lock | 稍后重试 ok: RET | 返回，进入临界区 mutex_unlock: MOVE REGISTER,#0 | 将mutex置为0  RET | 释放锁，返回 这段代码与之前采用基于TSL的硬件解决方案中的enter_region的代码很相似，但有一个关键区别：当enter_region进入临界区失败时，它始终自旋等待；而mutex_lock在获取锁失败时，它调用thread_yield让出CPU的使用权，这样就没有忙等待。当该进程再次获得CPU时，它将再次尝试获取锁。\n管程 管程（monitor） 是一种高级同步原语。一个管程是一个由过程、变量、数据结构等组成的一个集合，他们组成一个特殊的模块或软件包。进程可以在任何需要的时候调用管程中的过程，但他们不能在管程之外声明的过程中直接访问管程内的数据结构。\n管程有一个很重要的特性，即任一时刻管程中只能有一个活跃进程，这一特性使管程能够有效地完成互斥。管程是编程语言的组成部分，进入管程时的互斥由编译器完成，C语言并不支持管程，而Java支持管程。\n消息传递 消息传递（message passing） 是一种进程间通信的方法，它使用了两条原语：send和receive。send给接收方发送一条消息，而receive从发送方接收一条消息。如果没有消息可用，接收方可能会阻塞直到消息到达，也可能带一个错误码立即返回。\n为了防止消息丢失，发送方和接收方可以达成以下协议：一旦收到消息，接收方马上会送一条确认消息。如果发送方在一定时间内未收到确认，则重传该消息。\n屏障 在有些应用中划分了若干阶段，并且规定，除非所有的进程都准备好进入下一阶段，否则任何进程都不能进入下一阶段。可以通过在每个阶段的结尾放置 屏障（barrier） 来实现这种行为。当一个进程到达屏障，它会被屏障阻拦，直到所有进程都到达该屏障才一并放行。屏障可用于一组线程的同步。\n避免锁：读-复制-更新 最快的锁就是根本没有锁。在某些情况下，我们可以允许写操作更新数据结构，即便还有进程正在使用它。关键在于确保每个读操作要么读取数据的旧版本，要么读取数据的新版本，绝不能是新旧版本数据的组合。\n进程调度 当两个进程同时竞争一个 CPU 时，必须选择出一个进程让其获得 CPU。在操作系统中，完成选择工作的组件被称为 调度程序（scheduler），调度程序可能根据实际情况采用不同的调度算法。\n有些进程会将绝大多数时间花在计算上，而有些进程会被大多数时间花在 I/O 上。前者称为计算密集型（computed-bound），后者称为I/O 密集型（I/O-bound）：\n调度的一个关键问题就是合适进行调度，有多种情形需要调度。例如：\n 在创建一个新进程之后，是运行父进程还是子进程。 当一个进程退出时，必须从就绪进程中选取一个运行。如果没有就绪进程，通常会运行一个系统提供的空闲进程。 当一个进程由于 I/O 或其它原因阻塞时，需要选择另一个进程运行。 当一个 I/O 中断发生时，必须做出调度决策。  调度算法 根据时钟中断时做出的调度决策，可以将调度算法分为两类：非抢占式调度算法挑选一个进程，然后让该进程运行至阻塞，或者直到该进程自动释放CPU，该进程不会被强迫挂起。抢占式调度算法挑选一个进程，让该进程运行一段时间，如果时间到了且该进程仍在运行，则将它挂起并运行其它就绪进程。抢占式调度算法依赖时钟中断，以便调度程序能够获得CPU的控制权。\n调度算法的目标 对于不同的操作系统环境，采取的调度算法也是不同的，常见的环境有：批处理系统、交互式系统、实时系统。为了设计调度算法，我们有必要考虑什么是一个好的调度算法。某些目标取决于操作环境，而某些目标又是通用的。\n在所有的情形中，公平是很重要的。相似的进程应该得到相近的对待，给其中一个进程更多的 CPU 份额是不公平的。操作系统可能需要强制施行某些策略以保证公平性。另一个共同目标是保持系统的所有部分尽可能地忙碌，完成尽可能多的工作。\n对于批处理系统，通常使用以下 3 个指标来衡量系统的工作状态：\n 吞吐量。吞吐量指系统每小时完成的作业数量。 周转时间。周转时间指从一个批处理作业提交到完成的平均时间。 CPU 利用率。CPU 利用率用来衡量 CPU 的繁忙程度。  对于交互式系统，最重要的就是最小响应时间，即从发出命令到得到响应之间的时间。\n实时系统有着与交互式系统不同的特性，所以有着不同的调度目标。一般来说，实时系统或多或少都必须满足截止时间。作为一种以时间为主导的系统，实时系统可以分为 硬实时（hard real time） 和 软实时（soft real time）。前者表示任务必须满足绝对的截止时间，而后者能够容忍任务偶尔超过截止时间。\n常见的调度算法 批处理系统中常用的调度算法有：先来先服务（first-come first-served）、最短作业优先（shortest job first）和最短剩余时间优先（shortest remaining time next）。\n轮转调度（round robin）：每个进程被分配一个时间段，称为时间片（quantum），即允许该进程在该时间片内运行。如果在时间片结束时进程还在运行，将剥夺CPU并分配给另外一个进程。如果该进程在时间片结束前阻塞或结束，CPU也会立即切换。时间片轮转调度中的时间片大小很重要。从一个进程切换到另一个进程是需要一定时间的——保存和载入寄存器值及内存映像、更新各种表格和列表、清除和重新调入缓存等。进程切换（process switch） 有时候也称 上下文切换（context switch）。时间片太短容易导致过多的进程切换，降低CPU效率；而时间片太长可能导致短交互请求的响应时间变长。将时间片设为20~50ms通常是一个合理的折中选择。轮转调度一般用于交互式系统。\n优先级调度：优先级调度的基本思想是：给每个进程指定一个优先级，优先级越高的进程越先运行。而轮转调度中的每个进程的优先级是一样的。\nOSTEP 的读书笔记中有更多的调度算法，见 进程调度。\n参考资料  ANDREW S. TANENBAUM, HERBERT BOS. Modern Operating Systems, 4th Edition. Pearson, 2015. Critical section.  ","href":"/operating_systems/processes_and_threads/","title":"进程与线程"},{"content":"在多道程序设计环境中，多个线程可能会争夺有限的资源。线程先请求资源，若请求不能被立即满足，线程就会进入等待状态。有时候，两个线程各自持有不同的资源并等待对方所持有的资源，这种情况可能导致 死锁(deadlock)。死锁的定义为：every process in a set of processes is waiting for an event that can be caused only by another process in the set。虽然定义中使用了进程，但这也适用于线程。\n资源 大部分死锁是由线程无法获得互斥资源引起的，这些资源可以是各种硬件设备(如打印机)、也可以是一段信息(如数据库内的记录)。资源种类繁多，一类资源可以具有若干实例，任何一个实例都可以满足对该类资源的请求。简而言之，资源(resource) 就是任何能够被获取、使用并最终释放的东西。线程在使用某个资源前必须先请求该资源，并且必须在使用完之后释放它。很明显，线程所请求的资源数量不得超过系统内资源的总量。\n资源可以分为两种：可抢占的(preemptable) 和 不可抢占的(nonpreepmptable)。通常情况下，死锁和不可抢占资源有关。可抢占资源中潜在的死锁可以通过资源的重分配来化解。\n一般情况下，和使用资源有关的事件如下：\n 请求。线程请求资源，若请求不能被立即满足，线程必须等待。 使用。线程在资源上执行某些操作。 释放。线程释放资源。  资源的请求和释放可能是通过系统调用来完成的。例如，某些系统可能提供了诸如request()/release()、open()/close()或wait()/signal()一类的系统调用。对于被内核管理的资源，操作系统会维护一张系统表，表中记录了每个资源是空闲的还是已分配的(还会记录资源被分配给了哪个线程)。如果线程请求的资源已将被分配给其它线程，就将它加入到对应资源的等待队列中。\n死锁的特征 死锁的必要条件 当以下四个条件同时满足时，就会出现死锁：\n 互斥(Mutual exclusion)。资源要么因其已被分配给某个线程而不可用，要么就是可用的。 占有并等待(Hold and wait)。线程持有至少一个资源并等待其它线程正持有的资源。 非抢占(No preemption)。在持有资源的线程完成前，该资源不可被其它线程抢占。 循环等待(Circular wait)。一组线程形成等待环路，环路中的每一个线程都在等待下一个线程持有的资源。  虽然死锁只有在以上四个条件同时满足时才会出现，但这四个条件并不是完全独立的。例如，循环等待条件暗示了占有并等待条件。\n资源分配图 系统资源分配图(system resource-allocation graph, RAG) 是一个有向图，它可以精确的描述死锁。它由一组顶点V和一组边E构成，其中，V又由两部分组成：系统中的所有活动线程T和系统中所有的资源类型R(注意是资源类型而不是资源数量)；E有两种类型：request edge和assignment edge，request edge $T_i \\to R_i$ 表示线程 $T_i$ 请求并等待资源 $R_j$ 的实例，而assignment edge $R_j \\to T_i$ 表示线程 $T_i$ 已经持有一个资源 $R_j$ 的实例。为了便于识别，我们用圆形表示线程，用矩形表示资源，在矩形内部用点表示资源的实例。\n上图展示了这样一个资源分配情况。 集合T、R、E：\n $$T = {T_1, T_2, T_3}$$ $$R = {R_1, R_2, R_3, R_4}$$ $$E = {T_1 \\to R_1, T_2 \\to R_2, R_1 \\to T_2, R_2 \\to T_1, R_2 \\to T2, R_3 \\to T_3}$$  资源实例：\n 1个 $R_1$ 实例。 2个 $R_2$ 实例。 1个 $R_3$ 实例。 3个 $R_4$ 实例。  线程状态：\n 线程 $T_1$ 持有1个 $R_2$ 实例并等待一个 $R_1$ 实例。 线程 $T_2$ 持有1个 $R_1$ 实例和1个 $R_2$ 实例并等待1个 $R_3$ 实例。 线程 $T_3$ 持有1个 $R_3$ 实例。  如果资源分配图中没有环，就说明线程之间没有死锁，否则说明可能存在死锁。现在来看有环的情况：\n 如果每种资源都只有一个实例，那么存在死锁。 如果环中只包含了资源类型的一个子集，若子集内的每种资源只有一个实例，那么存在死锁。 如果每种资源有多个实例，那么可能存在死锁。  下面是一个具有两个环( $T_1 \\to R_1 \\to T_2 \\to R_3 \\to T_3 \\to R_2 \\to T_1 $、 $T_2 \\to R_3 \\to T_3 \\to R_2 \\to T_2$ )的资源分配图。线程 $T_1$ 、 $T_2$ 和 $T_3$ 死锁了，线程 $T_2$ 在等 $T_3$ 释放 $R_3$ ，而 $T_3$ 在等 $T_1$ 或 $T_2$ 释放 $R_2$ ，$T1$ 也在等 $T_2$ 释放$R_1$ 。\n在看下面这张资源分配图，虽然其中有一个环( $T_1 \\to R_1 \\to T_2 \\to R_2 \\to T_1$ )，但不存在死锁。因为线程 $T_4$ 可以释放一个资源 $R_2$ 的实例， $T_3$ 获得这个实例之后图中就不存在环了。\n死锁的处理 一般而言，我们有四种处理死锁的策略：\n 忽略死锁。 破坏死锁的四个必要条件之一，预防死锁的产生。 谨慎分配资源，避免死锁的出现。 检测死锁的发生并设法恢复。  鸵鸟算法 鸵鸟在遇到危险时，会将头埋在沙子里，假装看不见。这是最简单的解决方案，即忽略死锁。\n死锁预防 死锁若发生，其四个必要条件必须同时满足。因此，只要能破坏死锁发生的四个必要条件之一，我们就能预防死锁。\n破坏互斥条件。因为死锁是在访问共享资源时发生的，若资源不是共享的，那就不可能发生死锁。如果访问共享资源时不需要互斥(例如只读文件)，也不会发生死锁。\n破坏占有并等待条件，有两种方法可以破坏它。一种方法是在线程运行之前就为它分配好需要的所有资源，这不太实用，因为大多数程序都是动态申请资源的。另一种方法是只允许线程不持有资源的情况下请求资源。以上两种方法都有缺陷：资源的利用率低，还可能产生饥饿现象。\n破坏非抢占条件，我们可以这么干：如果线程在持有一些资源的情况下请求其它不能立即得到的资源，那么让该线程当前持有的资源都可以被抢占。换句话说，线程当前持有的资源是隐式释放了的。原线程可以在被抢占的资源归还后重启，就像这些资源刚被分配给它一样。\n前面三种解决方案在大多数情形下都不太实用，有多种方法可以破坏循环等待条件，即避免环路的出现。一种方法时保证每个线程在任何时刻只能占用一个资源，若要请求另一个资源，就必须释放当前持有的资源。另一种方法就是对所有资源进行统一编号，线程可以在任何时刻请求资源，但所有的请求必须按照资源编号的顺序提出，这样不会出现环路。在任何时候，总有一个线程持有已分配资源中编号最大的那个资源(如果有资源被分配的话)，这个线程不能请求其它已分配的各种资源(编号肯定都小于它持有的那个最大编号)，它要么执行完毕，要么去请求编号更高的资源(总是可用的)。最终，它会执行完并释放占有的资源，这是其它的线程也可以以同样的方式执行完。\n死锁避免 死锁避免算法通过动态检查资源的分配状态来确保不会发生循环等待。资源分配状态包括资源的可用数量、已分配数量及线程请求的数量。\n安全状态 如果系统按照某种顺序为每线程分配资源，可以使得每个线程都能拿到资源并执行完成，那么这样的资源分配顺序就是安全序列(safe sequence)。只要系统中存在至少一个安全序列，系统就处于安全状态(safe state)。反之，系统处于不安全状态，可能出现死锁。若线程序列 $\u0026lt;T_1, T_2, \u0026hellip;, T_n\u0026gt;$ 是一个安全序列，那么对于线程 $T_i$ 而言， $T_i$ 请求的资源数量不大于当前可用资源加上所有 $T_j(j \u0026lt; i)$ 线程所占用的资源之和(当 $T_j$ 执行完成后，会释放其占用的资源)。\n资源分配图算法 在每种类型的资源都只有一个实例的前提下，我们可以利用RAG来避免死锁的出现。具体做法是：引入一种新的边——claim edge。claim edge $T_i \\to R_j$ 表示线程 $T_i$ 在将来可能会请求资源 $R_j$ (claim edge在RAG中用虚线表示)。当线程 $T_i$ 请求资源 $R_j$ 时，就将claim edge $T_i \\to R_j$ 转化为request edge $T_i \\to R_j$ ；当资源 $R_j$ 被线程 $T_i$ 释放时，就将assignment edge $R_j \\to T_i$ 转化为claim edge $T_i \\to R_j$ 。\n为了使RAG算法正确工作，只有在线程的所有claim edge被添加到RAG中之后，该线程才能被允许请求资源。或者，线程可以请求已经设置好的claim edge边关联的资源，并且不允许在线程持有资源的情况下添加claim edge到RAG。claim edge实际上起到了延迟RAG中环路出现作用。\n为了更好的说明RAG算法，考虑下图，假设线程 $T_1$ 请求资源 $R_2$ ，尽管 $R_2$ 当前未被占用，但我们也不能把它分配给 $T_1$ 。因为这个举动将导致RAG中出现环路，系统将进入不安全状态。如果 $T_2$ 请求 $R_2$ ，并且 $T_1$ 请求 $R_1$ ，就会出现死锁。\n银行家算法 当资源有多个实例时，RAG算法就无法使用了，这个时候可以使用银行家算法(banker\u0026rsquo;s algorithm)。算法要做的就是判断满足线程的资源请求是否会导致系统进入不安全状态，若是，则拒绝请求，否则予以分配，系统继续处于安全状态。\n为了实现银行家算法，我们需要维护几个和资源分配状态有关的数据结构，其中n代表线程数，m代表资源种类数：\n Available。m维向量，其中Available[i]表示资源 $R_i$ 的可用实例数。 Max。n x m维矩阵，其中Max[i][j]表示线程 $T_i$ 对资源 $R_j$ 的最大需求数。 Allocation。n x m维矩阵，其中Allocation[i][j]表示线程 $T_i$ 当前持有资源 $R_j$ 的实例数。 Need。n x m维矩阵，其中Need[i][j]表示线程 $T_i$ 要完成工作还需要的资源 $R_j$的实例数。注意：Need[i][j] = Max[i][j] - Allocation[i][j]。  为了简化算法的表示，我们定义，对于长度为n的向量A和向量B，A ≤ B if and only if A[i] ≤ B[i] for all i = 1,2,...,n。\n安全性算法 为了使用银行家算法，我们需要判断系统是否处于安全状态。检测系统是否处于安全状态的算法如下：\n 设Work和Finish分别是长度为m和n的向量。Work为可用资源的一个工作副本，它可以在分析过程中被修改，初始值为Available。Finish是一个布尔类型的向量，用来表示某个线程是否能够执行完成，其所有元素均初始化为false。 找到一个满足Finish[i] == false \u0026amp;\u0026amp; Need[i] ≤ Work的i。满足条件的线程还未完成，但可以为其分配资源。如果不存在这样的i，跳转到步骤4。 Work = Work + Allocation[i]，Finish[i] = true。这表示线程i执行完成并将其占用的资源归还到工作池。跳转到步骤2。 如果所有的i都满足Finish[i] == true，那么系统当前处于安全状态，因为我们找到了一个安全序列。  为了判断系统是否处于安全状态，算法可能需要执行 $m \\times n^2$ 次。\n资源请求算法(银行家算法) 资源请求算法(Resource-Request Algorithm) 用来判断某个请求的安全性，只有一个请求是安全的，才为其分配资源。若满足某个请求后系统仍处于安全状态，那么这个请求就是安全的。用Request[i][j]表示线程 $T_i$ 请求资源 $R_j$ 的实例数，当线程 $T_i$ 发出一个资源请求时，将按照以下步骤处理：\n 若Request[i] ≤ Need[i]，跳转步骤2。否则抛出错误，因为线程请求的资源数超过了它需要的最大值。 若Request[i] ≤ Available，跳转步骤3。否则，由于资源不够，线程 $T_i$ 必须等待。 检查是否可以安全地满足Request[i]。先假装请求的资源已被授予，然后运行安全性算法检查系统是否仍然处于安全状态。若是，满足Request[i]，否则线程 $T_i$ 必须等待直到Request[i]变成安全的。假装分配资源的伪代码为：  可用资源减少：Available = Available - Request[i]; 已分配资源增加：Allocation[i] = Allocaion[i] + Request[i]; 资源需求量减少：Need[i] = Need[i] - Request[i]。    一个例子 假设系统中有5个线程，10个资源A实例、5个资源B实例和7个资源C实例，系统当前的快照状态如下：\n   线程 Allocation(A, B, C) Max(A, B, C) Available(A, B, C) Need(A, B, C)     $$T_0$$ (0, 1, 0) (7, 5, 3) (3, 3, 2) (7, 4, 3)   $$T_1$$ (2, 0, 0) (3, 2, 2)  (1, 2, 2)   $$T_2$$ (3, 0, 2) (9, 0, 2)  (6, 0, 0)   $$T_3$$ (2, 1, 1) (2, 2, 2)  (0, 1, 1)   $$T_4$$ (0, 0, 2) (4, 3, 3)  (4, 3, 1)    运行银行家算法可以发现当前系统处于安全状态，因为存在一个安全序列 $\u0026lt;T_1, T_3, T_4, T_2, T_0\u0026gt;$ 。\n假设现在线程 $T_1$ 需要请求1个资源A和2个资源C，即Request[1] = (1, 0, 2)。为了判断Request[1]是否是安全的，我们先检查Request[i] ≤ Available，条件成立，现在假装Request[1]已被满足，系统将进入一个新的状态：\n   线程 Allocation(A, B, C) Max(A, B, C) Available(A, B, C) Need(A, B, C)     $$T_0$$ (0, 1, 0) (7, 5, 3) (2, 3, 0) (7, 4, 3)   $$T_1$$ (3, 0, 2) (3, 2, 2)  (0, 2, 0)   $$T_2$$ (3, 0, 2) (9, 0, 2)  (6, 0, 0)   $$T_3$$ (2, 1, 1) (2, 2, 2)  (0, 1, 1)   $$T_4$$ (0, 0, 2) (4, 3, 3)  (4, 3, 1)    现在检查系统是否处于安全状态，运行安全性算法，我们可以找到安全序列 $\u0026lt;T_1, T_3, T_4, T_0, T_2\u0026gt;$ 。因此我们可以立即满足线程 $T_1$ 的请求。满足线程 $T_1$ 的请求之后，假设线程 $T_0$ 又来请求资源，Request[0] = (0, 2, 0)，因为(0, 2, 0) ≤ （2, 3, 0)，所以我们假装资源已分配，系统的状态如下：\n   线程 Allocation(A, B, C) Max(A, B, C) Available(A, B, C) Need(A, B, C)     $$T_0$$ (0, 3, 0) (7, 5, 3) (2, 1, 0) (7, 2, 3)   $$T_1$$ (3, 0, 2) (3, 2, 2)  (0, 2, 0)   $$T_2$$ (3, 0, 2) (9, 0, 2)  (6, 0, 0)   $$T_3$$ (2, 1, 1) (2, 2, 2)  (0, 1, 1)   $$T_4$$ (0, 0, 2) (4, 3, 3)  (4, 3, 1)    运行安全性算法会发现，我们找不到任何安全序列，所以不能满足线程 $T_0$ 的这个请求。\n死锁检测与恢复 当系统既不能预防也不能避免死锁，那死锁就有可能出现。这个时候，系统可能就需要具备检测死锁的存在并从死锁恢复的能力了。\n死锁检测 每类资源只有一个实例的死锁检测 当RAG中存在环路时，可能存在死锁。因此，我们检测RAG中是否存在环路即可。\n每类资源有多个实例的死锁检测 当每类资源有多个实例时，我们可以利用银行家算法中的数据结构并稍微修改安全性算法来检测死锁的存在。算法如下：\n 设Work和Finish分别是长度为m和n的向量。Work为可用资源的一个工作副本，它可以在分析过程中被修改，初始值为Available。Finish是一个布尔类型的向量，用来表示某个线程是否已经完成。若Available[i] = 0，则Finish[i] = true，否则Finish[i] = false。 找到一个满足Finish[i] == false \u0026amp;\u0026amp; Request[i] ≤ Work的i。满足条件的线程还未完成，但可以为其分配资源。如果不存在这样的i，跳转到步骤4。 Work = Work + Allocation[i]，Finish[i] = true。这表示线程i执行完成并将其占用的资源归还到工作池。跳转到步骤2。 若存在i，使Finish[i] == false，那么系统中存在死锁。Finish[i] == false的线程就是已经发生死锁的线程。  假设系统中有5个线程，7个资源A实例、2个资源B实例和6个资源C实例，系统当前的快照状态如下：\n   线程 Allocation(A, B, C) Request(A, B, C) Available(A, B, C)     $$T_0$$ (0, 1, 0) (0, 0, 0) (0, 0, 0)   $$T_1$$ (2, 0, 0) (2, 0, 2)    $$T_2$$ (3, 0, 3) (0, 0, 0)    $$T_3$$ (2, 1, 1) (1, 0, 0)    $$T_4$$ (0, 0, 2) (0, 0, 2)     运行死锁检测算法，我们可以找到安全序列 $\u0026lt;T_0, T_2, T_3, T_1, T_4\u0026gt;$ 使得Finish中所有元素均为true，因此当前系统中没有死锁。若当前系统的状态为(线程 $T_2$ 多请求了一个资源C)：\n   线程 Allocation(A, B, C) Request(A, B, C) Available(A, B, C)     $$T_0$$ (0, 1, 0) (0, 0, 0) (0, 0, 0)   $$T_1$$ (2, 0, 0) (2, 0, 2)    $$T_2$$ (3, 0, 3) (0, 0, 1)    $$T_3$$ (2, 1, 1) (1, 0, 0)    $$T_4$$ (0, 0, 2) (0, 0, 2)     这个时候系统就存在死锁了。尽管我们可以回收线程 $T_0$ 占用的资源，但也无济于事，线程 $T_1$ 、 $T_2$ 、 $T_3$ 和 $T_4$ 死锁了。\n死锁检测算法的使用 现在我们已经有死锁检测算法了，接下来的问题是何时去检测它们。一种方法是每当有资源请求时就去检测，虽然越早发现死锁越好，但这可能占用大量的CPU时间。另一种方法是每隔一段时间就检查一次，或者当CPU使用率降到一定阈值以下时去检查。考虑到CPU的使用效率，如果死锁线程达到了一定数量，就没多少线程可以运行了，所以CPU会经常空闲。\n从死锁恢复 当我们成功检测到了死锁，下一步就是采取一些方法使系统重新正常工作：\n 利用抢占恢复。临时将资源从一个线程抢占到另一个线程，另一个线程使用完就送回原线程。 回滚。周期性的创建检查点，当检测到死锁时，回退到某个之前的检查点继续。 杀死进程。杀死一个或多个进程，使之释放资源，便于其它线程正常运行。  和死锁相关的其它问题 活锁 活锁(livelock) 不会阻塞线程，但线程也不能继续执行，因为线程将不断重复执行相同的操作，而且总会失败。这就像两个过于礼貌的人在半路上面对面地相遇：他们彼此都给对方让路，然后又在另一条路上相遇了，并且他们就这样反复地避让下去。可以在重试机制中引入 随机性 来解决问题，这样能减少冲突。\n饥饿 当线程由于无法访问它所需要的资源而不能继续执行时，就发生了饥饿(Starvation)。例如：Java应用程序对线程的优先级设置不当，导致部分线程无法获取CPU时钟周期。可以通过FIFO策略来避免饥饿，也可以在引入老化机制，动态的调整线程优先级，使每个线程的优先级都有机会变成最高的。\n参考资料  ANDREW S. TANENBAUM, HERBERT BOS. Modern Operating Systems, 4th Edition. Pearson, 2015. Abraham Silberschatz, Greg Gagne, Peter B. Galvin. Operating System Concepts, 10th Edition. Wiley, 2018.  ","href":"/operating_systems/deadlocks/","title":"死锁"},{"content":"物理地址与虚拟地址 Computer Systems: A Programmer\u0026rsquo;s Perspective 一书中是这么描述物理地址的：计算机系统的主存被组织成一个由 M 个连续的字节大小的单元组成的数组，每个字节都有一个唯一的 物理地址（Physical Address）。也就是说，物理地址表示的是数据在主存中的物理位置。\n早期的计算机中没有存储器抽象，每一个程序都直接访问物理地址，我们把 CPU 直接访问物理地址的这种寻址方式称为 物理寻址(physical addressing)。而 虚拟地址(Virtual Address) 是 CPU 在程序运行期间生成的一个地址，虚拟地址又叫逻辑地址（Logical Address），虚拟寻址（virtual addressing） 是现代计算机所采用的寻址形式。内存管理单元（Memory Management Unit, MMU） 是 CPU 上的一个专用硬件，负责完成虚拟地址到物理地址的映射，这个过程叫做 地址翻译（address translation）。\n上图左边展示了一个物理寻址的示例，该示例展示了一条指令，它从物理地址 4 开始读取 4 个字节。当 CPU 执行这条指令时，会生成一个物理地址，CPU 通过内存总线将物理地址 4 传给主存。主存从物理地址 4 开始取出 4 个字节，并将它返给 CPU。而上图右边展现了一个虚拟寻址的例子，CPU 在执行指令时拿到的是一个虚拟地址 4100，它将这个虚拟地址传给 MMU，MMU 将这个虚拟地址映射到物理地址 4，然后将物理地址传给主存。\n使用物理地址存在的一些问题：首先，用户程序可以访问任意地址，容易破坏操作系统。此外，使用物理地址会让同时运行多个程序变得很困难。\n地址空间 要使多个应用程序同时处于内存中并且不互相影响，就需要解决两个问题：保护和重定位。地址空间（address space） 是一个地址集合，进程可用它在内存中寻址。每个进程都有一个自己独立的地址空间，一般情况下，进程各自的地址空间是相互独立的。\n计算机的物理内存总是有限的，而所有进程所需的 RAM 总量通常比内存要大得多。有两种处理内存超载的通用方法：交换（swapping） 和 虚拟内存（virtual memory）。交换技术将一个进程完整地载入内存，运行一段时间，然后再将它存回硬盘。空闲进程主要存储在硬盘上，所以只要它们不运行就不会占用内存。虚拟内存能够使程序在只有一部分被载入内存的情况下运行。\n空闲内存管理 操作系统必须对动态分配的内存进行管理。追踪内存的使用情况的方式主要有两种：位图（bitmap） 和 空闲链表（free list）。\n位图 使用位图时，内存被划分为分配单元（分配单元的大小根据实际情况确定，小到几个字，大到几千个字节），每个分配单元对应于位图中的一位。位图中用 1 表示空闲，0 表示已被使用；也可以用 0 表示空闲，1 表示已使用。因此，位图又称 位向量（bit vector）。\n例如，假设内存中前 32 个分配单元的第2、3、4、5、8、9、10、11、12、13、17、18、25、26和27个分配单元是空闲的，而其它的已被使用。若用1表示分配单元已被使用，则空闲内存可以表示为：11000011000000111001111110001111...。\n分配单元的大小是一个很重要的设计因素。分配单元越小，位图越大。位图的大小同时取决于内存的大小和分配单元的大小。使用位图的主要问题是：在决定把一个占用k个分配单元的进程调入内存时，内存管理器必须搜索位图，在位图中找出k个连续的空闲块，这是一个很耗时的操作。\n空闲链表 使用空闲链表时，需要维护一个记录已分配内存段和空闲内存段的链表，其中链表中的一个节点要么保护一个进程，要么表示两个进程间的空闲区。链表中的每一个节点都有节点标识(例如用H(hole)标识空闲，P(process)表示进程)、起始地址、长度和一个指向下一节点的指针。上图就展示了一个按地址顺序排列的单链表结构，有时候双向链表可能更合适。\n当创建新进程或从硬盘换入的进程时，有几种算法可以为其分配内存：\n 首次适配（first fit）：内存管理器从空闲链表头进行搜索，直到找到一个能够容纳下进程的空闲区，若空闲区大于进程所需内存的大小，则将空闲区分为两部分：一部分供进程使用，另一部分形成新的空闲区。首次适配算法的速度非常快，因为它尽可能少的搜索节点。 下次适配（next fit）：工作方式和首次适配相同，不同点在于每次找到合适的空闲区后就记录当前位置，下次寻找空闲区时就从之前记录的位置开始搜索。 最佳适配（best fit）：从头到尾搜索整个链表，找到能容纳进程的最小空闲区。因为要搜索整个链表，所以最佳适配算法的速度比较慢，而且容易产生大量无用的小空闲区。 最差适配（worst fit）：最佳适配算法会产生很多非常小的空闲区，为了避免这一问题，最差适配算法总是给进程分配最大的可用空闲区，使新形成的空闲区较大而可以继续使用。 快速适配（quick fit）：为常用大小的空闲区单独维护一个链表，这样一来，寻找一个指定大小的空闲区是十分迅速的。  虚拟内存 为了解决进程所需空间比物理内存大的这个问题，MMU 将一部分硬盘空间作为 RAM 的补充，形成 虚拟内存（virtual memory）。虚拟内存的基本思想是：每个程序都有自己的地址空间，这个地址空间被划分为了很多被称为 页（page） 的块。每一页都是一个连续的地址区间。页被映射到物理内存上面，运行程序并不需要所有的页都在物理内存中。当程序引用了其地址空间的某一部分时，若这一部分这物理内存内，则硬件执行必要的映射，否则操作系统需要将不在物理内存中的那一部分地址载入物理内存并重新执行失败的指令。\n虚拟内存适合在多道程序设计系统中使用，同一时刻物理内存中存有多个程序的页，当一个程序等待其页面被载入内存时，CPU 可以被另一个进程使用。虚拟内存本质上是一个虚拟地址空间，硬盘上必须有一个该地址空间的完整副本。\n页式管理 大部分虚拟内存系统中都会使用一种被称为 分页（paging） 的技术。虚拟地址空间被划分为若干固定大小的单元，这里的单元称为 页（page）。物理内存也被划分为若干固定大小的单元，不过组成物理内存的单元被称为 页框（page frame）。页和页框的大小通常是一样的。\n虚拟地址是一种解决内存超载的方法，这意味着虚拟地址空间大于物理地址空间。也就是说，当页和页框的大小相同时，页的数量将多余页框的数量。也就意味着，有一部分页没有被映射到页框。当程序访问了一个未映射的页，即物理内存中没有该页对应的页框，就会发生 页错误（page fault）。这个时候，MMU会发出一个缺页中断，使 CPU 陷入操作系统，操作系统先将一个页框对应的页换出到硬盘，把需要访问的页从硬盘换入到这个页框中，修改映射关系，重新启动被中断的指令。\n分页相关工作 操作系统在以下四段时间内需要做和分页有关的工作：进程创建时、进程执行时、发生页错误时和进程终止时。\n当在分页系统中创建一个新进程时，操作系统必须决定出程序和数据所占的初始空间大小并为它们创建页表。操作系统需要在内存中为页表分配空间并初始化。当进程被换出时，页表不需要驻留在内存中，但是在进程运行时，页表必须在内存中。此外，操作系统还需要再硬盘上的交换分区中分配空间，以便页被换出时有地方存放。交换分区也必须用程序正文和数据初始化，以便新进程在启动中遇到页错误时可以载入需要的页。页表和交换分区的信息必须被记录在进程表中。\n当进程被调度执行时，MMU 会被重置，TLB 会被清空，这样就清除了上一个进程的痕迹。新进程的页表成为当前页表。\n当页错误发生时，操作系统必须读取硬件寄存器并找出导致错误的那个虚拟地址，然后计算出所需要的页并在硬盘上找到它。之后，操作系统需要为新页找到一个可用的页框并将新页放进去。最后，回退程序计数器，使它指向引起页错误的指令，并重新执行该指令。\n当进程退出时，操作系统必须释放进程的页表、页和页在硬盘上所占用的空间。如果某些页是共享的，只有当使用这个页的最后一个进程终止时，才可以释放相应的空间。\n页表 每个由CPU生成的虚拟地址被划分为两部分：一个 页号（page number） 和一个 页偏移（page offset）。如果虚拟地址空间的大小为 $ 2^m $ 并且页的大小为 $ 2^n $ 字节，那么虚拟地址的高m-n位就被用作页号，而虚拟地址的低n位被用作页偏移。因此，虚拟地址可以被表示成下面这样：\n页号被用来索引页表（page table），以得到虚拟页对应的页框号，页偏移则为虚拟地址在对应页框中的位置。如下图所示，页表中包含了每个页框的起始地址，页框的起始地址加上偏移量就得到了物理地址。\nMMU通过以下步骤来完成虚拟地址到物理地址的翻译：\n 从虚拟地址中取出页号p并将其用作页表的索引。 从页表中取得p对应的页框号f。 将虚拟地址中的页号p替换为页框号f。  由于页偏移d没有被修改，所以页框号f和页偏移d就构成了真正的物理地址。\n分页系统面临的问题 在任何分页系统中，都面临两个问题：\n 虚拟地址到物理地址的映射必须非常快。 如果虚拟地址空间很大，那么页表也会很大。  第一个问题是由于每次访问内存都需要进行虚拟地址到物理地址的映射，所有的指令最终都必须来自内存，并且很多指令也会访问内存中的操作数。也就是说：每条指令都至少需要访问一次或两次页表。\n第二个问题和现代计算机使用的虚拟地址大小有关。对于一个32位的虚拟地址，假设页大小为4KB，将得到超过100万页，这意味着页表将包含超过100万个表项。如果每个页表项大小为4字节，每个进程就需要4MB的物理地址空间来存放页表。每个进程都需要有自己的页表，因为每个进程都有自己的虚拟地址空间。\n大而快速的页映射需求成为了构建计算机的一个巨大约束。从概念上看，最简单的设计就是使用一组快速的 硬件寄存器 组成的单个页表，表中的每一项都对应一个虚拟页。当启动一个进程时，操作系统将该进程在内存中的页表的副本加载到这组寄存器中，之后进程运行期间就不再需要到内存中访问页表了。这个方法的优势在于简单，地址映射过程中也不需要访问内存。缺点是在页表很大时，代价高昂，多数情况下并不实用。此外，每次进程上下文切换时，都需要重新载入整个页表，性能低下。还有一种极端的方法就是将整个页表都保存在内存中。硬件只需要一个寄存器，寄存器中存放页表开始位置的指针。当进程发生上下文切换时，只需要重新装载这个寄存器。这种做法的缺点是：在每条指令执行期间，都需要访问一次或多次内存以读取页表，速度非常慢。\nTLB 通过加快虚拟地址到物理地址的映射过程解决了映射速度慢的问题，而多级页表和反向页表提供了巨大虚拟地址空间的解决方案。\nTLB 为了加速完成地址映射，减少到内存访问页表的次数，TLB(Translation Lookaside Buffer) 出现了。它是一个小型的硬件设备，包含了少量页和页框的映射关系，可以让 MMU 在不访问内存的情况下完成虚拟地址到物理地址的映射。TLB 通常在 CPU 当中，它很小，其包含的表项数通常在 32 和 1024 之间。MMU 在收到 CPU 生成的虚拟地址后，会首先检查页号是否存在于 TLB 中，如果存在，就可以立马得到页框号。如果页号不在 TLB 中(发生 TLB缺失（TLB miss） )，就必须去访问内存中的页表，获取到页框号之后，再去访问内存。然后，将刚才所用的页号和页框号添加到 TLB 中，以便加速下次访问。如果 TLB 已经满了，就必须采用某种置换策略（比如 LRU）从 TLB 淘汰一个表项，并用新找到的页表项替换它。当 TLB 从页表加载时，所有的字段都来自于内存。\nTLB 的完整工作流程如下：\nTLB 缓存映射 TLB 中的映射关系是一对一的，这种基于缓存行的映射方案主要有 3 种：\n 全相联映射（Fully Associative Mapping） 直接映射（Direct Mapping） （n路）组相联映射（(n-way) Set-Associative Mapping）  相联（Associative）指的是缓存的键与值之间的映射范围。全相联映射中，一个键可以映射到所有值，而组相联映射中一个键仅可以映射到一组值。\n若采用全相联映射实现 TLB，那么在根据页号查找页框号时，就需要遍历所有的缓存条目，在缓存条目较多时查询速度会下降，这种方案有着明显的性能缺陷。\n直接映射有点类似于哈希函数，假设 TLB 有 64 个条目，那么通过 page number % 64 就可以直接计算出页框号的位置。但这种方式在缓存命中率方面又会有问题。当我们需要进行页置换时，只有唯一的页面可供选择（因为给定虚拟地址，页框就确定下来了），因为直接映射对各种页面置换算法很不友好。\n若一个页号可以映射到多个（数量不太多）页框号，则可以有效避免全相联映射的性能问题，还能让高频使用的页框被保留下来，这就是组相联映射。例如，因特尔 i7 CPU 的 L1 TLB 采用 4-way 64 条目的设计，L2 TLB 则采用 8-way 1024 条目的设计。\n大内存分页 多级页表 如果我们有一个32位的虚拟地址空间，页大小为4KB，页表项大小为4B，那么即使程序所引用的只是虚拟地址空间中很小的一部分，也总是需要一个4MB($2^{32}\\div2^{12}\\times2^2=2^{22}$)的页表驻留在内存中。对于64位地址空间，问题将变的更加复杂。很明显，我们并不希望在内存中划出这么大一块连续的区域用来存储页表。多级页表的秘诀就在于：避免让整个页表一直驻留在内存中，尤其不应该保留那些用不上的页。\n对于二级页表而言，页表本身也被分成了很多页。考虑一个32位的地址空间，若页大小为4KB，则虚拟地址被分成20位页号和12位页偏移。因为我们也对页表分页，所以页号进一步被划分为10位页号和10位页偏移。因此，虚拟地址可以表示成下面这样：\n在上面的虚拟地址中，p1索引外层页表，p2索引内层页表，d为页偏移。虚拟地址到物理地址的翻译过程如下：\n一级页表中的每个表项负责映射虚拟地址空间中一个4MB的块，而二级页表中的每个表项都负责映射一个4KB的虚拟内存页面。举个例子，假设有一个32位的虚拟地址0x00403004(十进制为4206596)，这个虚拟地址对应p1=1, p2=3, d=4。MMU首先使用p1来索引一级页表得到表项1，它对应地址4M ~ 8M-1(第1个4M块)。然后使用p2来索引二级页表得到表项2，它对应的虚拟地址范围是它所在4M块内的12288 ~ 16383(第3个4K块)，对应绝对地址4206592 ~ 4210687，，这个表项就含有虚拟地址0x00403004的页框号。\n二级页表从两个方面减少了内存占用：\n 如果一级页表中的某个表项是空的，那么相应的二级页表就根本不存在。 只有一级页表才需要常驻内存，系统可以在需要时创建、换入或换出二级页表。只有最常使用的二级页表才需要缓存在主存中。  以上所说的二级页表可以扩充为三级、四级甚至更多级。级数越多，灵活性越大。\n哈希页表 对于32位的虚拟地址空间，多级页表可以工作得很好。但对于更大的虚拟地址空间，多级页表就不是一个好主意了，这个时候一般使用 哈希页表（hashed page table）。 哈希页表对虚拟页号进行哈希得到哈希值，哈希表中的每个表项都包括一个链表(解决哈希冲突)，链表的节点包含3个字段：虚拟页号、虚拟页号映射的页框号和指向下一节点的指针。\n算法首先将虚拟地址中的虚拟页号哈希到哈希表，然后将虚拟页号和哈希表中第一个元素的虚拟页号相比，若相同，则使用该元素中的页框号去生成物理地址，否则沿着链表继续搜索。\n反向页表 反向页表（inverted page table） 是多级页表的一个替代方案。反向页表中，每一个表项对应的是一个页框，而不再是页。如果说多级页表是为虚拟地址空间设计的，那么反向页表就是为物理地址空间设计的。反向页表中的每个表项都记录了该页框对应的虚拟页和拥有该虚拟页的进程的信息。因此，系统中只有一个页表，并且对于每个页框来说，只有一个表项和它对应。\n因为是所有进程公用一个页表，所以我们需要区分出不同进程，上图在查找页表时使用进程id。当需要访问内存时，内存子系统会使用进程id和虚拟页号来搜索页表，如果在表项i处找到了符合要求的信息，则使用i和d生成物理地址。若搜索失败，则说明这是一个非法的地址访问。\n尽管反向页表节省了大量空间，但它也有严重的缺陷：从虚拟地址到物理地址的转换变得非常困难。当进程访问页面p时，硬件不能再通过将p作为页表索引来查找物理页框，而是必须搜索整个反向页表来查找满足要求的表项。此外，每个内存访问操作都需要进行一次搜索操作。\nTLB能够帮助缓解方向页表的缺陷，如果TLB能够记录所有频繁访问的页，地址转换就可能变得像往常的页表一样快。但是当TLB未命中时，需要使用软件搜索整个反向页表，一个可行的方法是建立一张哈希表，用虚拟地址来哈希，将内存中具有相同哈希值的虚拟页链接在一个链表上。一旦新的页框号被找到，就更新TLB。\n大页表 若有一个进程需要 1G 的内存，而默认分页大小为 4K，这时该进程就需要 $1G / 4K = 262144$ 个页。即使是拥有 1024 个条目的 i7 L2 TLB 也会 $262144 / 1024 = 256$ 个页复用 1 个缓存行的情况，很容易冲突，这是可以考虑采用大内存分页（Large Page 或 Huge Page）。若操作系统提供 4M 大小的页，则只需要 256 个页，这能大大提高 TLB 的查询性能。Linux 内核至 2.6 版本起就提供了 HugePage 的功能，默认不开启。我们可以通过修改系统配置项来开启它，具体操作方法可以参考 Debian 的 wiki。\n页面置换算法 当缺页中断出现时，操作系统必须从内存中换出一个页，以便为即将调入的页腾出空间。如果被换出的页在驻留内存期间被修改过，则必须更新其在硬盘上的副本。但是如何选择要换出的页呢，不同的页面置换算法采用了不同的置换策略。TLB 作为页表的高速缓存，它应该尽可能地将未来使用频率最高的数据保留住，并换出未来使用频率最低的数据。\n当我们访问缓存查找数据时，要么命中（Hit），要么缺失（Miss）。假设访问缓存的时间（延迟）是 L，访问缓存数据缺失的概率为 M，缺失的平均代价是 C（即缓存缺失后获取数据的平均时间），则缓存的平均响应时间为：\n$$ (1 - M) \\times L + M \\times C = L + (C - L) \\times M $$\nL 通常是不会变的，它有缓存本身决定。所以，我们需要想办法降低 C 和 M，页面置换算法主要考虑的就是如何提高缓存命中率。\n最优页面置换算法 最优（optimal, OPT） 页面置换算法的思想是：考察每页接下来不被访问的时间，置换掉不被访问时间最长的那个页。\n对固定数量的页框来说，最优页面置换算法能够保证最低的页错误率。该算法难以实现，因为算法需要知道页在未来的使用情况。\n最近未使用页面置换算法 从概率上说，最近没有使用的数据，未来使用的概率会比最近经常使用的数据低。 操作系统可以为每一页设置一个两个标志位：R 表示页最近的访问情况，M 表示页最近的修改情况，访问页时，将 R 位设为 1，修改页时，将 M 位设为 1。将出现 4 类组合：\n 00：未访问，未修改。 01：未访问，已修改。 10：已访问，未修改。 11：已访问，已修改。  最近未使用（Not Recently Used, NRU） 页面置换算法在发生页错误时，随机从 RM 值最小的页中选取一个淘汰。算法隐含的意思是：在最近一个时钟周期内，淘汰一个没有被访问但已修改的页比淘汰一个被频繁访问但未修改的页要好。\n先进先出页面置换算法 操作系统维护一个所有当前在内存中的页的链表，最新换入的页放在链表尾，而最早换入的页在链表头。当发生页错误时，先进先出（First-In First-Out, FIFO） 页面置换算法会淘汰链表头部的页面，并将新换入的页面放在链表尾。算法本身很简单，但没有利用到缓存的局部性原理，可能将经常使用的页换出。\n第二次机会页面置换算法 为了避免 FIFO 算法可能将常用页面换出这一问题，第二次机会（second chance） 算法检查最老页的 R 位，如果为 0（这个页面未被访问），立即换出；如果为1，就将R位的值重置为0，并将该页放到链表尾，修改载入时间使得它看起来就像是刚被加载到内存的一样，然后继续搜索，直到找到一个未访问（R=0）的页。若所有的页都被访问过，第二次机会算法就退化成了的 FIFO 算法，算法将把所有的页都访问一遍并在这个过程中重置 R 位，最终由回到第一个 R 位被重置的页，并将其淘汰，所以算法总是可以结束的。\n时钟页面置换算法 尽管第二次机会算法很合理，但它经常需要在链表中移动页，这一操作可能不是很有必要，还降低了效率。一个更好的办法是将所有的页都保存在一个类似于钟面的环形链表中，并用一个表针指向最老的页面。当发生页错误时，算法首先检查表针指向的页，根据该页的R位进行操作：若为0，则淘汰该页面，否则清除R位并向前移动表针，重复这个过程直到找到一个R位为0的页并将其换出。这就是 时钟（clock） 算法。\n最近最少使用页面置换算法 基于局部性原理，在前面几条指令中频繁使用的页很有可能在后面的几条指令中被使用。当发生页错误时，置换未使用时间最长的页，这就是 最近最少使用（Least Recently Used, LRU） 算法。和 NRU 相比，LRU 会考虑一个时间范围内的数据，数据的参考范围更大。LRU 的思想是：最近一段时间最少使用到的数据应该被淘汰，空间应该留给最近频繁使用的数据。\nLRU 是一个不错的算法，主要的问题就是如何实现 LRU 算法。算法的实现可能需要硬件的支持，问题就在于如何快速找到最近最少使用的那个页。常见的实现有两种：\n 使用计数器。为页表中的每个表项都关联一个上次使用时间（time-of-use）的字段和一个计数器（记录逻辑上页的上次访问时间）。当页被访问时，计数器加一，然后将计数器的值更新到对应的 time-of-use 中，这样我们可以换出 time-of-use 值最小的页。这种实现在每次访问内存时都需要在页表中搜索 LRU 页并修改该页的 time-of-use 值。 使用队列。维护一个队列尝试取第一条指令时，将触发缺页中断，迫使操作系统将包含第一条指令的页装入到内存。由全局变量和栈引起的页错误通常也会接踵而至。一段时间后，进程需要的大部分页都在内存中了，进程开始在页错误发生频率较低的情况下运行。这个策略被称为请求调页（demand paging），因为页是按需载入，而不是预先载入。  大部分进程在工作时都会表现出一种 局部性访问（locality of reference） 行为，即进程不会均匀地访问它的地址空间，访问往往是集中于一小部分页上。一个进程当前正在使用的页的集合被称为它的 工作集（working set），如果整个工作集都在内存中，那么进程很少会遇到页错误。若内存太小而无法容纳整个工作集，进程在运行过程中就会遇到大量的页错误，进而减慢进程的运行速度。如果程序每执行几条指令就产生一个页错误，那么就称这个程序出现了颠簸（trashing）。\n不少分页系统都会设法追踪进程的工作集，以确保进程的工作集在进程运行前就在内存中，这种方法被称为 工作集模型（working set model），目的在于大大降低页错误率。在进程运行期预先载入其工作集也叫 预先调页（prepaging），需要注意的是进程的工作集是随时间变化的。工作集算法的思想就是：当发生页错误时，淘汰一个不在工作集中的页，如果页表中所有的页都在工作集中，就淘汰生存时间最长的那个。\n工作集时钟页面置换算法 工作集算法其实是比较耗时的，因为当页错误发生时，可能需要扫描整个页表才能确定被淘汰的页。有一种改进的算法，它基于时钟算法，又使用了工作集信息，称为 WSClock。\n也时钟算法一样，WSClock 也需要一个循环链表。一开始，链表是空的。当第一个页被载入后，它也被加到链表中。随着越来越多的页加入到链表，它们会形成一个环。依然使用一个表针指向最老的页面，当发生页错误时，首先检查表针指向的页，如果R位为1，说明该页最近被访问过，不适合被淘汰，于是将该页的R位重置，指针指向下一页；若R位位0，先检查这个页是否被修改，如果未被修改，置换它，否则表针继续前移(避免由于调度硬盘写操作引起的进程切换，因为内存中被修改过的页在换出前需要被同步回硬盘)，期望找到一个未访问且未修改的页。\n段式管理 段（segmentation） 指一段固定长度的，连续的地址空间。例如将一个程序分为代码段、数据段、堆栈段等，每个段都是一个独立的地址空间。有两种类型的段：\n 虚拟内存段（virtual memory segmentation）：一个进程被划分为很多段，不需要所有的段同时处于内存中。 简单段（simple segmentation）：一个进程被划分为很多段，进程运行时所有的段都在内存中。  段表 段表（segment table） 被用来完成二维虚拟地址到一维物理地址的映射，每一个表项包含该段在内存中的起始物理地址和段的长度。CPU生成的逻辑地址由两部分组成：段号(s)和段偏移(d)。MMU使用段号查找段表，得到段的起始地址和段长，如果段偏移大于段长，则说明这是一个越界访问，将导致段错误（segmentation fault）；否则使用段在内存中的起始地址加上段偏移，得到物理地址。\n页式管理 vs 段式管理 页式管理和段式管理的主要区别：\n   关键点 页式管理 段式管理     大小 固定大小的页和页框，硬件决定 可变大小的段，使用者决定   碎片 可能产生内部碎片 可能产生外部碎片   表 MMU从页表得知页的位置和状态，速度比段表慢，但可以使用TLB加速 段表包含段ID和相关信息，比页表快    页式管理的优点：\n 不会产生外部碎片。 页框不需要是连续的。  页式管理的缺点：\n 可能产生内部碎片。 比段式管理更长的内存查找时间，不过可以通过TLB补救。  段式管理的优点：\n 不会产生内部碎片。 段表比页表小，因此更节省内存。 段的平均大小通常比大多数页大，一个段可以存储更多的进程数据。 比页式管理更小的开销。  段式管理的缺点：\n 可能产生外部碎片。  段页式管理 一些现代计算机采用了 段页式（segmented paging） 的内存管理方案。主存先被划分为若干可变大小的段，每个段进一步被划分为若干固定大小的页。每个段包含一个页表，因此一个进程可能有多个页表。这个时候，一个虚拟地址又三部分组成：段号、段内页号和页偏移。当进行内存访问时，先通过段表找到对应的段，然后通过段内页表找到对应的页，最后根据页偏移找到物理地址。\n参考资料  ANDREW S. TANENBAUM, HERBERT BOS. Modern Operating Systems, 4th Edition. Pearson, 2015. Randal E. Bryant, David R. O’Hallaron. Computer Systems: A Programmer\u0026rsquo;s Perspective, 3th Edition. Pearson, 2016. Abraham Silberschatz, Greg Gagne, Peter B. Galvin. Operating System Concepts, 10th Edition. Wiley, 2018. Translation lookaside buffer. Hugepages.  ","href":"/operating_systems/main_memory/","title":"主存"},{"content":"计算机系统由硬件系统和软件系统两大部分组成。硬件包括处理器、主存、磁盘、打印机、键盘、鼠标、显示器、网络接口以及各种输入/输出设备等。操作系统管理这些硬件资源，并为用户程序提供了一个更简单的计算机模型。软件系统包括系统软件和应用软件，操作系统属于系统软件。\n大多数计算机有两种运行模式：内核态(kernel mode)和用户态(user mode)。操作系统运行在内核态，在内核态模式下，操作系统具有对所有硬件的完全访问权，可以执行机器能够运行的任何指令。软件系统的其余部分运行在用户态。在用户态下，只能使用机器指令的一个子集，例如用户态下不能使用有关 I/O 和内存保护的指令。为了从操作系统中获得服务，用户程序必须使用 系统调用(system call) 以陷入内核并调用操作系统，TRAP 指令把用户态切换到内核态并启用操作系统，当有关工作完成后，在系统调用后面的指令把控制权返回给用户程序。\n操作系统运行在裸机之上，为所有其它的软件提供基础的运行环境。它有两个基本独立的任务：为应用程序提供一个资源集的清晰抽象，管理硬件资源。\n用户接口程序一般指 shell 或 GUI(Graphical User Interface)，它运行在用户态的最底层，允许用户通过他们运行其它程序，诸如 Web 浏览器、音乐播放器等。我们常见的 bash(Bourne Again SHell)就是大多数 Linux 系统默认的 shell 程序。除了用户可直接使用的 shell 和 GUI 外，操作系统一般还提供了供编程人员调用的 API。\n抽象是管理复杂性的一个关键。好的抽象可以把一个几乎不可能管理的任务划分为两个可管理的部分：第一部分是有关抽象的定义和实现，第二部分是随时使用这些抽象解决问题。操作系统的一个主要任务就是隐藏硬件，创建好抽象，并实现和管理它所创建的抽象对象，最终呈现给应用程序(以及程序员)良好、清晰、一致的抽象。\n参考资料  ANDREW S. TANENBAUM, HERBERT BOS. Modern Operating Systems, 4th Edition. Pearson, 2015.  ","href":"/operating_systems/operating_systems/","title":"操作系统所处的位置"},{"content":" 这篇笔记的主要内容来自Memory Management in the Java HotSpot VM。其主要讲的是J2SE 5.0中HotSpot VM的内存管理，文章中描述了J2SE5.0中的垃圾收集器，并在如何选择和配置垃圾收集器、设置被管理内存的区域大小等方面提出了一些建议。\n 内存管理系统需要考虑以下三个方面：\n 如何分配内存。 如何识别存活对象。 如何回收死亡对象占据的空间，以便将来使用。  显式内存释放 vs 自动内存管理 动态内存分配 几乎所有的现代编程语言都采用动态内存分配(dynamic momory allocation)，即允许进程在运行时分配或释放那些无法在编译期间确定大小的对象，并且对象的存活时间可以超过创建它们的子程序的存活时间。动态分配的对象并不位于于栈(分配对象的程序的 活动记录（activation record） 或 栈帧（stack frame） )上，也没有被静态储存（对象的名字被绑定到编译或链接期间所确定的存储位置），而是位于堆（heap）中。在堆中分配对象有一些好处，它允许程序员：\n 动态选择新对象的大小，从而避免程序运行中出现因硬编码而出现的一些错误。 定义和使用像 list、tree、map 这样的递归数据结构。 将新创建的对象返回给父程序。例如工厂方法。 将函数作为另一个函数的返回结果。例如一些函数式编程语言中的 closure 和 suspension。  堆中分配的对象是通过 引用(reference) 来访问的。通常，引用是一个指向对象的指针，即对象在内存中的地址。然而，引用也可能间接地指向对象，例如引用指向 句柄(handle)，句柄再指向对象。句柄的优点是：在对象被移动时，只需要更新句柄中到对象的指针，而不需要更新程序中所有其它对该对象(或句柄)的引用。\n显式内存释放 在一些编程语言里，程序员需要手动管理内存。手动管理内存一件非常复杂的工作，并可能出现很多错误，这些错误可能导致程序异常甚至崩溃。因此，绝大部分开发时间都被用在了调试并尝试解决这些问题上。手动回收可能会出现以下两个问题：\n 悬挂指针(dangling pointer)。当一个对象还被其它对象引用时，若该对象占用的内存被回收，那些指向它的引用就会称为悬挂指针。悬挂指针一旦出现，程序的行为就会变得不可预测。因为，当尝试通过悬挂指针访问原对象时，被引用的内存空间可能早已分配给其它对象。 内存泄漏(memory leak)。例如，为了释放一个单链表所占用的内存空间，我们需要回收链表中的所有节点，若只回收了头节点，其它节点就无法再被访问，它们占据的内存空间也就无法被回收。内存泄漏会不断的消耗可用内存，直至内存枯竭。  自动内存管理 自动化内存管理解决了显式内存管理中的很多问题。垃圾收集器(Garbage Collector, GC) 防止可以防止悬挂指针的出现，因为只有当一个对象不被任何其它对象引用时，这个对象才能被回收。原则上讲，GC会回收所有不可达对象，但有两点需要注意：\n 追踪式回收(tracing collection) 引入了“垃圾”这一定义明确的概念，但“垃圾”并不一定包括所有不再被使用的对象。 出于效率原因，某些对象可能不会被回收。  GC还解决了内存泄漏的问题，因为它会自动释放所有不再被引用的对象。\n什么是垃圾收集 简而言之，垃圾收集器负责：\n 为新生对象分配内存。 保证不回收被引用的对象。 回收那些不再可达对象的所占据内存空间。  垃圾收集是指发现垃圾并回收的这一过程。那么何时会发生垃圾收集呢，这要看使用何种垃圾收集器(或垃圾收集算法)了。通常情况下，当堆或堆的一部分已满，或已分配的空间超过了某个阈值时，就会进行垃圾收集。\n垃圾收集器的特点 一个理想的垃圾收集器应该具备以下特点：\n 既 安全（safe），又 完善（comprehensive）。也就是说，存活对象不能被错误的回收，并且一个垃圾不应该在经过几轮回收之后依然存在。 高效（efficient），它不会长时间暂停应用程序。 减少甚至消除内存碎片。整理（compaction） 就是一种消除碎片的方式。 可伸缩（scalable）。对于多处理器系统上的多线程应用来说，内存的分配与回收都不应该成为性能瓶颈。  设计选择 当设计或选择垃圾回收算法时，需要考虑很多点：\n Serial vs Parallel。串行简单但慢，并行快但更复杂并且可能导致内存碎片。 Concurrent vs Stop-the-world。并发复杂但暂停应用时间短且可能影响程序性能，STW 简单但暂停应用时间长。 Compacting vs Non-compacting vs Copying。整理式收集器将所有存活对象移动到一起并完全回收其它内存，为新对象分配内存简单又快速。非整理式收集器原地释放垃圾对象的空间，不移动对象，它更快但可能导致内存碎片，还可能加大为大对象寻找合适内存空间的难度。复制式收集器将存活对象复制到一个不同的内存区域，源空间可以直接被认为是完全空闲的，接下来可以直接在它上面为新对象分配内存，这既简单又迅速，但需要额外的时间来复制对象以及额外的空间来存储复制后的对象。  性能指标 有很多指标可以用来衡量垃圾收集器的性能：\n 吞吐量（Throughput）：非垃圾收集时间占总时间的百分比。 垃圾收集开销（Garbage collection overhead）：垃圾收集时间占总时间的百分比。 暂停时间（Pause time）：应用执行期内因垃圾收集而暂停的总时间。 收集频率（Frequency of collection）：垃圾收集多长时间发生一次。 内存占用（Footprint）：GC 运行时占用的内存大小。 及时性（Promptness）：从对象成为垃圾到该对象占用空间可用所经历的时间。  不同类型的应用需要不同的性能指标。例如，交互式应用可能要求较短的暂停时间，实时应用可能要求暂停时间不得超过某个上界，而嵌入式应用可能更关注内存占用。\n分代收集 在 分代收集（generational collection） 算法中，内存被划为为不同的代，不同代保存不同年龄的对象。收集器根据不同代上对象的特点采用不同的垃圾回收算法。分代垃圾收集利用了 弱分代假说（weak generational hypothesis），即：\n 大多数对象都是朝生夕死的。 存在少量年老代对象到年轻代对象的引用。  和年老代相比，年轻代上垃圾收集更加频繁。由于年轻代空间通常很小并且很可能包含大量死亡对象，所以年轻代上的垃圾收集通常很快。\n当年轻代上的对象熬过一定次数的垃圾收集之后，它们就会被 提升（prompt） 到年老代。年老代通常比年轻代大很多，但年老代上的空间占用的增长速度比年轻代要慢。因此，年老代上的垃圾收集的发生频率更低，但花费的时间更多。\nJ2EE 5.0 HotSpot JVM中的垃圾收集器 J2EE 5.0 对应的 HotSpot VM 中包含了 4 种垃圾收集器（它们都是分代收集器）。\nHotSpot 中的分代 HotSpot VM 中的内存由三个分代组成：\n 年轻代（young generation）。大多数对象一开始会被分配在年轻代。 年老代（old generation）。年老代中包含了那些在年轻代中熬过了一定次数垃圾收集的对象，一些大对象也可能直接被分配到年老代。 永久代（permanent generation）。永久代主要存放描述类与方法的那些对象，以及类与方法本身。  年轻代由一个 Eden 区 和两个 Survivor 空间 组成。大多数对象一开始被分配到 Eden 区，少数对象直接分配到年老代。Survivor 空间保存了那些至少经历过一次垃圾收集的对象，在任何时刻，只有一个 Survivor 空间会保存这些对象，而另一个 Survivor 空间是空着的并且在下次垃圾收集之前都不会被使用。\n垃圾收集的种类 当年轻代空间不足时，年轻代上的垃圾收集就会发生，年轻代上的垃圾收集又称 小型 GC（minor GC）。当年老代或永久代空间不足时，完全 GC（Full GC） （有时候又称 大型 GC（major GC） ）就会发生，Full GC 会在所有的分代上进行垃圾收集，即回收整个堆。通常情况下，垃圾回收会先发生年轻代上，然后才是年老代和永久代，不同分代上采用的算法不同。如果回收过程中需要整理内存，各个分代上的整理也是单独发生的。\n有时候，当年轻代上的垃圾收集先发生时，一些对象需要被从年轻代提升到年老代时，而年老代的剩余空间可能不足以容纳下所有的这些对象。在这种情况下，对于除 CMS 以外的收集器来说：并不会运行年轻代上的垃圾回收算法，而是会将年老代上的垃圾回收算法应用于整个堆。CMS 是一个例外，它只能回收年老代而不能回收年轻代。\n快速内存分配 顺序分配 顺序分配（sequential allocation） 使用的是一大块内存，每次从空闲块的一端开始分配所请求大小空间的内存。顺序分配所需要的数据结构非常简单：只需要一个 free 指针和一个 limit 指针。例如，下图展示了为一个大小为 n 的对象分配内存的过程中 free 指针的变化情况：\n当需要为一个新生对象分配内存时，只需要检查剩余空间是否足够容纳下这个对象。如果能，就更新 free 指针并初始化这个对象。\n顺序分配又被称为阶跃指针分配（bump pointer allocation），因为 free 指针是阶跃式的。正因为这个特点，free 指针又称 阶跃指针（bump pointer）。此外，顺序分配也被称为线性分配，因为分配的内存地址在一个指定的内存块上是线性的。顺序分配有以下特点：\n 非常简单。 非常高效。 与空闲链表分配相比，顺序分配的缓存局部性更好，尤其是在移动式垃圾收集器中。 对于非移动式垃圾收集器，顺序分配就没空闲链表分配那么合适了，因为它有可能导致内存碎片。  在 空闲链表分配（free-list allocation） 中，有一种数据结构记录着空闲内存单元的位置和大小。空闲内存单元的组织方式并非一定是链表，也可以是其它的形式。\nTLAB 在多线程应用中，分配操作需要是线程安全的。在 JVM 中，对象创建是非常频繁的行为，即使仅仅修改一个指针所指向的位置，在并发情况下也可能不是线程安全的，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又使用了原来准备分配给对象 A 的内存的情况。有两种方案可以解决这个问题：\n 对内存分配动作进行同步处理。实际上 JVM 采用 CAS + 失败重试的方式保证更新操作的原子性； 把内存分配动作按照线程划分在不同的空间进行，即预先为每个线程在堆中分配一小块内存——TLAB（Thread-Local Allocation Buffer）。TLAB 加上阶跃指针，分配速度可以非常快。当需要为新生对象分配内存时，首先尝试在 TLAB 中分配内存，若 TLAB 上分配失败，就需要采用同步的方式处理了。  串行收集器 当使用 串行收集器（Serial Collector） 时，年轻代和年老代上的垃圾收集都是串行发生的（使用单核 CPU）。在回收过程中，整个应用是被完全暂停的（stop-the-world）。\n年轻代上的串行收集器 上图展示了使用串行收集器在年轻代上的操作：Eden 区中存活的对象会被复制到最初为空的那一个 Survivor 空间（即目标空间 To）。但是有些在 Eden 区中存活的对象可能非常大(大到目标空间无法容纳下)，它们会被直接复制到年老代。最初不为空的那个 Survivor 空间（即源空间 From）中的存活对象也会被复制到目标空间，其中足够老的对象会直接被复制到年老代。需要注意的是：当目标空间已满，而 Eden 区和源空间内还有存活对象时，这些存活对象不论年龄多大都会被直接复制到年老代。复制操作完成之后，Eden 区和源空间内所有剩余对象都被认为是已经死亡的，收集器会直接清空 Eden 区和源空间。最后，只有先前的目标空间包含存活对象，这个时候，将两个 Survivor 空间的角色互换，为下一次垃圾收集做好准备。串行收集器执行完之后，年轻代就成了下面这个样子：\n年老代上的串行收集器 在年老代和永久代上，串行收集器采用的是 标记-清除-整理（mark-sweep-compaction） 算法。在标记阶段，收集器识别出所有依然存活的对象。在清除阶段，收集器并不会真正进行清除垃圾（非存活对象）的操作，而是简单地识别出这些垃圾。最后，收集器会整理堆，将所有的存活对象全部滑动到年老代的一端（永久代类似）。这么做以后，年老代的另一端就是一个连续空闲块，随后就可以采用阶跃指针快速为新对象分配空间。下图展示了整理前后年老代的情况：\n并行收集器 并行收集器（Parallel Collector） 又称 吞吐量收集器（throughput collector），它主要是为了充分利用拥有多处理器的机器上的多个 CPU。\n并行收集器其实是串行收集器的多线程版本。在进行垃圾回收时，并行收集器在年轻代和年老代上都采用了和串行收集器一样的算法，并且都会暂停整个应用。不同的是，并行收集器会在应用暂停期间使用多个线程并行地进行回收工作，回收速度更快。应用的吞吐量因此而提升。\n并行整理收集器 并行整理收集器（Parallel Compacting Collector） 是 J2SE 5.0 update 6 中新引入的收集器。它和并行收集器的不同之处在于：它在年老代上使用了新的垃圾回收算法，年轻代上的垃圾回收算法和之前一样。需要注意的是：并行整理收集器最终会取代并行收集器。\n在回收年老代和永久代时，并行整理收集器还是会暂停整个程序，但滑动整理操作大部分是并行的。每个分代在逻辑上被划分为若干固定大小的 区域（region）。整个收集过程分为三个阶段：\n 标记阶段（marking phase）。从应用代码（GC Roots）出发，直接可达的存活对象（初始存活对象集合）会被分给不同的垃圾收集线程，然后并行标记所有的存活对象。当发现一个对象依然存活时，就会用这个对象的大小和位置信息来更新其所在区域的数据。 汇总阶段（summary phase） 。汇总阶段操作的是区域，而不是对象。由于前面几次垃圾收集的整理操作，每个分代左边部分包含的存活对象比较多，通过整理回收的空间也比较少，因此不值得在它上面进行整理。所以汇总阶段所做的第一件事就是从左边部分开始检查区域的密度(存活对象所占的空间比例)，直到碰到一个密度较小，值得花时间去整理的区域，然后从这个区域开始，为其后的每个区域计算并保存存活数据的第一个字节的新位置。需要注意的是：汇总阶段当前的实现是串行的。 整理阶段（compaction phase） 。收集器利用汇总阶段的数据识别出需要被填充的区域，然后并行的将存活对象复制到这些区域中。最终，堆的一端中包含了大量存活对象，而另一端是一个非常大的空闲块。  并发标记清除收集器 对于那些需要快速的响应时间的应用，吞吐量就不再那么重要了。年轻代上的 GC 通常不会导致长时间的停顿，而年老代上的 GC 执行虽然没那么频繁，但可能导致长时间的停顿（尤其是当堆很大时）。为了解决这个问题，HotSpot 引入了 并发标记清除（Concurrent Mark-Sweep, CMS）收集器 。CMS 收集器又叫 低延迟收集器（low-latency collector）。\nCMS 收集器在年轻代上使用的收集器和串行收集器相同。\n使用 CMS 收集器时，年老代上的大部分收集操作都是在应用执行期间并发进行的。CMS 收集器将收集过程分为了 4 个阶段：\n 初始标记（initial mark）。暂停应用，然后标记应用代码（GC Roots）直接可达的存活对象。 并发标记（concurrent mark）。从 GC Roots 直接可达的对象出发，并发的标记所有间接可达的存活对象。由于应用在运行，对象之间的引用关系会在并发标记期间有所变化，所以在并发标记阶段结束时，不能保证所有的存活对象都被标记。 重新标记（remark）。为了防止并发标记阶段错标，收集器会暂停整个应用，然后并发地标记那些在并发标记阶段引用关系发生变化的对象。这一阶段耗时比初始阶段长，但又远短于并发标记阶段。 并发清除（concurrent sweep）。经过重新标记阶段，堆中所有存活的对象都已被标记。并发清除阶段会回收那些未被标记的对象。  下图展示了年老代上的串行收集器与 CMS 收集器的区别：\nCMS 收集器是唯一一个不对堆进行整理的收集器。即在释放死亡对象占用的空间后，它不会将移动存活对象移动到年老代的一端。这么做节省了时间，但会导致空闲空间变得不连续，因此需要采用空闲链表来管理空闲内存。CMS 收集器的另一个缺点是：它要求的堆空间比其它垃圾收集器都要大。因为应用可以在标记阶段运行并可能继续分配内存，进而间接地消耗了年老代的空闲空间。此外，尽管 CMS 收集器保证在标记阶段识别出所有存活对象，但一些存活对象可能在这个过程中成为垃圾并且不会被回收，而是要等到下一次垃圾回收才会被回收。这些对象被称为 浮动垃圾（floating garbage）。最后，由于缺乏整理，堆中可能出现碎片。为了处理这个问题，CMS 收集器会跟踪常用对象大小，估算未来的分配请求，可能还会拆分或合并空闲块以满足要求。\n不同于其它收集器，CMS 收集器并不是在年老代被填满之后才进行收集，而是尽可能早的开始回收，以便它可以在年老代被填满之前能完成回收工作。否则，CMS 收集器会使用一个更加耗时并且会暂停整个应用的标记-清除-整理算法（即并行和串行收集器采用的算法）进行回收。为了避免这种情况，CMS 收集器会统计一些垃圾回收的数据，然后基于这些数据在恰当的时候触发回收操作。当年老代的空间占用率超过某个值时，CMS 收集器也会启动回收操作。这个域值可以通过命令行参数 -XX:CMSInitiatingOccupancyFraction=n 来指定，其中的 n 代表占年老代的百分比，默认为 68。\n总之，和并行收集器相比，CMS 收集器缩短了年老代上垃圾收集导致的暂停时间，但却付出了一些代价：回收年轻代的速度变慢了，吞吐量降低了，消耗的堆空间更大了。\n参考资料  Memory Management in the Java HotSpot VM. Richard Jones, Antony Hosking, Eliot Moss. The Garbage Collection Handbook: The Art of Automatic Memory Management. Chapman and Hall/CRC, 2011.  ","href":"/java/jvm/memory_management_in_javase5/","title":"HotSpot VM中的内存管理(J2SE 5.0)"},{"content":" 这篇笔记主要来自于The Java HotSpot Performance Engine Architecture，感觉主要是在讲 JDK8中 HotSpot VM 的整体架构。\n Java HotSpot VM 原来是 Sun 公司为 Java 平台实现的高性能虚拟机，是 Java SE 的基础组成部分。为了使 Java 应用达到达到最佳的性能，HotSpot 采用了很多高级技术，包括先进的内存模式、垃圾收集器和自适应优化器。\nHotSpot VM架构 内存模型（Memory Model） 对象的访问定位 Java 程序通过栈上的引用（reference）操作堆上的具体对象。在早期 JVM 中，比如 Classic VM，对象的访问定位是通过 间接句柄（indirect handle） 实现的。由于引用中存放的是句柄的地址，所以在对象被移动时，只需要改变句柄中到对象实例数据的指针即可。这一点虽然可以简化垃圾回收过程中对象的重定位过程，但却成为了性能瓶颈，因为在 Java 中通过句柄访问实例变量需要进行 两次 间接访问。\n在 HotSopt VM 中，对象的访问定位是通过 直接指针（direct pointers） 来实现的，只需要 一次 访问就可以获取到实例变量。当对象在内存回收过程中被移动时，垃圾收集器需要负责找到并更新所有到该对象的引用。\n两字（Two-Word）的对象头 HotSpot VM 中的对象头大小为两个机器字，而 Classic VM 中的对象头为三个字。通常，Java 对象都很小，所以对象头的大小对空间消耗的影响还是很大的。对象头的第一个字（mark word）包含了 hashCode 和 GC 的状态等信息，而第二个字（kclass pointer）是一个指向对象所属类（class）的指针。只有数组对象的对象头中有第三个部分——这一部分记录了数组的大小。\n以对象表示的反射数据 类、方法和其它内部反射数据都直接被表示成堆上的对象。这既简化了 VM 内部的对象模型，也允许类被为其它对象设计的垃圾回收器回收。\n支持原生线程 每个线程的方法活动栈都是使用宿主操作系统的栈和线程模型表示的。Java 方法和原生方法使用的是同样的栈，因此 C 和 Java 之间的调用更快。通过使用操作系统的线程调度机制，Java 线程是完全可抢占的。使用原生操作系统线程和调度机制的一个主要优点就是可以透明地利用原操作系统的多处理机制。\n垃圾收集（Garbage Collection） HotSpot VM 的内存系统是分代的，这为不同应用场景下选择不同垃圾收集器提供了极大的灵活性。为了满足不同的停顿时间和吞吐量要求，HotSpot 支持选择不同的垃圾回收算法。\n准确性 HotSpot 垃圾收集器是一个完全准确的垃圾收集器。相反，很多其它的垃圾收集器都是保守的（conservative）或部分准确的。保守式垃圾回收吸引人的一点在于，它可以很容易地被添加到不支持垃圾收集的系统中，但它容易出现内存泄漏，还可能导致堆上碎片。\n保守式收集器并不确切的知道所有对象引用的位置。因此，它必须假设一个可能引用某个对象的内存字就真的是对象引用。这意味着它可能会犯一些错误，例如：当一个整数当作是一个对象指针。看起来像指针的内存单元被认为是真的指针，GC 就变得不准确了。这会带来一些负面影响：第一，可能发生无法复现或调试的内存泄漏。第二，因为保守式收集器可能犯错，因此它必须这么做：要么使用句柄来间接引用对象（这降低了性能），要么避免移动对象（因为移动对象需要更新所有在该对象上的引用，这在收集器不能确切知道一个表面上的引用是否是真的引用的情况下是无法做到的）。由于缺乏移动对象的能力，保守式收集器可能导致内存碎片，也无法使用一些高级的分代复制收集算法。\n分代复制收集（Generational Copying Collection） 分代垃圾收集器利用了一个事实：在大多数程序中，绝大多数（通常超过95%）对象的存活时间都非常短。通过将新生对象放置在一个单独的空间里，分代收集器可以实现：\n 因为新生对象的空间是连续分配在一个单独的空间里的，分配速度是非常快的，因为这只需要更新一个指针并做一次该空间是否用尽的检查。 在该空间用尽之前，其中的大多数对象都已经死亡了。这个时候收集器只需要简单地将少数存活的对象移动到其它地方，不再需要为该空间内的死亡对象做其它回收操作。  并行年轻代收集器（Parallel Young Generation Collector） 上面描述的单线程复制收集器适用于很多场合，但它会成为并发程序中的瓶颈。为了充分利用多处理器上的多个 CPU，HotSpot VM 还为年轻代提供了一个可选的多线程收集器，其中使用了多个并行工作的线程来完成存活对象的追踪和复制。这减少了收集年轻代空间的暂停时间，并最大化了垃圾收集的吞吐量。\n当移动对象时，并行收集器努力使相关对象在一起，从而提高内存的局部性以及缓存的利用率，并提高了分配器的性能。这是通过采用 深度优先的顺序 复制对象来实现的。\n标记-整理收集器（Mark-Compact Collector） 尽管分代复制回收器可以高效地收集大多数死亡对象，但长期存活的对象仍然在年老对象区域聚集。有时候，由于可用内存过低或者显式要求，年老对象上的垃圾收集必须进行。HotSpot VM 默认使用标准的标记-整理回收算法来进行年老对象区域的垃圾收集，算法从 RC Roots 开始遍历整个存活对象图，然后进行清理，整理死亡对象留下的空隙。通过压实堆中的空隙而不是将它们收集到空闲链表中，可以避免内存碎片。\n并发标记-清除收集器（Concurrent Mark-Sweep Collector） 对于那些使用大量堆空间的应用，默认的年老代标记-整理收集器可能引起程序中断，因为应用线程被暂停的时间和堆的大小成比例。在应用线程被暂停的非常短暂的时间内，HotSpot VM 为年老对象空间实现了一个可以利用处理器的空闲时钟周期（或空闲处理器）来收集大堆的并发收集器。当应用线程执行时，收集器会进行追踪和清理工作。有时，这会降低应用的最大吞吐量，因为一些处理器的时钟周期被用来进行并发收集。然而，在平均和最坏的情况下，垃圾收集的暂停次数通常降低了1~2个数量级，这使应用的响应时间变得更加的平滑，不会出现默认的同步标记-整理算法在大堆上可能发生的突发。\n并行年老代收集器（Parallel Old Generation Collector） 为了提高占用那些使用巨大堆空间应用的可扩展性，HotSpot VM 为年老代实现了一个并行的标记-整理收集器。CMS 收集器主要关注的是如何降低暂停时间，而并行年老代收集器关注的是如何在暂停整个应用（stop-the-world）的情况下同时使用多个线程以提高吞吐量。\n超快的（Ultra-Fast）的线程同步 Java 提供了语言级别的线程同步，使得它能够非常容易地用细粒度锁来表达多线程程序。而 Classic VM 等早期 JVM 中，同步的实现和其它操作比起来非常低效，细粒度同步因此成了主要的性能瓶颈。\nHotSpot VM 采用自适应自旋技术实现竞争性同步，采用超快的常量时间技术实现非竞争性同步，极大的提升了同步性能。\n64 位架构 早期的 HotSpot VM 的寻址空间被限制为 4GB 内存，即使在 64 位操作系统中也是这样。而在 64 位的 JVM 中，Java 程序可以使用所有的内存。\nHotSpot 编译器 为了提升程序的性能，很多为传统语言开发的编译技术也被应用到了 Java 中。JIT（just-in-time）编译器将 Java 字节码迅速翻译成机器码。然而，JIT编译存在一些问题：\n 由于编译器在用户时间内运行，它的编译速度非常重要：如果它不是非常快，用户会在启动程序时察觉到明显的延迟。因为高级优化通常会显著减慢编译速度，所以进行高级优化非常困难。 即使 JIT 有时间进行所有的优化，Java 也还是没有 C 或 C++ 那样高效。原因有很多：  Java 是动态安全的，这意味着 Java 程序不会违反语言的语义，也不会直接访问任意内存。因此，Java 中的动态类型检查必须非常频繁的执行。 Java 在堆上分配所有对象，而 C++ 中的多数对象是在栈上分配的。这意味着对象的分配速度在 Java 中比在 C++ 中更高。此外，Java 支持自动垃圾收集，它和 C++ 的内存分配开销（包括潜在的清除写屏障的开销）截然不同。 Java 中的大多数方法调用都是虚的（virtual），并且比 C++ 更加频繁。这不仅意味着方法调用性能更显著，还意味着堆方法调用进行静态编译优化更加困难。 由于支持类的动态加载，Java 程序可以迅速被改变。这使得一些全局优化难以进行。    为了解决Java语言的性能问题，HotSpot 采用了自适应优化技术。\n热点检测（Hot Spot Detection） 几乎所有的程序都将绝大部分执行时间花在了执行少数代码上。基于这一点，自适应优化技术解决了 JIT 编译中的问题。与其逐方法进行即时编译，HotSpot VM 使用解释器立即运行程序，并分析执行的代码以检测热点代码片段。然后，专心使用全局原生代码编译器编译热点代码。通过避免编译不常执行的代码，HotSpot 编译器可以更加重视程序中的对性能至关重要的部分，而不必增加全部的编译时间。\n方法内联（Method Inlining） Java 中虚方法调用的频率是一个重要的优化瓶颈。HotSpot 的自适应优化器一旦收集到了热点代码的信息，它不仅会把热点代码编译成原生代码，还会在其上进行大量的方法内联。\n内联可以降低方法调用的动态频率，这就节省了相关方法调用的时间。更重要的是，内联会为优化器提供更大的代码块，是传统编译器优化更加高效。\n动态去优化（Dynamic Deoptimization） 内联基于某种形式全局分析，而动态加载是与之冲突的，因为动态加载会改变程序中的全局关系。因此，HotSpot VM 必须能够动态的反优化之前已优化的热点代码。\n参考资料  The Java HotSpot Performance Engine Architecture.  ","href":"/java/jvm/hotspot_engine_architecture/","title":"HotSpot引擎架构"},{"content":" 这篇笔记地主要内容来自Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide。文章中介绍了一些调优目标以及 JDK 8中 Hotspot VM 提供的一些垃圾收集器，并提出了很多调优建议。\n GC（Garbage Collector） 是一个内存管理工具，它通过以下操作实现了自动内存管理：\n 将新生对象分配到 年轻代（Young Generation），并将足够老的对象提升到 年老代（Old Generation）。 通过并发（或并行）标记阶段找出年老代中的存活对象。当整个 Java 堆的空间使用率超过阈值时，HotSpot VM 就会触发标记阶段。 通过并行复制整理存活对象，以释放堆空间，恢复可用内存。  Ergonomics  Ergonomics is the process by which the Java Virtual Machine (JVM) and garbage collection tuning, such as behavior-based tuning, improve application performance.\n JVM 会根据所运行平台选择默认的 GC、堆大小和运行时编译器等。此外，基于行为的调优会动态地调整堆大小以满足不同应用的要求。\n基于行为的调优 Java SE 为 并行收集器 提供了两个参数以实现不同的应用行为：最大暂停时间（maximum pause time goal） 和 吞吐量（application throughput goal）。这两个参数在其它收集器中是不可用的。在某些时候，预期行为可能不会出现，因为应用需要一个至少能存放下所有存活对象的堆空间。对堆空间的最小要求可能会妨碍达成预期目标。\n最大暂停时间 暂停时间指的是 GC 停止应用并回收内存所花费的时间。GC 会维护一个平均暂停时间以及这个平均暂停时间上的方差，平均暂停时间是加权的，越晚发生的停顿权重越大。当平均暂停时间与方差之和大于最大暂停时间时，HotSpot就认为GC没有满足最大暂停时间的要求。\n最大暂停时间目标是通过命令行参数 -XX:MaxGCPauseMillis=\u0026lt;nnn\u0026gt; 来指定，它告诉 GC 期望的暂停时间不应该超过 \u0026lt;nnn\u0026gt; 毫秒。为了达到这个目标，GC 会调整堆的大小以及其它相关参数，这些调整可能会让 GC 的执行更加频繁，从而有可能降低程序的吞吐量。GC 会优先满足最大暂停时间目标而不是吞吐量目标，但有时候可能连暂停时间目标也满足不了。\n吞吐量 吞吐量是用 GC 时间和非 GC 时间(应用时间)来衡量的，相关的命令行参数是 -XX:GCTimeRatio=\u0026lt;nnn\u0026gt;。GC 时间与应用时间的比值为 1/(1 + \u0026lt;nnn\u0026gt;)。例如，-XX:GCTimeRatio=19 表示 GC 时间占总时间的 5%。\nGC 时间为年轻代与年老代 GC 时间的总和。如果吞吐量目标没有满足，GC 会增大分代的大小，从而使应用能在两次垃圾收集之间的执行更久。\n内存占用 当最大暂停时间和吞吐量都达到要求后，GC 会缩小整个堆空间直到某个目标（当前版本 JVM 中总是指吞吐量）不再满足，然后再尝试满足那个目标。\n调优策略  除非你知道你所需要的堆空间比默认最大堆大小还要大，否则不要设置最大堆内存，而是为你的应用设置一个足够大的吞吐量目标。 为了使应用达到期望的吞吐量，堆的通常会自动扩容和缩容。应用行为的改变也可能导致堆容量的改变。 如果堆已经达到了最大值，而吞吐量仍然低于期望值，那么你的最大堆内存就设置得太小了。这个时候，将最大堆内存设置为一个稍小于总物理内存的值，再次运行应用，如果吞吐量还是低于期望值，那么你的吞吐量目标就定得太高了（已经超过物理内存所能提供的最大吞吐量了）。 如果吞吐量目标达到了，但应用的暂停时间太长了，就设置一个最大暂停时间目标。为了达到最大暂停时间目标，期望的吞吐量可能又无法达到了，这个时候，需要进行权衡，选择一个折中的目标。 GC 为了达到设置的目标，通常会引起堆大小的振荡，即使应用已经处于一个稳定状态也一样。最大吞吐量目标与最大暂停时间目标和内存占用目标时相互竞争的，因为前者可能需要更大的堆，而后两者可能需要的是更小的堆。  分代 人们从经验中发现，绝大多数应用都具备一些共同的特点。为了使垃圾回收过程更加高效，分代收集利用了这些特点，其中最重要的一条就是 弱分代假说（weak generational hypothesis） ——大多数对象的存活时间都非常短。内存以代的形式进行管理，代实际上是一个存放有不同年龄对象的内存池。当某个代被填满时，该代上面的垃圾回收就会发生。\n下图展示了 GC 中（不包括并行收集器和 G1）默认的堆布局:\n设置分代大小 有很多参数可以影响堆的大小。下图展示了堆中 已提交空间（committed space） 与 虚拟空间（virtual space） 的区别：\n-Xmx 用来指定堆的保留空间，如果 -Xms 参数的值比它小，那么并不是所有的保留空间都会被立即提交给 JVM，其中未提交的部分就是上图中用Virtual 标出来的那一部分。堆的不同部分（年轻代和年老代）可以按需扩张，直到达到虚拟空间的上限。\n整个堆 下面所讨论的堆容量的扩张与收缩和默认堆大小并不适用于并行收集器，然而控制堆大小和分代大小的参数却是适用于并行收集器的。最影响垃圾回收性能的因素就是可用内存的总大小，因为垃圾收集在堆被填满时才发生，所以垃圾回收的吞吐量与可用内存总大小成反比。\n默认情况下，JVM 会在每次进行垃圾回收时扩张或缩减堆的容量，以便将空闲空间与存活对象的比值限制在一个特定范围（百分比）内，这个范围是通过参数 -XX:MinHeapFreeRatio=\u0026lt;minimun\u0026gt; 和 -XX:MaxHeapFreeRatio=\u0026lt;maximum\u0026gt; 来设置的，而整个堆的大小边界是通过参数 -Xms\u0026lt;min\u0026gt; 和 -Xmx\u0026lt;max\u0026gt; 来确定的。如果某个分代中空闲空间的比例低于 \u0026lt;minimum\u0026gt;，该分代就会扩张，从而将空闲空间的比例保持在 \u0026lt;minimum\u0026gt;。类似地，如果空闲空间的比例高于 \u0026lt;maximum\u0026gt;，该分代就会被收缩，从而将空闲空间的比例保持在 \u0026lt;maximum\u0026gt;，不过这受限于分代的最小容量。\n下面是一些服务端应用中有关堆大小的通用准则：\n 除非暂停时间成为了你的最大问题，就尝试分配尽可能多的内存给 JVM。默认值通常都太小了。 将 -Xms 和 -Xmx 设为相同值，从而避免 JVM 对堆进行扩容或缩容。但是，如果你的设置不当的话，JVM 是不会帮你救你的。 通常情况下，当你增加处理器时，你也应该增加可用内存，因为内存分配可以是并行的。  年轻代 除可用内存大小之外，第二个最影响垃圾回收性能的就是年轻代占堆空间的比例了。通常，年轻代越大，minor GC 发生的频率就越低。然而，在有限的堆空间下，年轻代越大就意味着年老代越小。而通常情况下，年老代越小，major GC 就越频繁。因此，最优的设置取决于应用中对象存活时间的分布情况。\n默认情况下，年轻代的大小由参数 NewRatio 控制。例如，-XX:NewRatio=3 表示年轻代与年老代的比例为 1:3，即年轻代占整个堆空间的 1/4。NewSize 和 MaxNewSize 参数用来指定年轻代容量的下限与上限，两者的值相同会使年轻代的大小保持不变。\n调整 Survivor 空间的大小 我们可以使用 SurvivorRatio 参数来调整 Survivor 空间的大小，但这通常不会给性能带来多大的影响。例如，-XX:SurvivorRatio=6 表示一个 Survivor 空间与 Eden 区大小之比为 1:6，即每个 Survivor 空间的大小为 Eden 区的 1/6，为年轻代的 1/8。\n若 Survivor 空间过小，复制回收时溢出的对象就会直接被提升到年老代；若 Survivor 空间过大，就会造成空间的浪费。在每次垃圾回收中，JVM都会选择一个阈值（对象在被提升到年老代之前可以被复制的次数），这个阈值的目的是将 Survivor 空间的占用率保持在 50%。\n对服务端应用而言，下面是一些通用准则：\n 先决定能够分配给 JVM 的最大堆内存，然后找出年轻代的最佳设置值。  注意：最大堆内存应该总是小于机器已安装内存，这样可以避免过多的页错误和颠簸。   如果堆大小是固定的，那么增大年轻代就会缩小年老代。将年老代的大小保持在一个足够大的值，以便它在任何时候都能容纳下应用中的存活对象，并且还有一些松弛空间（10% 到 20% 甚至更多）。 在之前所陈述的永久代的约束条件下：  为年轻代分配足够的内存。 当你增加处理器时，你也应该增加年轻代的大小，因为内存分配可以是并行的。    可用收集器 Java HotSpot VM 包含三种不同类型的 GC，每种 GC 都有不同的性能特征：\n 串行收集器（serial collector） 使用一个线程完成所有的回收工作，由于不存在线程间通信的开销，它还是相对高效的。串行收集器最适合用在单处理器机器上，因为它无法利用多处理器机器上的多个 CPU，尽管它对多处理器上运行的小型应用（内存最大约100MB）来说还是很有用的。串行收集器在某些硬件和操作系统配置下会默认启用，也可以通过 -XX:+UseSerialGC 选项显式使用它。 并行收集器（parallel collector） 又称 吞吐量收集器（throughput collector），它并行的进行 minor GC，这能显著降低 GC 的开销。它是为运行在多处理器硬件上的中大型应用准备的。并行收集器也是某些硬件和操作系统配置下的默认 GC，我们也可以通过 -XX:+UseParallelGC 选项显式使用它。  并行整理（parallel compaction） 使并行收集器能够并行进行 major GC。如果没有并行整理，major GC 就会使用单线程以串行地方式进行，这极大的限制了应用地可伸缩性。当 -XX:+UseParallelGC 选项被使用时，并行整理会被默认启用。若要关闭它，可以使用 -XX:-UseParallelOldGC 选项。   并发收集器（mostly concurrent collector） 的大部分工作都是并发进行的，从而使应用被暂停的时间保持在一个较短的范围内。并发收集器为那些响应时间优于吞吐量的中大型应用设计的，因为用来减小暂停时间的技术同时会降低吞吐量。HotSpot VM 提供了两个最大并发收集器：  CMS：通过 -XX:+UseConcMarkSweepGC 启用。 G1：通过 -XX:+UseG1GC 启用。    如何选择 GC 除非你的应用对暂停时间有非常严苛的要求，否则先运行应用并让 JVM 自己选择一个 GC。按需调整堆的大小以提高性能，如果期望的性能无法满足，就使用下面的准则选择 GC：\n 如果应用很小（堆空间不超过 100MB），使用 -XX:+UseSerialGC 选择串行收集器。 如果应用运行在单处理器上且对暂停时间无要求，就让 JVM 自己选择 GC，或者选择串行收集器（-XX:+UseSerialGC）。 如果应用的峰值性能最为重要，且对暂停时间无要求或暂停 1 秒及以上时间是可接受的，就选择并行收集器（-XX:+UseParallelGC ）。 如果响应时间比总吞吐量更重要，且暂停时间必须保持在约 1 秒内，就选择并发收集器（-XX:+UseConcMarkSweepGC 或 -XX:+UseG1GC）。  如果采用了推荐的 GC，但应用还是达不到期望的性能，先尝试调整分代的大小。如果还是不行，就再尝试不同 GC：在多处理器上，用并发收集器减小暂停时间，用并行收集器提升吞吐量。\n并行收集器 并行收集器和串行收集器类似，主要不同在于它采用了多个线程以加速回收垃圾。GC 线程的数量可以通过命令行参数 -XX:ParallelGCThreads=\u0026lt;N\u0026gt; 来指定。\n分代 之前有提到，并行收集器中的堆布局和普通的串行收集器是不同的。下图展示了并行收集器中的分代布局：\nParallel Collector Ergonomics 目标优先级 当使用并行收集器时，HotSpot VM 默认会按照以下顺序实现调优目标：\n 最大暂停时间目标。 吞吐量目标。 最小内存占用目标。  首先满足暂停时间目标，然后才会考虑吞吐量。当前面两个都满足时，才会考虑内存占用。\n调整分代大小 收集器会记录并在每次垃圾收集之后更新一些统计数据(如平均暂停时间)，这些统计数据会帮助GC判断是否已经达到调优目标或做出一些调整。需要注意的是：收集器并不会使用显式地进行垃圾回收（例如 System.gc()）时形成的数据。\n分代容量的增长和缩减是以固定百分比进行的，并且增长率和缩减率不同。默认情况下，分代会以 20% 的增量扩容，而以 5% 的增量压缩。年轻代与年老代容量扩容的增量可以通过 -XX:YoungGenerationSizeIncrement=\u0026lt;Y\u0026gt; 与 -XX:TenuredGenerationSizeIncrement=\u0026lt;T\u0026gt; 来指定，而缩容的增量是通过 -XX:AdaptiveSizeDecrementScaleFactor=\u0026lt;D\u0026gt; 指定的。如果扩容的增量为X个百分比，则缩容的增量为 X/D 个百分比。\n并发收集器 HotSpot VM 提供了两个 并发收集器（mostly concurrent collector）：\n Concurrent Mark Sweep (CMS) Collector：适用于那些偏爱更短的暂停时间并且可以接受同 GC 分享 CPU 的应用。 Garbage-First (G1) Collector：为拥有大内存的多处理机设计的面向服务端应用的收集器。它尝试在实现高吞吐量的同时大概率满足应用对暂停时间的要求。  CMS 收集器 通常情况下，那些有着相对较大的年老代并且运行在拥有 2 个或更多处理器的机器上的应用往往会从 CMS 收集器中获益。对于那些要求暂停时间非常短的应用来说，CMS 也是非常适合的。启用 CMS 收集器的命令行参数为 -XX:+UseConcMarkSweepGC。\n和其它的 GC 类似，CMS 收集器也是基于分代的，因此 minor GC 和 major GC 都会发生。CMS 收集器试图通过使用独立的 GC 线程在应用线程执行期间并发地追踪可达对象来较少 major GC 引起的应用暂停时间。在每次 major GC 中，CMS 会在收集开始时（标记 GC Roots 直接可达的存活对象）短暂暂停所有应用线程，然后在收集过程中再次暂停（重新标记那些在并发标记中由于引用更新而漏标的对象）所有应用线程。第二次暂停通常比第一次暂停时间要长，在每次暂停期间，都会有多个线程同时进行垃圾收集相关工作。其余阶段（包括追踪存活对象和清除不可达对象）中，GC 线程都是在应用执行期间并发执行的。Minor GC 可以与正在进行的 major GC 重叠，工作方式也与并行收集器，但并行收集器在 minor GC 期间会暂停所有应用线程。\n并发模式失败 当 CMS 收集器无法在年老代空间耗尽之前完成回收不可达对象所占用的空间时，或者当 赋值器（mutator） 无法从年老代中为对象分配一个足够大的空闲块时，应用就会被暂停，GC会在所有应用暂停的情况下完成回收工作。并发模式失败（Concurrent Mode Failure） 指的就是 GC 无法 并发地 完成垃圾回收工作，这暗示我们该对 CMS 收集器进行调优了。\nGC 时间过长与 OutOfMemoryError CMS 收集器会在 GC 时间过长时抛出 OutOfMemoryError，即：当垃圾回收耗费的时间超过了总时间的 98%，并且回收的空间少于 2% 时，OutOfMemoryError 就会被抛出。这个特性是用来防止由于堆太小而导致应用运行时间太长，我们可以使用 -XX:-UseGCOverheadLimit 禁用它。\n浮动垃圾 由于应用线程会在 major GC 之间并发地执行，GC 追踪到的存活对象可能在垃圾收集结束之前变得不再可达，这类对象在当前进行的这一次回收过程中并不会被回收，它们被称为 浮动垃圾（floating garbage）。浮动垃圾的数量取决于并发收集的持续时间以及对象引用更新的频率，不过这些浮动垃圾会在下一次垃圾回收过程中被回收掉。\n并发回收何时开始 串行收集器会在年老代被填满之后暂停应用并进行 major GC。与此相反，并发收集的开始时间必须提前确定，以便收集工作在年老代空间耗尽之前完成。否则，应用会由于并发模式失败而被长时间暂停。有几种开始并发收集的方式：\n 基于历史数据，CMS 收集器会预估年老代还有多久被耗尽以及一次 major GC 会花多长时间。CMS 会以在年老代空间耗尽之前完成收集工作为目标开始进行垃圾回收，这些估算时间通常是保守的，因为并发模式失败的代价太高了。 当年老代中被占用的容量超过一个阈值时，垃圾收集工作也会开始。这个阈值默认约为 92%（可能在不同发行版本中做调整），我们可以通过 XX:CMSInitiatingOccupancyFraction=\u0026lt;N\u0026gt; 来指定这个阈值，其中 \u0026lt;N\u0026gt; 为一个范围在 (1, 100) 间的整数。  增量模式  注意：增量模式在 Java SE 8 中正被弃用，将来的版本中可能会移除。\n G1 收集器 Garbage-First（G1） 收集器是一款面向服务端应用的收集器，目标为拥有大内存的多处理机。它尝试在实现高吞吐量的同时大概率满足应用对暂停时间的要求，这说明 G1 并不是一个实时 GC。\nG1 将堆分成若干个大小相等的 区域（region），每一个区域都是一个连续的虚拟地址空间。G1 通过并发标记阶段确定整个堆中对象的存活性，并发标记阶段完成后，G1 就知道哪些区域中的存活对象比较少了。它会先回收这些存活对象少的区域，而这通常可以为应用提供大量的空闲空间，这就是 G1（Garbage-First）名字的来源，即：G1 专注于收集和整理那些差不多被可回收对象（垃圾）占满的区域。\nG1 会将堆中一个或多个区域中的存活对象复制到堆中的单个区域，复制过程中也会完成整理和释放空闲内存的工作。为了降低暂停时间并提升吞吐量，清空区域的操作在多处理机上是并行进行的。G1 采用的是“化整为零”的策略，在整理上比其它 GC 技高一筹：CMS 收集器不会整理堆，因而会导致内存碎片；并行收集器中的并行压缩过程是在整个堆上进行的，那会使暂停时间很可观。\n值得注意的是：G1 并不是一款实时收集器，它大概率会满足暂停时间的要求，但并不是一定会满足。利用从已发生的垃圾回收中采集的数据，G1 会估计目标时间内能够回收区域的数量。G1 最关注的是如何为那些占用内存大且对 GC 延迟有要求的应用提供解决方案，这些应用的通常占用不低于 6GB 的堆空间，并且要求延迟时间低于 0.5 秒。\nHotSpot 开发团队希望用 G1 来代替 CMS 收集器，它与 CMS 的主要不同点在于：G1 是一款整理式收集器。和 CMS 一样，G1 也是为那些要求低 GC 延迟的应用设计的。\n下图展示了 G1 是如何划分堆的：\n堆被划分为若干固定大小的区域（图中的灰色格子）。从逻辑上来讲，G1中依然有分代的概念。一组空闲区域被指定为逻辑年轻代（图中浅蓝色部分），分配操作就是从这个逻辑年轻代开始的。当年轻代满了之后，这组区域上的垃圾收集就会发生。有时候，这组区域以外的区域（年老区域，深蓝色）上的垃圾收集也会同时进行，这被称为 混合收集（mixed collection）。上图展示的正是混合收集，因为年轻区域和年老区域都正在被回收，被收集的区域在图中用红色格子标出。根据年龄，存活对象可能被复制到 Survivor 区域（标有\u0026quot;S\u0026quot;），也可能被复制到年老区域（图中未展示）。图中标有 \u0026ldquo;H\u0026rdquo; 的那些区域包含的是那些大小超过区域的一半或需要特殊对待的大对象。\n浮动垃圾 对象可能在G1回收的过程中死亡并未被回收。为了保证找到所有的存活对象，G1使用了一项叫做 SATB（snapshot-at-the-beginning） 的技术。SATB规定：任何在并发标记开始的时候存活的对象在整个 GC 过程中也是被认为是存活的。因此，G1 中也会出现浮动垃圾。\n卡表 如果 GC 不收集整个堆，GC 就必须知道哪里有从堆中未收集部分指向正被收集部分的指针。这在分代 GC 中很典型：未收集部分通常是年老代，正被收集部分是年轻代。记忆集（remembered set） 就是保存这些信息（年老代到年轻代的指针）的数据结构。卡表（card table） 是记忆集的一种具体实现，HotSpot VM 中用一个字节数组来表示卡表，每一个字节都被称为一张卡，每张卡都对应了堆中的一个地址空间。\n参考资料  Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide.  ","href":"/java/jvm/garbage_collection_tuning_guide_in_javase8/","title":"垃圾回收调优指南(Java SE 8)"},{"content":"线程池是管理一组同构工作线程的资源池，内部主要分为四部分：\n 线程池管理器：负责线程池的创建、销毁、添加任务等管理工作。 工作队列（Work Queue）：保存所有等待执行的任务。 工作者线程（Worker Thread）：从工作队列中取出一个任务并执行，然后返回线程池并等待下一个任务。 任务（Task）：实现了统一的接口，被工作者线程处理和执行。  与“为每个任务都创建一个线程”相比，使用线程池不仅可以平摊线程在创建和销毁过程中产生的巨大开销，还能提高程序的响应性（当请求到达时，工作线程通常已经存在，可以节省创建线程的时间）。通过调整线程池的大小，不仅可以创建足够多的线程以使 CPU 保持忙碌状态，还可以防止多线程相互竞争资源而使应用程序耗尽内存或失败。最后，线程池可以统一管理资源，方便我们对任务进行管理等。\n设置线程池的大小 我们调整线程池大小的主要目的是为了充分利用 CPU 和内存等资源，从而最大限度地提高程序的性能。线程池的理想大小取决于被提交的任务的类型以及所部署系统的特性。通常不应该在代码中固定线程池的大小，而应该通过某种配置机制来提供，或者根据 Runtime.getRuntime().availableProcessors() 来获取可用处理器的数量之后动态计算。\n如果线程池过大，那么大量的线程将在相对很少的 CPU 和内存资源上发生竞争，这不仅会导致更高的内存使用量，而且还可能耗尽资源。如果线程池过小，那么将导致许多空闲的处理器无法执行工作，从而降低吞吐率。\nJava Concurrency in Practice 一书建议我们：对于计算密集型任务，在拥有 N 个处理器的系统上，当线程池的大小为 N+1 时，通常能实现最优的利用率（即使当计算密集型的线程偶尔由于缺页故障或者其他原因而暂停时，这个“额外”的线程也能确保 CPU 的时钟周期不被浪费）。对于包含I/O操作或其它阻塞操作的任务，由于线程并不会一直执行，因此线程池的规模应该更大。要正确的设置线程池的大小，必须估算出任务的等待时间与计算时间的比值。书中还给了我们另一个计算线程数的公式。\n假设：\n$$N_{cpu} = number\\ of\\ CPUs$$ $$U_{cpu} = target\\ CPU\\ utilization, 0\\ \\le \\ U_{cpu} \\le \\ 1$$ $$\\frac{W}{C} = ratio\\ of\\ wait\\ time\\ to\\ compute\\ time$$\n若要使处理器达到期望的使用率，线程池的最有大小等于：\n$$N_{threads} = N_{cpu} * U_{cpu} * (1 + \\frac{W})$$\n创建线程池 我们可以使用 ThreadPoolExecutor 来创建一个线程池：\npublic ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } ThreadPoolExecutor 的构造函数包含7个核心参数：\n corePoolSize：线程池的常驻核心线程数量。如果设置过小，可能导致频繁的创建和销毁线程，如果设置过大，又会造成系统资源的浪费，开发者应该根据实际业务场景来调整此值。 maximumPoolSize：池内所允许的最大线程数量。 keepAliveTime：当池内线程数量大于 corePoolSize 或当allowCoreThreadTimeOut 设置为 true 时，空闲线程在被销毁前的最大存活时间。 unit：指定 keepAliveTime 参数的单位。 workQueue：线程池执行的任务队列。当线程池中所有的线程都在处理任务时，新到来的任务就会在 workQueue 中排队等待执行。常用的任务队列有：有界队列 ArrayBlockingQueue、无界队列 LinkedBlockingQueue 和同步队列 SynchronousQueue（内部没有缓冲区）。 threadFactory：执行器创建新线程时所使用的工厂，池中所有的线程都由它创建（通过 addWorker() 方法）。默认的线程工厂是 DefaultThreadFactory，它为线程指定了名字和优先级。我们可以通过实现 ThreadFactory 接口来实现更多自定义操作。 handler：用来执行线程池的拒绝（饱和）策略。当 workQueue 满了并且线程池不能再创建线程来执行新提交的任务时，就会对新任务执行拒绝策略。拒绝策略其实是一种限流保护机制。  线程池的基本大小（corePoolSize）、最大大小（maximumPoolSize）以及存活时间（keepAliveTime）等因素共同负责线程的创建和销毁。基本大小也是线程池的目标大小，即在没有任务执行时线程池的大小，并且只有在工作队列满了的情况下才会创建超出这个数量的线程。线程池的最大大小表示可以同时活动的线程数量的上限。如果某个线程的空闲时间超过了存活时间，那么将被标记为可回收的，当线程池的当前大小超过了基本大小时，这个线程将被终止。\n值得注意的是：在创建 TreadPoolExecutor 的初期，线程并不会立即启动，而是等到有任务提交时才会启动，除非调用 prestartCoreThreads()。\n通过 Executors 快速创建线程池 Java类库中的 Executors 提供了不少创建线程池的静态工厂方法：\n  newFixedThreadPool()。创建一个固定容量的线程池，每提交一个任务就创建一个线程，直到达到线程池的容量，这时线程池的规模将不再变化（在线程池关闭之前，若有线程因故终止，线程池将补充新的线程）。这种线程池一般适用于任务数量不均匀、对内存压力不敏感但对系统负载比较敏感的场景。\n  newCachedThreadPool()。创建一个可缓存的线程池，如果线程池的当前规模超过了处理需求时，就回收空闲线程，而当需求增加时，就添加新的线程。它的特点在于线程池的规模几乎可以无限增加（实际上最大可以达到 2^32 - 1），非常适合用来执行要求低延迟的短期任务。\n  newSingleTreadExecutor()。创建单个工作线程来执行任务，如果这个线程异常结束，则会创建另一个线程来替代。它能确保任务按照某种顺序串行执行（例如 FIFO、LIFO、优先级）。\n  newScheduledTreadPool()。创建固定容量的线程池，并且以延迟或者定时任务的方式来执行任务，具体的实现主要有 3 种：\nScheduledExecutorService service = Executors.newScheduledThreadPool(10); service.schedule(new Task(), 10, TimeUnit.SECONDS); service.scheduleAtFixedRate(new Task(), 10, 10, TimeUnit.SECONDS); service.scheduleWithFixedDelay(new Task(), 10, 10, TimeUnit.SECONDS);  schedule 比较简单，表示延迟指定时间后执行一次任务，以上即 10 秒后执行一次任务就结束。 scheduleAtFixedRate 表示以固定频率执行任务，以上即表示第一次延迟 10 秒后每隔 10 秒执行一次任务。 scheduleWithFixedDelay 和第二种类似，区别在于对周期的定义。sheculeAtFixedRate 以任务开始的时间为起点计时，时间到就执行下一次任务，而不管任务执行要多久。而 scheduleWithFixedDelay 以任务结束时间为下一次循环的时间七点开始计时。    newSingleThreadScheduledExecutor()。与 newScheduledTreadPool() 非常相似，它只是 ScheduledThreadPool 的一个特例，内部只有一个线程。\n  newWorkStealingPool() 用于创建 ForkJoinPool。ForkJoinPool 也是线程池，但它与 ThreadPoolExecutor 有着很大不同。它非常适合用来执行可以产生子任务的任务（尤其是任务执行时长不均匀的场景），整个过程主要涉及两个步骤：首先是拆分任务（Fork）为子任务，然后是汇总（Join）子任务的结果得到任务的结果。此外，ForkJoinPool 的内部结构也与 ThreadPoolExecutor 大不相同：在 ForkJoinPool 中，每个线程都有自己独立的任务队列（是一个 Deque，用于存储分裂出来的子任务），而 ThreadPoolExecutor 中所有线程共用一个队列。\n  阿里巴巴的《Java开发手册》中为了规避资源耗尽的风险，禁止使用 Executors 类去创建线程池。这是因为 Executors 创建出来的线程池有一些弊端：\n newFixedThreadPool() 和 newSingleThreadPool() 创建出来的线程池允许的请求队列（workQueue）长度为 Integer.MAX_VALUE。如果线程池中任务的处理比较慢，那么随着请求的增多，队列中可能堆积大量的任务，进而占用大量内存，导致 OOM。 newCachedThreadPool() 和 newScheduledThreadPool() 创建出来的线程池允许的线程的最大数量（maximumPoolSize）为 Inteter.MAX_VALUE，当任务数量特别多时，就可能导致大量的线程被创建，最终因超过操作系统的上限而无法再创建新线程，或者引发 OOM。  除了 ForkJoinPool 另外五种创建线程池的方法都可以认为是利用 ThreadPoolExecutor 来实现的，区别就在于传入的参数不同而已。\n   线程池 corePoolSize maximumPoolSize keepAliveTime workQueue\u0026mdash;\u0026mdash;\u0026ndash;     newFixedThreadPool 构造函数传入 同 corePoolSize 0 LinkedBlockingQueue   newCachedThreadPool 0 Integer.MAX_VALUE 60s SynchronousQueue   newSingleTreadExecutor 1 1 0 LinkedBlockingQueue   newScheduledTreadPool 构造函数传入 Integer.MAX_VALUE 0 DelayedWorkQueue   newSingleThreadScheduledExecutor 1 Integer.MAX_VALUE 0 DelayedWorkQueue    关闭线程池 我们可以通过调用线程池的 shutdown() 或 shutdownNow() 方法来关闭线程池。它们的原理一样：都是通过遍历线程池中的工作线程，然后调用线程的 interrup() 方法来中断线程，所以无法响应中断的任务可能永远无法终止。但它们之间存在一定的区别： shutdown() 方法将执行 平缓 的关闭过程：不再接受新任务，同时等待已经提交的任务执行完成（包括已提交但还未执行的任务）。shutdownNow() 方法将执行粗暴的关闭过程：尝试取消所有运行中的任务，并且不再启动队列中尚未开始执行的任务。等所有任务都完成后，线程池就转入 Terminated(已终止) 状态。\nJava线程池的复用原理 ThreadPoolExecutor 通过 execute(Runnable command) 方法来执行一个线程：\npublic void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) reject(command); } 当提交一个新任务到线程池时，线程池的处理流程如下：\n 如果线程池中正在执行的线程数少于线程池的基本大小（corePoolSize），则创建一个新的线程来执行任务。否则，进入下一步。 尝试向往工作队列里加入一个线程，如果工作队列满了，进入下一步。 尝试创建一个新线程来执行任务。如果失败，则采用给定的饱和策略来处理这个任务。  execute() 方法内部调用了 addWorker(Runnable firstTask, boolean core) 方法，它的参数说明如下：\n firstTask：线程应首先运行的任务，如果没有则可以设置为 null； core：判断是否可以创建线程数量的最大值，如果等于 true 则表示使用 corePoolSize 作为上限值创建 worker，false 则表示使用 maximumPoolSize 作为上限创建 worker。  拒绝策略 当线程池无法接收新的任务时，它会按照我们提供的拒绝策略来拒绝任务。通常有两种情况：\n 线程池内的工作已经饱和（即任务队列满了，运行中的线程数也达到了 maximumPoolSize），线程池没有能力继续处理新提交的任务 线程池被关闭（比如调用 shutdown() 方法）。即便此时线程池内部依然没有执行完成的任务正在执行，但线程池也不会接收任何新的任务  使用者可以通过调用 ThreadPoolExecutor 的 setRejectedExecutionHandler(RejectedExecutionHandler handler) 来修改其饱和策略。JDK 自带了几种不同的 RejectedExecutionHandler 实现，每种实现都对应着有不同的饱和策略：\n AbortPolicy：终止策略。这是默认的策略，该策略直接会抛出 RejectedExecutionException。调用者可以将其捕获，然后根据实际需求编写处理代码。 DiscardPolicy：丢弃策略。新提交的任务被直接丢弃，调用者不会收到任何通知。这是有一定风险的，可能造成数据丢失。 DiscardOldestPolicy：丢弃最老的任务。丢弃下一个将被执行的任务，然后尝试重新提交新的任务。这也存在数据丢失的风险。 CallerRunsPolicy：由调用者执行。不丢弃任务，也不抛出异常，而是将任务退回给调用者。它不会在线程池中的某个线程中执行新提交的任务，而是在调用了execute(Runnable command)方法的线程（调用者线程）中继续执行该任务。这么做的好处是任务不会被丢弃，但是会减缓任务提交的速度，给了线程池一定的缓冲期。  参考资料  Brian Goetz, Tim Peierls, Joshua Bloch, Joseph Bowbeer, David Holmes, and Doug Lea. Java Concurrency in Practice. Addison-Wesley Professional, 2006. 方腾飞, 魏鹏, 程晓明. Java并发编程的艺术. 机械工业出版社, 2015.  ","href":"/java/concurrency/java_thread_pool/","title":"Java 线程池"},{"content":"进程（process）是资源分配的基本单元，而线程（thread）是程序执行的基本单元。一个进程可以包含多个线程，多个线程之间共享进程的资源。和进程相比，线程更加轻量化，所以线程又叫轻量级进程。\n创建线程 网上好多资料都说有三种创建线程的方式：\n 继承Thread类，重写run()方法。 实现Runnable接口。 实现Callable接口。  但本质上只有一种创建线程的方式，那就是构造一个 Thread 类。为什么这么说呢？我们来看上面几种方式的实际使用情况：\n继承 Thread 类 public class ExtendsThread extends Thread { @Override public void run() { System.out.println(\u0026#34;通过继承 Thread 类来实现线程\u0026#34;); } } 通过继承 Thread 类来实现线程最大的缺点就是代码未来的可扩展性被限制。Java 是不支持多继承的，一旦我们的类继承了 Thread 类，那么它就不能再继承其它类了。\n实现 Runnable 接口 class RunnableThread implements Runnable { @Override public void run() { System.out.println(\u0026#34;通过实现 Runnable 接口来实现线程\u0026#34;); } Thread 本身也实现了 Runnable 接口。\n实现 Callable 接口 class CallableTask implements Callable\u0026lt;Long\u0026gt; { @Override public Long call() throws Exception { System.out.println(\u0026#34;通过实现 Callable 接口来实现线程\u0026#34;); return System.currentTimeMillis(); } } Executors.newSingleThreadExecutor().submit(new CallableTask()); 实现 Callable 接口与实现 Runnable 接口最大的区别在于返回值：前者有返回值的，而后者无返回值。\n The Callable interface is similar to Runnable, in that both are designed for classes whose instances are potentially executed by another thread. A Runnable, however, does not return a result and cannot throw a checked exception.\n 但是，不管是 Runnable 还是 Callable，它们都表示 被线程执行的任务，它们本身并不是线程。接口的不同实现只是意味着交给线程执行的内容不同而已。\n使用线程池 我们可能还会想到线程池，使用线程池的时候，貌似不需要我们手动创建线程。那么线程池中的线程是如何创建的呢？实际上，线程池中的线程本质上是通过 ThreadFactory 创建的，不过最终的线程还是通过 new Thread() 创建出来的。我们可以来看一下 ThreadFactory 的默认实现 DefaultThreadFactory 的源码：\nprivate static class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() { SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = \u0026#34;pool-\u0026#34; + poolNumber.getAndIncrement() + \u0026#34;-thread-\u0026#34;; } public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; } } 线程的启动 线程的启动很简单，调用 Thread 类的 start() 方法即可。start() 方法最终会调用 Runnable 的 run() 方法来执行任务。\n@Override public void run() { if (target != null) { target.run(); } } 那么，start() 与 run() 有何不同呢？首先，run() 方法是通过实现 Runnable 接口得来的，而 start() 是 Thread 类自身的一个方法，用于启动当前线程：\n/** * Causes this thread to begin execution; the Java Virtual Machine * calls the {@code run} method of this thread. * \u0026lt;p\u0026gt; * The result is that two threads are running concurrently: the * current thread (which returns from the call to the * {@code start} method) and the other thread (which executes its * {@code run} method). * \u0026lt;p\u0026gt; * It is never legal to start a thread more than once. * In particular, a thread may not be restarted once it has completed * execution. */ public synchronized void start() { /** * This method is not invoked for the main method thread or \u0026#34;system\u0026#34; * group threads created/set up by the VM. Any new functionality added * to this method in the future may have to also be added to the VM. * * A zero status value corresponds to state \u0026#34;NEW\u0026#34;. */ if (threadStatus != 0) throw new IllegalThreadStateException(); /* Notify the group that this thread is about to be started * so that it can be added to the group\u0026#39;s list of threads * and the group\u0026#39;s unstarted count can be decremented. */ group.add(this); boolean started = false; try { start0(); started = true; } finally { try { if (!started) { group.threadStartFailed(this); } } catch (Throwable ignore) { /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ } } } 另外，start() 方法通过 synchronized 来保证线程安全，并且只能调用一次，还会影响线程的状态（由NEW变为RUNNABLE），而 run() 只是一个普通方法，可以被多次调用，并且不会改变线程的状态。\n停止线程 通常，我们不会手动停止一个线程，而是让线程运行到结束，自然停止。但是有许多特殊的情况需要我们提前停止线程，比如：用户突然关闭程序，或程序运行出错重启等。\n被弃用的停止方法 Java 在 Thread 类中提供了 stop() 方法，用于强行停止当前线程。但这个方法自 JDK1.2 开始就被标记为 @Deprecated，一同被标记的还有 suspend() 和 resume() 方法。那么，为何 JDK 要弃用这些方法呢？因为 stop() 会直接停止当前线程，线程就没有足够的时间来处理停止前需要完成的工作，可能会导致数据的完整性等问题。 suspend() 和 resume() 的问题则在于：调用 suspend() 的线程不释放锁就直接进入休眠，这可能导致死锁，因为如果线程在休眠期间持有锁的话，这把锁在线程被 resume() 之前是不会被释放的。\n来看一个具体的例子：假设线程 A 调用 suspend() 让线程 B 挂起，线程 B 进入休眠，而线程 B 刚好持有锁 L。假设此时线程 A 想要访问线程 B 持有的锁 L，由于线程 B 没释放锁 L 就休眠了，所以线程 A 是拿不到锁 L 的，它就会陷入阻塞。这样一来，线程 A 和线程 B 都无法继续向下执行。\nJava SE 的 API 文档里面其实也给出了这三个方法被弃用的原因（Why are Thread.stop, Thread.suspend and Thread.resume Deprecated?），并给出了相关的替代方案。\n使用状态标记 对于大多数情况，JDK 建议我们使用一个状态变量来指示目标线程是否应该停止运行。目标线程周期性地检查这个状态变量的值，当状态变量暗示要停止目标线程时，就从 run() 方法返回。为了确保停止请求的及时传播，这个状态变量应该被 volatile 修饰，或者对该变量的访问进行同步处理。\nprivate volatile Thread blinker; public void stop() { blinker = null; } public void run() { Thread thisThread = Thread.currentThread(); while (blinker == thisThread) { try { Thread.sleep(interval); } catch (InterruptedException e){ } repaint(); } } 然而，并不是仅仅使用 volatile 标记状态变量就百分百没问题了，我们还得倍加小心。下面是网上的一个例子：\npublic class VolatileCannotStop { // 生产者  static class Producer implements Runnable { public volatile boolean canceled = false; private BlockingQueue\u0026lt;Integer\u0026gt; storage; public Producer(BlockingQueue\u0026lt;Integer\u0026gt; storage) { this.storage = storage; } @Override public void run() { int num = 0; try { while (num \u0026lt;= 100000 \u0026amp;\u0026amp; !canceled) { if (num % 50 == 0) { storage.put(num); // 阻塞操作  System.out.println(num + \u0026#34;是50的倍数，放入仓库\u0026#34;); } num++; } } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(\u0026#34;生产者结束运行\u0026#34;); } } } // 消费者  static class Consumer { BlockingQueue storage; public Consumer(BlockingQueue storage) { this.storage = storage; } public boolean needMoreNums() { return Math.random() \u0026gt; 0.97; } } public static void main(String[] args) throws InterruptedException { BlockingQueue storage = new ArrayBlockingQueue(8); Producer producer = new Producer(storage); new Thread(producer).start(); Thread.sleep(500); Consumer consumer = new Consumer(storage); while (consumer.needMoreNums()) { System.out.println(consumer.storage.take() + \u0026#34;被消费了\u0026#34;); Thread.sleep(100); // 这段时间内消费者又可以将 storage 塞满，然后阻塞  } System.out.println(\u0026#34;消费者不需要更多数据了\u0026#34;); // 一旦消费者不需要更多数据了，我们就应该让生产者停下来，然而实际情况却是生产者停不下类  producer.canceled = true; System.out.println(producer.canceled); } } 直接看 main() 方法，首先创建了生产者/消费者共用的仓库 storage，仓库容量是 8，然后创建生产者并启动生产者线程，紧接着主线程进行 500 毫秒的休眠，保障生产者有足够的时间把仓库塞满，仓库被塞满后生产者就会阻塞，500 毫秒后消费者也被创建出来，并判断是否需要使用更多的数字，然后每次消费后休眠 100 毫秒，这样的业务逻辑是有可能出现在实际生产中的。\n当消费者不再需要数据，就会将 canceled 的标记位设置为 true，理论上此时生产者会跳出 while 循环，并打印输出生产者运行结束。然而结果并不是我们想象的那样，尽管已经把 canceled 设置成 true，但生产者仍然没有停止，这是因为在这种情况下，生产者在执行 storage.put(num) 时会被阻塞，在它被叫醒之前是没有办法进入下一次循环判断 canceled 的值的，所以在这种情况下用 volatile 是没有办法让生产者停下来的。在这种情况下，应该使用 interrupt() 来中断线程。即使生产者处于阻塞状态，它仍然能够感受到中断信号，并作出响应。\n使用 interrupt 在 Java 中，停止线程的正确姿势是使用 interrupt()。但 interrupt() 仅仅起到通知被停止线程的作用。而对于被停止的线程而言，它拥有完全的自主权，可以选择立即停止，也可以选择一段时间后停止，甚至可以选择不停止。不采用强制停止的做法的原因是：贸然强行停止线程可能造成一些安全问题，若要避免这些问题，就需要给目标线程一些时间进行收尾工作。\nwhile (!Thread.currentThread().isInterrupted() \u0026amp;\u0026amp; (has more work to do)) { do more work } 调用某个线程的 interrupt() 方法之后，这个线程的中断标记位就会被设置成 true。每个线程都有这样的标记位，当线程执行时，应该定期检查这个标记位，如果标记位被设置成 true，就说明有程序想终止该线程。下面是一个具体的例子：\npublic class StopThread { public static void main(String[] args) throws InterruptedException { Thread t = new Thread(new Runnable() { @Override public void run() { int count = 0; while (!Thread.currentThread().isInterrupted() \u0026amp;\u0026amp; count \u0026lt; 1000) { System.out.println(\u0026#34;count = \u0026#34; + (count++)); } } }); t.start(); Thread.sleep(5); t.interrupt(); } } 程序还没打印完 1000 个数就会停下来，因为对应的线程被 interrupt 了。\n休眠中的线程是可以感受到中断信号的，被中断的线程会抛出一个 InterruptedException，同时清除中断信号，将中断标记位设置成 false。调用方可以捕获这个异常，进行处理，还可以再次中断线程。再次中断后，后续执行的方法依然可以检测到调用过程发生过中断，可以做出相应的处理，整个线程可以正常退出。\n线程的状态 Java线程在其生命周期中可能处于6种状态（这6种状态指的线程在 JVM 中的状态，并不是操作系统中线程的状态），同一时刻，线程只能处于其中的一个状态。\n   状态 说明     NEW A thread that has not yet started is in this state.   RUNNABLE A thread executing in the Java virtual machine is in this state.   BLOCKED A thread that is blocked waiting for a monitor lock is in this state.   WAITING A thread that is waiting indefinitely for another thread to perform a particular action is in this state.   TIMED_WAITING A thread that is waiting for another thread to perform an action for up to a specified waiting time is in this state.   TERMINATED A thread that has exited is in this state.    以下是线程之间的状态迁移图：\nwait() 与 notify() 若 synchronized 关键字修饰的某个共享资源 R 的锁已经被线程 T1 获得，其它需要资源 R 的锁才能运行的线程就会被阻塞直至 T1 释放 R 上的锁。一般情况下，这个锁是在同步代码块执行完才被释放的。\n但是在同步代码块执行期间，已持有锁的线程 T1 可以调用资源 R 的 wait() 方法释放锁，然后进入等待状态，当前线程被挂起。锁的持有者可以调用 notify() 方法随机唤醒一个处于等待状态的线程，或 notifyAll() 方法去唤醒所有处于等待状态的线程。\n只有持有与共享资源 R 相关联的 monitor 的锁的线程才应该去调用 notify() 或 notifyAll() 方法。一个线程可以通过3种方式获得与资源 R 相关联的 monitor 的锁，这三种方式刚好对应于同步代码块的三种形式。因为只有持有 monitor 的锁，才能进入同步代码块。\nsleep() 与 wait() sleep() 与 wait() 都可以暂停线程执行，但它们还有一些区别：\n 原理不同。sleep() 是 Thread 类的静态方法，是线程自己用来控制自身执行的，它可以使线程自己暂停一段时间，把执行的机会让给其它线程，等睡眠时间一到，便会自动“醒来”。而 wait() 是 Object 类的方法，用于线程间的通信，这个方法会使当前持有对象锁的线程释放锁并进入等待状态，直至其它线程调用对象的 notify() 或 notifyAll() 方法才会醒来。wait() 方法也支持设置超时时间。 对锁的处理机制不同。调用 sleep() 方法只是让当前线程暂停一段时间，不涉及线程间通信，也不会释放锁。而调用 wait() 方法会释放锁。 使用区域不同。sleep() 方法可以在任何区域使用。而由于调用 wait() 方法前必须先获得对象锁，因此只能在同步代码块内使用。 对异常的处理方式不同。调用 sleep() 方法时必须处理 InterruptedException 异常。而调用 wait() 方法时不用关心异常处理。  sleep() 与 yield() sleep()方法与yield()方法都属于Thread类，它们的区别主要体现在：\n 前者被调用后会立马进入阻塞状态，在一段时间内不会再执行。而后者只是使当前线程重新回到RUNNABLE状态，因此可能马上又被执行。 前者在方法声明上抛出了InterruptedException异常，而后者没有声明任何异常。  join() JDK中对join()方法的解释为：Waits for this thread to die.\n实际上，当线程A调用了线程B的join()方法后，线程A会让出CPU的执行权给线程B。直到线程B执行完成或者过了超时时间，线程A才会继续执行。\n多线程同步 同步的实现 Java 提供了多种多线程同步的方法：\n 使用 synchronized 关键字。见Synchronized关键字。 使用 wait() 与 notify() 方法。 使用 Lock。见Locks。  线程优先级 Thread 类定义了三个和线程优先级有关的属性：\n/** * The minimum priority that a thread can have. */ public static final int MIN_PRIORITY = 1; /** * The default priority that is assigned to a thread. */ public static final int NORM_PRIORITY = 5; /** * The maximum priority that a thread can have. */ public static final int MAX_PRIORITY = 10; 优先级越高的线程获得 CPU 时间片的概率越大，但这并不是意味着优先级越高的线程一定越先执行。在 Java 中，设置线程优先级方法是 setPriority(int newPriority)，有效的优先级范围为：1-10。\n参考资料  Brian Goetz, Tim Peierls, Joshua Bloch, Joseph Bowbeer, David Holmes, and Doug Lea. Java Concurrency in Practice. Addison-Wesley Professional, 2006. 方腾飞, 魏鹏, 程晓明. Java并发编程的艺术. 机械工业出版社, 2015. Java Thread Primitive Deprecation.  ","href":"/java/concurrency/java_thread/","title":"Java 线程"},{"content":"Java语言提供了多种线程间通信机制（同步、while轮询、等待/通知、管道等等），其中最基础的通信方式就是 同步（synchronization）。Java中的同步是通过monitor来实现的，每个Java对象都有一个与之相关联的monitor，线程可以在其上进行加锁和释放锁的操作。同一时刻只能有一个线程持有某个monitor的锁，任何其它尝试给该monitor加锁的线程在获得锁之前都会被阻塞。一个线程可以多次给某个monitor加锁，这就是锁的重入，多次加锁对应着多次解锁，因为每次unlock操作只会消除一次lock的效应。\nSynchronized 关键字 Java提供了一种内置的锁机制来支持原子性：同步代码块（Synchronized Block）。同步代码块就是用 synchronized 关键字修饰的代码块，它包括两个部分：作为锁的对象引用和由这个锁保护的代码块。\n用关键字 synchronized 修饰的方法是一种横跨整个方法体的同步代码块，其中该同步代码块的锁就是和该方法相关的对象（方法执行期间 this 关键字所代表的对象）。静态 synchronized 方法以 Class 对象为锁：\nstatic synchronized void staticMethod() { // do something } 以下是以 lock 对象为锁：\nsynchronized（lock）{ // access or modify the shared state that is protected by the lock } 而以下是以调用同步方法的实例本身为锁：\nsynchronized void instanceMethod() { // do somehing ... } 同步代码块是可重入的。\nJava对象头 The Java® Virtual Machine Specification (Java SE 11 Edition) 本身并没有规定程序运行时对象的具体内存布局，因此在不同的虚拟机实现中，对象和数组的内存布局可能有所不同。下面主要关注 HotSpot VM 这一 JVM 实现。\nHotSpot VM 使用了一种名为 OOPs(Ordinary Object Pointers) 的数据结构表示指向对象的指针。虚拟机使用 oopDesc 这个特殊的数据结构来描述所有的指针（包括对象和数组）。每个 oopDesc 都包含以下信息：\n mark word。这一类用于存储对象自身的运行时数据，比如 HashCode、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等，在运行期间，Mark Work 里面存储的数据会随着锁标志的变化而变化。 klass word。这一部分主要包含的是语言级别的类信息（比如类名、修饰符、父类信息等），即类型的元数据，Java 通过这个指针确定对象是哪个类的实例。该部分可能被压缩。  对于 Java 对象（用 instanceOop 表示）而言，对象头包括 mark word、klass word，可能还有对齐填充。\n对于 Java 数组（用 arrayOop 表示）而言，对象头包括 mark word、klass word、 sizeof(int) 大小（一般为 4 字节）的数组长度，可能还有对齐填充。数组的对象头中还必须有一块用于记录数组长度的数据，因为 JVM 可以通过普通 Java 对象的元数据信息确定 Java 对象的大小，但是数组的长度是不确定的。\n锁的状态级升级过程 锁一共有4种状态，级别从低到高分别是：无锁、偏向锁、轻量级锁和重量级锁。锁的状态只支持升级，不支持降级。\n以下是锁的四种状态下 markd word 的简要内容：\n   锁状态 存储内容 最后两位（锁标志）     无锁 hashCode、GC分代年龄、是否偏向锁（0） 01   偏向锁 偏向线程ID、偏向时间戳、GC分代年龄、是否偏向锁（1） 01   轻量级锁 指向栈中锁记录的指针 00   重量级锁 指向互斥量（重量级锁）的指针 10    mark word 的具体细节可以查看 OpenJDK 源码，32 位和 64 位模式下存储的内容是不一样的。\n无锁 不对资源进行锁定，所有的线程都能访问并修改共享资源，但每次只有一个线程能修改成功。\n在无锁状态下，线程会不断尝试修改共享资源。如果没有冲突，就修改成功并退出，否则 继续循环尝试直到修改成功。\n偏向锁 大多数情况下，锁不仅不存在竞争，而且总是被同一个线程多次获得。偏向锁就是指同一个同步代码块一直被一个线程访问，那么该线程就会自动能够获取锁，这能降低获取锁的代价。\n当一个线程访问同步代码块并获取锁时，会在对象头和栈帧的锁记录里面存储偏向的线程ID，以后该线程在进入和退出同步代码块时不需要进行CAS操作来加锁和解锁，只需要简单的检测一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁，否者则需要再检测 Mark Word 中偏向锁的标识是否为1（如果未设置，则使用 CAS 竞争锁，否则尝试使用 CAS 将对象头的偏向锁指向当前线程）。\n偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其它线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销需要等到全局安全点（这个时间点上没有正在执行的字节码）上才能进行。它会先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定的状态。撤销偏向锁后恢复到无锁或轻量级锁状态。\n由于难以维护，Java 15 已经准备弃用偏向锁了。\n轻量级锁 当偏向锁被其它线程访问时，偏向锁就会升级为轻量级锁，其它线程会通过自旋的方式尝试获取锁，不会阻塞。\n当线程进入同步代码块时，若同步对象的锁状态为无锁状态，JVM 会再当前线程的栈帧中建立一个名为 Lock Record 的空间，用于存储对象目前的 Mark Word 的副本，然后将对象头中的 Mark Word 复制进去。复制成功后，JVM 会使用 CAS 操作尝试将对象的 Mark Word 更新为指向 Lock Record 的指针，并将 Lock Record 里的 owner 指针指向对象的 Mark Word。如果更新成功，该线程就获得了锁，对象头中的 Mark Word 会被设置为 00，表示此对象处于轻量级锁状态。若更新失败，JVM 会检查对象头的 Mark Word 是否指向当前线程的栈帧，如果是就说明当前线程已经拥有锁，就直接进入同步代码块继续执行，否则说明有多个线程在竞争该锁。\n若当前只有一个等待线程，则该等待线程会以自旋的方式进行等待。当自旋超过一定次数，或者一个线程持有锁，一个线程在自旋，第三个线程又来访问时，轻量级锁会升级为重量级锁。\n重量级锁 当锁升级为重量级锁时，Mark Word 存储的时指向重量级锁的指针，等待的线程会进入阻塞状态。\n锁升级小结 偏向锁通过比对 Mark Word 来解决加锁问题，避免 CAS 操作；轻量级锁通过 CAS 操作和自旋解决加锁问题，避免线程阻塞和唤醒带来的性能损耗；重量级锁则阻塞拥有锁的线程以外的所有线程。\n从JVM视角看同步 JVM中的同步操作是通过 monitor 的进入与退出来实现的。其实现方式又可分为显式（通过使用 monitorenter 和 monitorexit 指令）和隐式（通过方法的调用与返回指令）两种。根据被修饰的代码块的形式不同，synchronized 关键字的使用可以分为两种情况：同步方法（synchronized 修饰的是静态方法或实例方法）和同步语句（synchronized 修饰的是一个代码块）。\n同步方法 方法级别的同步操作是隐式实现的，不涉及 monitorenter 和 monitorexit 指令的使用，同步就是方法调用与返回的一部分。在运行时常量池中的 method_info 结构中，有一个叫做 ACC_SYNCHRONIZED 的标志位。方法调用指令会去检查这个标志位，当一个设置了 ACC_SYNCHRONIZED 的方法被调用时，执行线程会先进入 monitor，然后调用方法本身，最后不管方法是否正常执行完成都会退出 monitor。在执行线程拥有 monitor 的时间段内，其它线程是不可能再进入该 monitor 的。\n下面的 SyncMethodDemo 中包含两个同步方法，其中 staticMethod 是作用于类的静态同步方法，instanceMethod 是作用于对象实例的同步方法：\npublic class SyncMethodDemo { static synchronized void staticMethod() { System.out.println(\u0026#34;static synchronized method\u0026#34;); } synchronized void instanceMethod() { System.out.println(\u0026#34;synchronized instance method\u0026#34;); } } 使用 javap -v -c SyncMethodDemo.class 得到两个方法的字节码如下：\nstatic synchronized void staticMethod(); descriptor: ()V flags: (0x0028) ACC_STATIC, ACC_SYNCHRONIZED Code: stack=2, locals=0, args_size=0 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #3 // String static synchronized method 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 3: 0 line 4: 8 synchronized void instanceMethod(); descriptor: ()V flags: (0x0020) ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #5 // String synchronized instance method 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 7: 0 line 8: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this LSyncMethodDemo; 可以看到，在字节码中，方法加了一个 ACC_SYNCHRONIZED 标志。静态同步方法 staticMethod 和 instanceMethod 区别不大，只不过是多了一个 ACC_STATIC 标志。\n同步代码块 下面是 JVMS11 中给出的一个例子。同步代码块\nvoid onlyMe(Foo f) { synchronized(f) { doSomething(); } } 会被编译成以下字节码：\nMethod void onlyMe(Foo) 0 aload_1 // Push f 1 dup // Duplicate it on the stack 2 astore_2 // Store duplicate in local variable 2 3 monitorenter // Enter the monitor associated with f 4 aload_0 // Holding the monitor, pass this and... 5 invokevirtual #5 // ...call Example.doSomething()V 8 aload_2 // Push local variable 2 (f) 9 monitorexit // Exit the monitor associated with f 10 goto 18 // Complete the method normally 13 astore_3 // In case of any throw, end up here 14 aload_2 // Push local variable 2 (f) 15 monitorexit // Be sure to exit the monitor! 16 aload_3 // Push thrown value... 17 athrow // ...and rethrow value to the invoker 18 return // Return in the normal case Exception table: From To Target Type 4 10 13 any 13 16 13 any 不管方法正常退出还是异常退出，编译器都会确保每条 monitorenter 指令都有一条对应的 monitorexit 被执行。\n参考资料  方腾飞, 魏鹏, 程晓明. Java并发编程的艺术. 机械工业出版社, 2015. 美团技术团队. 不可不说的Java“锁”事. Intrinsic Locks and Synchronization. The Java® Virtual Machine Specification (Java SE 11 Edition). Memory Layout of Objects in Java.  ","href":"/java/concurrency/synchronization/","title":"Java 中的 synchronized"},{"content":" 文章中涉及的源代码摘自 OpenJdk 11。\n 乐观锁与悲观锁 乐观锁与悲观锁是一种广义上的概念，体现了我们看待线程同步的不同角度。\n乐观锁 乐观锁采用的思想是：冲突检测。例如：如果 A 和 B 同时编辑同一份文件，使用乐观锁策略， A 和 B 都能得到文件的一份拷贝并可以自由的编辑。假设 A 先完成工作，那么他可以毫无困难的更新他的修改。而当 B 在 A 提交之后完成工作并向提交他的修改时，并发控制策略将会起作用，如果检测到冲突，B 的提交将会被拒绝。\n乐观锁认为锁的持有者在使用数据的过程中不会有别的线程修改数据，所以不会添加锁，只有在提交数据时才会进行检测。Java 中的乐观锁大部分是通过 CAS (Compare And Swap) 实现的。CAS 是一种无锁算法，用来将某一内存地址的值从一个状态更新到另一个状态。CAS 操作包含三个参数：内存地址、预期原值和新值。如果内存地址的值和预期的原值相等的话，那么就可以把该位置的值更新为新值，否则不做任何修改。例如， AtomicInteger 类中的 compareAndSet() 方法：\npublic final boolean compareAndSet(int expectedValue, int newValue) { return U.compareAndSetInt(this, VALUE, expectedValue, newValue); } CAS 虽然高效，但是它存在一个问题：在其调用期间，目标内存地址的值改变了数次，但该地址的当前值却有可能与调用者之前获取到的值相等，这在某些情况下是有问题的。也就是说，CAS 无法探测到“某个值被修改，然后再被改回原值”的情况，这就是所谓的 ABA 问题。ABA 问题的常见处理方式是添加版本号，线程每次修改值之后就更新版本号，JDK 中的 AtomicStampedReference 类就是一个典型的例子，它内部维护了一个“版本号” Stamp，每次在比较时既比较当前值又比较版本号，这样就解决了 ABA 问题。\npublic class AtomicStampedReference\u0026lt;V\u0026gt; { private static class Pair\u0026lt;T\u0026gt; { final T reference; final int stamp; private Pair(T reference, int stamp) { this.reference = reference; this.stamp = stamp; } static \u0026lt;T\u0026gt; Pair\u0026lt;T\u0026gt; of(T reference, int stamp) { return new Pair\u0026lt;T\u0026gt;(reference, stamp); } } public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { Pair\u0026lt;V\u0026gt; current = pair; return expectedReference == current.reference \u0026amp;\u0026amp; expectedStamp == current.stamp \u0026amp;\u0026amp; ((newReference == current.reference \u0026amp;\u0026amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp))); } } 乐观锁适用于读多写少的场景，也适用于读写都多但并发冲突不激烈的场景。在冲突较少的情况下，乐观锁能让性能大幅提高。\n悲观锁 悲观锁采用的思想是：冲突避免。例如：如果 A 和 B 都想编辑同一份文件，使用悲观锁策略，若 A 先获得文件的编辑权，那么 B 就不能对文件进行编辑了。只有当 A 编辑完成并提交之后，B 才能对该文件进行操作。\n悲观锁是比较保守的，它认为在自己使用数据时会有别的线程来修改数据，因此在获取数据时会先加锁，确保数据不会被其它线程修改。Java 中，synchronized 和 Lock 的实现类采用的都是悲观锁策略。\n悲观锁适用于并发写入多、竞争激烈的场景。在这些场景下，使用悲观锁可以避免大量无用的反复尝试带来的消耗。\nLock接口 Lock 接口中定义了一组抽象的锁操作。与内置加锁机制不同的是，Lock 提供了一种无条件的、可轮询的、定时的以及可中断的锁获取操作，所有加锁和解锁的方法都是显式的。Lock 接口的定义如下：\npublic interface Lock { void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition(); } ReentrantLock 实现了 Lock 接口，并提供了与 synchronized 相同的互斥性和内存可见性。获取 ReentrantLock 的语义与进入同步代码块一致，释放 RenentrantLock 的语义也同退出同步代码块一致。与 synchronized 一样， ReentrantLock 也提供了可重入的加锁语义。但 ReentrantLock 比 synchronized 更加灵活，最典型的就是前者支持非公平和公平的锁获取机制，而后者只支持公平的锁获取机制。\n以下是 Lock 接口的标准使用形式，它比内置锁更复杂，并且必须在 finally 块中释放锁。否则，如果在被保护的代码中抛出了异常，这个锁将无法释放。\nLock lock = ...; lock.lock(); try { // access the resource protected by this lock } finally { lock.unlock(); } synchronized vs Lock 相同点 synchronized 与 Lock 有很多相同之处，其中有三点比较明显：\n 二者都是用来保证对共享资源的并发访问是线程安全的。 二者都可以保证可见性。即线程 A 在进入 synchronized 代码块之前或在 synchronized 代码块内进行的操作，对于后续获得同一 monitor 锁的线程 B 来说是可见的；线程 A 在调用 unlock() 之前的所有操作对后续在同一个锁上调用 lock() 的线程 B 来说是可见的。 二者都是可重入的。  不同点  前者是 JVM 隐式实现的，而后者是 Java 语言提供的 API。 前者的锁只能是非公平的，而后者的一些实现可以允许锁是公平的。 synchronized 使用的是 Object 对象的 wait()、notify()、notifyAll() 调度机制，而 Lock 使用的是 Condition 接口的 await()、signal()、signalAll() 完成线程之间的调度。 synchronized 的作用限于同步代码块（包括方法），而 Lock 需要手动给出锁的起始和结束位置。前者的执行是托管给 JVM 的，后者是通过代码手动控制的。 前者会自动释放锁，而后者需要手动释放锁。 前者只能以阻塞的方式获得锁，后者可以使用非阻塞的方式去获得锁。  \u0026hellip;\nAbstractQueuedSynchronizer(AQS) Lock 接口的实现基本都是通过聚合一个 AbstractQueuedSynchronizer 的子类来完成线程访问控制的。\n**AQS**是一个用于构建锁和同步器的框架，ReentrantLock、Semaphore、CountDownLatch、ReentrantReadWriteLock、SynchronousQueue等都是基于它实现的。\n在基于AQS构建的同步器类中，最基本的操作包括各种形式的获取操作和释放操作。获取操作是一种依赖状态的操作，通常也会阻塞。相反，释放操作并不是一个阻塞操作，当执行释放操作时，所有在请求时被阻塞的线程都会开始执行。\nAQS负责管理同步器类中的状态，它用一个int类型的字段 state 来保存状态信息，可以通过getState()、setState()以及CompareAndSetState()等方法来进行操作。state可以表示任意状态。例如：ReentrantLock用它来表示所有者线程已经重复获取该锁的次数；Semaphore用它表示剩余的许可数量；CountDownLatch用它来表示计数器当前的值；ReentrantReadWriteLock用state的高16位表示读状态，也就是获取读锁的次数，低16位表示获取写锁的次数。对于AQS来说，线程同步的关键就是对状态值state进行操作。\nAQS是一个FIFO的双向队列，内部通过节点head和tail记录队首和队尾元素，队列元素的类型为Node。\nAQS的实现分析 同步队列 AQS依赖内部的同步队列（一个FIFO的双向队列）来完成同步状态的管理，当前线程获取同步状态失败时，AQS会将当前线程以及等待状态等信息构成一个节点（Node）并将其加入同步队列，同时阻塞当前线程，当同步状态释放时，会把队首节点中的线程唤醒，使其再次尝试获取同步状态。\nNode节点 Node节点是构成同步队列的基础，AQS拥有首节点（head）和尾节点（tail）。head节点是获取同步状态成功的节点，它在释放同步状态时，会唤醒后继节点，而被唤醒的后继节点会在获取同步状态成功时将自己设置为head节点。\nNode中的thread变量用来记录存放进AQS队列里面的线程。Node节点内部的SHARED用来标记该线程在共享模式阻塞，EXCLUSIVE用来标记线程在互斥模式阻塞。waitStatus记录当前节点的等待状态：\n CANCELLED：由于在同步队列中等待的线程等待超时或者被中断，需要从同步队列中取消等待，节点进入该状态后将不在变化 SIGNAL：后继节点的线程处于等待状态，而当前节点的线程如果释放了同步状态或者被取消，将会通知后继节点，使后继节点的线程得以运行 CONDITION：节点在等待队列中，节点线程等待在Condition上，当其它线程调用了Condition的signal()方法后，该节点将会从等待队列移动到同步队列中 PROPAGATE：表示下一次共享式同步状态获取将会无条件地被传播下去 0：不处于以上任何一个状态 prev表示当前节点的前驱节点（当节点从队尾加入同步队列时被设置），next表示当前节点的后继节点。  独占式同步状态的获取与释放 获取锁 通过调用AQS的acquire(int arg)方法，可以以互斥的形式获取同步状态并忽略中断。acquire(int arg)方法的代码如下：\npublic final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 这段代码主要完成了获取同步状态、构造节点、加入同步队列以及在同步队列中自旋等待的相关操作。主要逻辑是：首先调用AQS子类自定义的tryAcquire(int arg)方法获取同步状态，若获取失败，则构造同步节点（Node.EXCLUSIVE）并通过addWaiter(Node node)方法将该节点加入到同步队列的队尾，最后调用acquireQueued(Node node, int arg)方法使该节点以自旋的方式获取同步状态（只有前驱节点为头节点时才能够尝试获取同步状态）。如果获取不到则尝试阻塞节点中的线程，被阻塞线程的唤醒主要依靠前驱节点的出队或阻塞线程被中断来实现。acquireQueued()方法源码如下：\nfinal boolean acquireQueued(final Node node, int arg) { boolean interrupted = false; try { for (;;) { final Node p = node.predecessor(); if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC  return interrupted; } // 判断获取锁失败后是否阻塞线程  if (shouldParkAfterFailedAcquire(p, node)) interrupted |= parkAndCheckInterrupt(); } } catch (Throwable t) { cancelAcquire(node); if (interrupted) selfInterrupt(); throw t; } } 当获取锁失败后，shouldParkAfterFailedAcquire() 方法会决定是否挂起当前线程，方法的实现如下：\nprivate static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws \u0026gt; 0) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus \u0026gt; 0); pred.next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don\u0026#39;t park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ pred.compareAndSetWaitStatus(ws, Node.SIGNAL); } return false; } 从上面的代码中可以看出，线程被挂起的前提条件是：前驱节点的状态为 SIGNAL，SIGNAL 状态的含义是后继节点处于等待状态，当前节点释放锁后将会唤醒后继节点。\n整个加锁的流程到这里就已经走完了，最后的情况是：没有拿到锁的线程会在队列中被挂起，直到拥有锁的线程释放锁之后，才会去唤醒其他的线程去获取锁资源。\n释放锁 释放同步状态则是通过release(int arg)方法来完成的：\npublic final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 这段代码执行时，会释放同步状态并唤醒后继节点（后继节点成为head节点，继而重新尝试获取同步状态）中的线程，具体唤醒等待线程时通过unparkSuccessor(Node node)方法来完成的，它调用了LockSupport的unpark(Thread thread)方法。\n共享式同步状态获取与释放 获取锁 共享式获取同步状态与独占式获取最主要的区别在于同一时刻是否能有多个线程同时获取到同步状态。通过调用AQS的acquireShared(int arg)方法可以以共享的方式获取同步状态，该方法的代码如下：\npublic final void acquireShared(int arg) { if (tryAcquireShared(arg) \u0026lt; 0) doAcquireShared(arg); } private void doAcquireShared(int arg) { final Node node = addWaiter(Node.SHARED); boolean interrupted = false; try { for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); if (r \u0026gt;= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC  return; } } if (shouldParkAfterFailedAcquire(p, node)) interrupted |= parkAndCheckInterrupt(); } } catch (Throwable t) { cancelAcquire(node); throw t; } finally { if (interrupted) selfInterrupt(); } } 在acquireShared(int arg)方法中，调用的tryAcquireShared(int arg)返回值若不小于0，则表示获取同步状态成功。若获取失败，就进入doAcquireShared(int arg)方法自旋，如果当前节点的前驱为head节点，再次尝试获取同步状态，获取成功便退出自旋，否则继续尝试。\n释放锁 同步状态的释放是通过releaseShared(int arg)方法来完成的：\npublic final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } private void doReleaseShared() { for (;;) { Node h = head; if (h != null \u0026amp;\u0026amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!h.compareAndSetWaitStatus(Node.SIGNAL, 0)) continue; // loop to recheck cases  unparkSuccessor(h); } else if (ws == 0 \u0026amp;\u0026amp; !h.compareAndSetWaitStatus(0, Node.PROPAGATE)) continue; // loop on failed CAS  } if (h == head) // loop if head changed  break; } } 不同于release(int arg)方法，releaseShared(int arg)方法会唤醒后继节点并确保释放操作向后传播。\nReentrantLock ReentrantLock允许一个线程对资源重复加锁，还支持设置锁的公平性。\nsynchronized关键字本是是支持重入的（隐式的），而ReentrantLock的重入需要显式进行，即调用lock()方法。\n重入 重入是指任意线程在获取锁之后能够再次获取该锁而不会被锁阻塞。这一特性的实现需要解决以下两个问题：\n 线程再次获取同一个锁。这个时候需要锁去识别线程是否为当前拥有锁的线程，如果是，则再次获取成功，完成重入。 锁的最终释放。需要保证线程重复获取n次锁并释放n次锁之后，其它线程能够获取到该锁。  重入的一种实现方法是：为每个锁关联一个计数值和一个所有者线程，当计数值为0时，这个锁就被认为不被任何线程持有。当线程请求一个未被持有的锁时，JVM会记下锁的持有者并将计数值设置为1，如果同一个线程再次获取这个锁，计数值就加1，当线程退出同步代码块时，计数值减1。当计数值为0时，这个锁将被释放。\n锁的公平性 在 ReentrantLock 的构造函数中提供了一个 fair 参数，用来指定所创建的锁是否公平，默认创建一个非公平的锁。在公平的锁上，线程将按照它们发出请求的顺序来获得锁，但在非公平的锁上，是允许“插队”的：当一个线程请求非公平的锁时，如果在发出请求的同时该锁的状态变为可用，那么这个线程将跳过队列中的所有等待线程并获得这个锁。\n在激烈竞争的情况下，非公平锁的性能高于公平锁的一个原因是：在恢复一个被挂起的线程与该线程真正开始运行之间存在着严重的延迟，而使用非公平锁减少了线程上下文切换的次数。虽然非公平锁带来了更高的性能，但可能导致线程饥饿。\nReentrantLock 默认采用非公平锁：\npublic ReentrantLock() { sync = new NonfairSync(); } 如果使用者希望使用基于公平锁的ReentrantLock，可以使用带参数的构造函数：\npublic ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } ReentrantLock 中的 lock() 实现为 sync.acquire(1)。acquire() 方法是 Sync 类从 AbstractQueuedSynchronizer 继承来的，上文中已经提到过其具体实现。tryAcquire() 方法尝试以独占的方式获取锁，如果获取锁失败，则把线程加入到阻塞队列中，下面来看 tryAcquire() 在 FariSync 和 NonfairSync 中的具体实现。FairSync 中的 tryAcquire() 方式实现为：\nprotected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() \u0026amp;\u0026amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc \u0026lt; 0) throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } return false; } 而 NonfairSync 中的实现则及其简单：\nprotected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } nonfairTryAcquire() 方法的实现位于抽象类 Sync 中，其具体内容为：\nfinal boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc \u0026lt; 0) // overflow  throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } return false; } 对比 FairSync 可以发现，FairSync 中的 tryAcquire() 方法仅多了一行：!hasQueuedPredecessors()。hasQueuedPredecessor() 方法用来查看队列中是否有比当前线程等待时间更久的线程，如果没有，就尝试一下是否能获取到锁，如果获取成功，则标记锁为已占用。\n这里有一个特例需要注意，tryLock() 方法不遵守我们设定的公平原则。因为它的源码是这样的：\npublic boolean tryLock() { return sync.nonfairTryAcquire(1); } 也就是说，即使我们设定锁为公平锁，如果在调用 tryLock() 时锁可用，就能立即获得锁，与前面是否有线程排队无关。\nReadWriteLock ReadWriteLock （读写锁）的接口定义如下：\npublic interface ReadWriteLock { Lock readLock(); Lock writeLock(); } ReadWriteLock 暴露了两个 Lock 对象，其中一个用于读操作，另一个用于写操作。要读取由 ReadWriteLock 保护的数据，必须先获得读锁，当需要修改 ReadWriteLock 保护的数据时，必须先获得写锁。\n在 ReadWriteLock 实现的加锁策略中，允许多个读操作同时进行（多个线程同时持有读锁），但每次只允许一个写操作（每次至多有一个线程持有写锁）。对于读多写少的场景，ReadWriteLock 能够提高性能。\n读写锁的获取规则如下：\n 如果一个线程持有读锁，那么其它线程可以继续申请读锁。 如果一个线程持有读锁，那么此时其它线程必须等待当前线程释放读锁之后才能申请写锁。 如果一个线程持有写锁，那么此时其它线程必须等待当前线程释放掉写锁之后才能申请读锁或写锁。  简而言之，读可以并发进行，读写不能同时进行，多个线程不能同时写。\nReentrantReadWriteLock ReentrantReadWriteLock 实现了 ReadWriteLock 接口，并加入了一些监控锁内部工作状态的方法。\nReentrantReadWriteLock 的实现依赖于 AQS，由于 AQS 内使用的是一个 int 类型的 state 变量来保存的同步状态，所以 ReentrantReadWriteLock 对 state 变量进行了按位切割，分两部分使用：高16位表示读状态，低16位表示写状态。\nReentrantReadWriteLock 支持锁的降级（写锁到读锁），但不支持锁的升级。\nLockSupport LockSupport是一个工具类，由UnSafe类实现，主要用来挂起和唤醒线程，它是创建锁和其它同步类的基础。LockSupport会为每个使用它的线程关联一个许可证（permit），在默认情况下调用LockSupport类的方法的线程是不持有许可证的。\n如果调用park()方法的线程已经拿到了与LockSupport关联的许可证，则调用LockSupport.park()时会马上返回，否则调用线程会被禁止参与线程调度，也就是会被阻塞挂起。\n在其它线程调用unpark(Thread thread)方法并将被阻塞线程作为参数时，因调用park()方法而被阻塞的线程会返回。如果其它线程调用了被阻塞线程的interrupt()方法，被阻塞线程也会返回，但不会抛出InterruptedException异常。\nContidion接口 在某些情况下，当内置锁过于死板时，可以使用显式锁。正如Lock是一种广义的内置锁一样，Condiiton也是一种广义的内置条件队列。Condition接口的定义如下：\npublic interface Condition { void await() throws InterruptedException; boolean await(long time, TimeUnit unit) throws InterruptedException; long awaitNanos(long nanosTimeout) throws InterruptedException; void awaitUninterruptibly(); boolean awaitUntil(Date deadlines) throws InterruptedException; void signal(); void signalAll(); } 一个Condition和一个Lock关联在一起，就像一个条件队列和一个内置锁相关联一样。可以通过Lock.newCondition()方法来创建一个Condition。\n特别注意：在Condition对象中，与wait()、notify()、notifyAll()方法对应的分别是await()、signal()和signalAll()。Condition对Object进行了扩展，因为它也包含wait()和notify()等方法。一定要确保使用正确的版本——await()和signal()。\n参考资料  Brian Goetz, Tim Peierls, Joshua Bloch, Joseph Bowbeer, David Holmes, and Doug Lea. Java Concurrency in Practice. Addison-Wesley Professional, 2006. 方腾飞, 魏鹏, 程晓明. Java并发编程的艺术. 机械工业出版社, 2015. Martin Fowler. Patterns of Enterprise Application Architecture. Addison-Wesley Professional, 2002.  ","href":"/java/concurrency/locks/","title":"锁"},{"content":"Wikipedia 中是这样描述 Proxy 的：\n A proxy, in its most general form, is a class functioning as an interface to something else. The proxy could interface to anything: a network connection, a large object in memory, a file, or some other resource that is expensive or impossible to duplicate. In short, a proxy is a wrapper or agent object that is being called by the client to access the real serving object behind the scenes. Use of the proxy can simply be forwarding to the real object, or can provide additional logic. In the proxy, extra functionality can be provided, for example caching when operations on the real object are resource intensive, or checking preconditions before operations on the real object are invoked. For the client, usage of a proxy object is similar to using the real object, because both implement the same interface.\n 代理模式 设计模式读书笔记之代理模式\n在经典的设计模式种，对于每一个 RealSubject 类，都需要创建一个 Proxy 代理类，当 RealSubject 这种需要被代理的类变得很多时，就需要定义大量的 Proxy 类，局面很容易一发不可收拾。JDK 动态代理可以有效地解决代理类数量过多这个问题。\nJDK 动态代理 java.lang.reflect.Proxy 提供了在程序运行期间 动态 创建接口实现类（即代理类）的方法，因为代理类是在程序运行过程中动态创建的，所以又被称为动态代理类（dynamic proxy class），被实现的接口被称为代理接口（proxy interface），代理类的实例被称为代理实例（proxy instance）。\nInvocationHandler 接口 每个代理实例都有一个相关联的实现了 InvocationHandler 接口的对象。InvocationHandler 接口的定义如下：\npublic interface InvocationHandler { public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; } 代理实例上的任何方法调用会被分派到到对应 InvocationHandler 的 invoke() 方法上。invoke() 方法有三个参数：\n proxy：代理实例 method：将要在 proxy 上调用的方法所对应的 Method 对象 args：传递给 method 的参数列表。若 method 没有参数，则应传递 null。  创建代理 Proxy 类的静态方法 newProxyInstance(ClassLoader loader, Class\u0026lt;?\u0026gt;[] interfaces, InvocationHandler h) 就是用来创建代理实例的。它有三个参数：\n loader：类加载器，用于加载代理类 interfaces：一个数组，包含代理类实现的所有接口 h：与代理类相关联的 InvocationHandler  我们可以通过下面的例子进一步理解。\n先定义一个接口：\n// 接口 public interface Subject { Object execute(); } 创建一个接口实现类：\npublic class RealSubject implements Subject { @Override public Object execute() { System.out.println(\u0026#34;Real subject is executing.\u0026#34;); return null; } } 创建代理类：\npublic class SubjectProxy implements InvocationHandler { private final Subject target; public SubjectProxy(Subject target) {this.target = target;} @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object result = method.invoke(target, args); return result; } } 创建一个代理实例，然后调用代理实例的方法简单测试一下：\npublic class SubjectProxyTest { public static void main(String[] args) { Subject subject = (Subject) Proxy.newProxyInstance( SubjectProxy.class.getClassLoader(), new Class[]{Subject.class}, new SubjectProxy(new RealSubject())); subject.execute(); } } 程序的运行结果为：\nReal subject is executing. 可以发现，虽然我们创建的是 Subject 对象，但实际被调用的方法却是 RealSubject 的方法。\n再深入一点 有没有什么办法可以看到程序运行时动态生成的代理类呢？答案是肯定的，我们可以在运行测试程序的时候加一个参数 -Djdk.proxy.ProxyGenerator.saveGeneratedFiles=true，将生成的代理类保存到当前的工作目录。在 JDK9 之前，这个参数是 -Dsun.misc.ProxyGenerator.saveGeneratedFiles=true。在笔者的电脑上，生成的代理类是 com.sun.proxy.$Proxy0，反编译得到的类是这样的：\n// // Source code recreated from a .class file by IntelliJ IDEA // (powered by FernFlower decompiler) //  package com.sun.proxy; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.lang.reflect.UndeclaredThrowableException; import ml.zhannicholas.reflection.dynamicproxy.Subject; public final class $Proxy0 extends Proxy implements Subject { private static Method m1; private static Method m2; private static Method m3; private static Method m0; public $Proxy0(InvocationHandler var1) throws { super(var1); } public final boolean equals(Object var1) throws { try { return (Boolean)super.h.invoke(this, m1, new Object[]{var1}); } catch (RuntimeException | Error var3) { throw var3; } catch (Throwable var4) { throw new UndeclaredThrowableException(var4); } } public final String toString() throws { try { return (String)super.h.invoke(this, m2, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final Object execute() throws { try { return (Object)super.h.invoke(this, m3, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final int hashCode() throws { try { return (Integer)super.h.invoke(this, m0, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } static { try { m1 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;equals\u0026#34;, Class.forName(\u0026#34;java.lang.Object\u0026#34;)); m2 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;toString\u0026#34;); m3 = Class.forName(\u0026#34;ml.zhannicholas.reflection.dynamicproxy.Subject\u0026#34;).getMethod(\u0026#34;execute\u0026#34;); m0 = Class.forName(\u0026#34;java.lang.Object\u0026#34;).getMethod(\u0026#34;hashCode\u0026#34;); } catch (NoSuchMethodException var2) { throw new NoSuchMethodError(var2.getMessage()); } catch (ClassNotFoundException var3) { throw new NoClassDefFoundError(var3.getMessage()); } } } 可以发现，代理类 $Proxy0 不仅实现了 Subject 接口，还继承自 Proxy 类。由于 Java 不支持多继承，所以只能通过接口来实现多态。这也是 JDK 动态代理的不足，即要求被代理的类至少需要实现一个接口。若被代理的类没有实现任何接口，cglib是一个不错的选择。\ncglib JDK的动态代理基于 反射 机制，生成一个 实现 代理接口的代理类，然后重写接口，实现方法增强，只能代理实现了接口的类。因为是基于反射，动态代理在生成类的时候非常快，但后续操作会很慢。\ncglib采用了 字节码 技术，通过 继承 目标类（生成子类）并重写父类的方法来达到增强的目的。cglib 可以在重写的方法中进行拦截，实现代理对象的相关功能。由于这种代理方式是基于继承的，所以 cglib 不能代理被 final 修饰的类。因为是基于字节码技术，cglib在生成类的时候很慢，但后续操作很快。\n简单示例 先定义一个原始类：\npublic class Person { public String hello(String name) { System.out.println(\u0026#34;Hello, \u0026#34; + name); return \u0026#34;Hello, \u0026#34; + name; } } 然后使用 cglib 进行代理：\npublic class PersonProxyTest { public static void main(String[] args) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(Person.class); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(\u0026#34;cglib methodintercept start...\u0026#34;); Object result = proxy.invokeSuper(obj, args); System.out.println(\u0026#34;cglib methodintercept end...\u0026#34;); return result; } }); Person personProxy = (Person) enhancer.create(); personProxy.hello(\u0026#34;Nicholas\u0026#34;); } } 运行这段代码，可以得到以下结果：\ncglib method intercept start... Hello, Nicholas cglib method intercept end... 这段代码里面用到了 Enhancer 类和 MethodInterceptor 接口。这两者的作用和JDK动态代理里面的 Proxy 和 InvocationHander 有些类似。setSuperClass(Class superclass) 告诉 Enhancer 去生成一个 superclass 的子类，而 setCallback(Callback callback) 告诉 Enhancer 该使用哪个 callback 来处理被代理类上的方法调用。MethodIntercepter 实现了 Callback 接口，允许我们对拦截的方法进行完全控制。\n参考资料  Proxy pattern. Dynamic Proxy Classes. cglib: The missing manual.  ","href":"/java/java_lang/proxy/","title":"Java 代理"},{"content":"反射(Reflection) 是Java语言的一大特性，它允许Java程序在运行过程中获取自身的相关信息，还能改变程序的内部属性。我们可以使用反射获取类、接口、字段、方法的属性，也可以用反射来实例化一个对象、进行方法调用、获取或修改字段的值。\n以下是Oracle官方对反射的解释：\n Reflection enables Java code to discover information about the fields, methods and constructors of loaded classes, and to use reflected fields, methods, and constructors to operate on their underlying counterparts, within security restrictions.\n 获取Class对象 Java中共有两种类型：引用类型和 基本类型。引用类型包括：类、数组(类和数组都继承自 java.lang.Object)和接口。基本类型包括：boolean、byte、short、char、int、long、float、double。 当Java程序运行时，JVM会为每种类型实例化一个不可变的**java.lang.Class**类，实例的很多数据都来自对应的 **.class文件。java.lang.Class**是反射API的入口，它不仅提供了获取及修改对象内部信息方法，还提供了世袭化对象的途径。有三种方式可以获取到 **Class**实例的引用：直接获取、使用 **Object**类的getClass()方法和使用 **Class**类的静态forName(String className)方法。\n直接获取 对于 **A**类，我们可以直接使用 **A.class**来获取它对应的 **Class**对象，这也适用于基本类型。对于包装类，其内部的静态final变量 **TYPE**与对应基本类型的 **Class**对象是同一个。例如：\nClass\u0026lt;Object\u0026gt; objectClass = Object.class; System.out.println(int.class == Integer.TYPE); // true System.out.println(int.class == Integer.class); // false System.out.println(void.class == Void.TYPE); // true 使用 **Object**类的getClass()方法 **Object**类有一个getClass()方法，它返回对象实例引用的 **Class**对象：\nClass\u0026lt;?\u0026gt; objectClass = new Object().getClass(); 使用 **Class**类的静态forName(String className)方法 **Class**类的静态forName(String className)方法会尝试返回虚拟机内className对应的 **Class**类对象的引用。例如：\nClass\u0026lt;?\u0026gt; objectClass = Class.forName(\u0026#34;java.lang.Object\u0026#34;); 不管采取何种方式获取一个给定类型的 **Class**对象引用，返回的引用(如果存在的话)都是同一个。例如：\nSystem.out.println(Object.class == new Object().getClass()); // true System.out.println(Object.class == Class.forName(\u0026#34;java.lang.Object\u0026#34;)); // true System.out.println(new Object().getClass() == Class.forName(\u0026#34;java.lang.Object\u0026#34;)); // true 获取类信息 我们可以使用反射来获取类的信息，例如包名、类名、类型、访问修饰符等。\n现在有一个 **User**类，其定义如下：\npackage ml.zhannicholas.reflection; import java.io.Serializable; import java.util.UUID; public class User implements Cloneable, Serializable { protected String uuid; protected String name; protected String email; public class Address {} public User() {} public User(String name) { this(name, null); } private User(String name, String email) { this.uuid = UUID.randomUUID().toString(); this.name = name; this.email = email; } @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } // getter and setter methods } **ActiveUser**是 **User**类的一个子类：\npublic final class ActiveUser extends User { public String approach; // getter and setter methods } 后面会使用反射获取这些类的信息。\n获取包信息 简单的获取包名：\nString packageName = userClass.getPackageName(); // ml.zhannicholas.reflection 获取 **Package**对象：\nPackage package = userClass.getPackage(); **Package**对象包含了包的元数据，比如包名、版本号等。\n获取类名 **Class**类提供了四个获取类名的方法，不同方法的返回值稍有差别： 获取类的simple name(不含包名):\npublic String getSimpleName(); 获取类的name(含包名):\npublic String getName(); 获取类的规范名(canonical name)：\npublic String getCanonicalName(); 获取对应类型的名字(type name)：\npublic String getTypeName(); 基本类型和void 对于原始类型和void，四种方法返回的结果相同：\nint.class.getSimpleName(); // int int.class.getName(); // int int.class.getTypeName(); // int int.class.getCanonicalName(); // int void.class.getSimpleName(); // void void.class.getName(); // void void.class.getTypeName(); // void void.class.getCanonicalName(); // void 对象类型 对于普通的Java对象，getSimpleName()返回简单的类名，其它三个方法的返回值和类的规范名相同：\nUser.class.getSimpleName(); // User User.class.getName(); // ml.zhannicholas.reflection.User User.class.getTypeName(); // ml.zhannicholas.reflection.User User.class.getCanonicalName(); // ml.zhannicholas.reflection.User 内部类 对于四种内部类，以上几种方法的返回值略有差异。\n静态内部类和非静态成员类 对于静态内部类(nested classes)和非静态成员类(inner class)，getSimpleName()返回简单的类名，getCanonicalName()返回类的规范名，而getName()和getTypeName()返回的结果相同，为$连接的外部类的规范名和内部类的简单名：\nUser.Address.class.getSimpleName(); // Address User.Address.class.getName(); // ml.zhannicholas.reflection.User$Address User.Address.class.getTypeName(); // ml.zhannicholas.reflection.User$Address User.Address.class.getCanonicalName(); // ml.zhannicholas.reflection.User.Address 匿名类 对于匿名类(anonymouse classes)，getCanonicalName()方法会返回null，getSimpleName()返回一个空字符串。getName()和getTypeName()返回的结果由三部分组成，从左到右分别是：外部类的规范名、$和该匿名类在所有匿名类中出现的位置(从1开始)：\npackage ml.zhannicholas.reflecton; public class ReflectionTest { public static void main(String[] args) { new User(){}.getClass().getSimpleName(); // 空字符串  new User(){}.getClass().getName(); // ml.zhannicholas.reflection.ReflectionTest$2  new User(){}.getClass().getTypeName(); // ml.zhannicholas.reflection.ReflectionTest$3  new User(){}.getClass().getCanonicalName() // null  } } 局部类 对于局部类(local classes)，getCanonicalName()方法会返回null，getSimpleName()返回简单的类名。getName()和getTypeName()返回的结果由四部分组成，从左到右分别是：外部类的规范名、$、该局部类在所有同名局部类中出现的位置(从1开始)和局部类的简单名：\npackage ml.zhannicholas.reflection; public class ReflectionTest { public static void main(String[] args) { testLocalClasses2(); testLocalClasses1(); } private static void testLocalClasses1() { System.out.println(\u0026#34;local classes test1: \u0026#34;); class LocalClass {} System.out.println(LocalClass.class.getName()); System.out.println(LocalClass.class.getTypeName()); System.out.println(LocalClass.class.getCanonicalName()); System.out.println(LocalClass.class.getSimpleName()); } private static void testLocalClasses2() { System.out.println(\u0026#34;local classes test2: \u0026#34;); class LocalClass {} System.out.println(LocalClass.class.getName()); System.out.println(LocalClass.class.getTypeName()); System.out.println(LocalClass.class.getCanonicalName()); System.out.println(LocalClass.class.getSimpleName()); } } 上面这段代码的运行结果为：\nlocal classes test2: ml.zhannicholas.reflection.ReflectionTest$2LocalClass ml.zhannicholas.reflection.ReflectionTest$2LocalClass null LocalClass local classes test1: ml.zhannicholas.reflection.ReflectionTest$1LocalClass ml.zhannicholas.reflection.ReflectionTest$1LocalClass null LocalClass 数组 对于数组，getSimpleName()返回数组元素类型对应的simple name加上数组维度数量的[],getTypeName()返回的是数组元素类型对应的type name加上数组维度数量的[]， getCanonicalName()返回的是数组元素类型对应的canonical name加上数组维度数量的[]，getName()返回的是数组维度数量的[加上数组元素类型对应的编码:\nUser[].class.getSimpleName(); // User[] User[].class.getName(); // [Lml.zhannicholas.reflection.User User[].class.getTypeName(); // ml.zhannicholas.reflection.User[] User[].class.getCanonicalName(); // ml.zhannicholas.reflection.User[] 全限定名与规范名 全限定名(fully qualified name)与规范名(canonical name)的定义和区别见JLS11。\n获取修饰符 通过getModifiers()方法可以获取一个类或接口的修饰符，返回的结果会被编码成一个int类型的整数，整数的每一位都代表不同的访问修饰符。我们可以 **Modifier**类的静态方法toString()获得解码后的访问修饰符。\nint modifier = User.class.getModifiers(); Modifier.toString(modifier); // public Modifier.toString(Serializable.class.getModifiers()); // public abstract interface 通过这里我们可以明确的知道接口类型是abstract的。\n值得注意的是，这里的访问修饰符指的是JVM中的访问修饰符，即JVMS11中定义的access_flags，共9种，而JLS11定义的类修饰符只有7种。\n获取父类 通过getSuperClass()方法可以获得一个类的父类对应的 **Class实例。对于基本类型、void、接口、Object**类本身，调用这个方法会返回null。对于数组，方法会返回 **Object**类对应的 **Class**实例：\nSystem.out.println(User.class.getSuperclass()); // class java.lang.Object System.out.println(ActiveUser[].class.getSuperclass()); // class java.lang.Object System.out.println(Object.class.getSuperclass()); // null System.out.println(int.class.getSuperclass()); // null System.out.println(void.class.getSuperclass()); // null System.out.println(Serializable.class.getSuperclass()); // null 获取类直接实现的接口 通过调用getInterfaces()方法，我们可以得到类直接实现的接口，或接口直接继承的接口。对于基本类型，方法返回一个长度为0的数组。对于数组类型，方法会返回一个长度为2的数组，对应的元素分别为 **java.lang.Cloneable**和 java.io.Serializable：\nSystem.out.println(Arrays.toString(User.class.getInterfaces())); // [interface java.lang.Cloneable, interface java.io.Serializable] System.out.println(Arrays.toString(ActiveUser.class.getInterfaces())); // [] System.out.println(Arrays.toString(int.class.getInterfaces())); // [] System.out.println(Arrays.toString(List.class.getInterfaces())); // [interface java.util.Collection] System.out.println(Arrays.toString(User[].class.getInterfaces())); // [interface java.lang.Cloneable, interface java.io.Serializable] 获取注解 可以通过以下方式获取一个类或接口上的注解：\nAnnotation[] annotations = Deprecated.class.getAnnotations(); 获取泛型参数 可以通过getTypeParameters()方法获取泛型参数，例如：\nTypeVariable[] typeVariables = Map.class.getTypeParameters(); for (TypeVariable typeVariable: typeVariables) { System.out.print(typeVariable.getName() + \u0026#34; \u0026#34;); } 以上代码获取了 **java.util.Map**接口内的泛型参数名字，输出结果为：\nK V 获取构造函数信息 **Class**类提供了4种可以获取构造函数信息的方法：\npublic Constructor\u0026lt;?\u0026gt;[] getConstructors(); public Constructor\u0026lt;T\u0026gt; getConstructor(Class\u0026lt;?\u0026gt;... parameterTypes); public Constructor\u0026lt;?\u0026gt;[] getDeclaredConstructors(); public Constructor\u0026lt;T\u0026gt; getDeclaredConstructor(Class\u0026lt;?\u0026gt;... parameterTypes); getConstructors()方法返回当前类中的所有公有构造函数(对于接口类型、基本类型、void类型和数组类型，返回的数组长度为0)，getDeclaredConstructors()方法返回当前类中所有声明的所有构造函数，如果当前类有默认构造函数，那么默认构造函数也会被添加到返回结果中。 getConstructor()和getDeclaredConstructor()可以返回具有特定参数列表的构造函数，如果找不到特定参数列表的构造函数，经抛出一个 **NoSuchMethodException**异常。\nprivate static void printConstructor(Constructor\u0026lt;?\u0026gt; constructor) { System.out.println(constructor.toGenericString()); } private static void printConstructors(Constructor\u0026lt;?\u0026gt;[] constructors) { for (Constructor\u0026lt;?\u0026gt; constructor: constructors) { printConstructor(constructor); } System.out.println(); } private static void getConstructors() throws NoSuchMethodException { System.out.println(\u0026#34;Constructors of ActiveUser.class:\u0026#34;); printConstructors(ActiveUser.class.getConstructors()); System.out.println(\u0026#34;Constructors of User.class:\u0026#34;); printConstructors(User.class.getConstructors()); System.out.println(\u0026#34;Declared constructors of ActiveUser.class:\u0026#34;); printConstructors(ActiveUser.class.getDeclaredConstructors()); System.out.println(\u0026#34;Declared constructors of User.class:\u0026#34;); printConstructors(User.class.getDeclaredConstructors()); System.out.println(\u0026#34;Declared constructors of User.class with parameters list (String.class):\u0026#34;); printConstructor(User.class.getDeclaredConstructor(String.class)); } getConstructors()方法会尝试去获取 **User**和 **ActiveUser**的构造函数并输出，运行方法，将会得到以下结果：\nConstructors of ActiveUser.class: public ml.zhannicholas.reflection.ActiveUser() Constructors of User.class: public ml.zhannicholas.reflection.User(java.lang.String) public ml.zhannicholas.reflection.User() Declared constructors of ActiveUser.class: public ml.zhannicholas.reflection.ActiveUser() Declared constructors of User.class: private ml.zhannicholas.reflection.User(java.lang.String,java.lang.String) public ml.zhannicholas.reflection.User(java.lang.String) public ml.zhannicholas.reflection.User() Declared constructors of User.class with parameters list (String.class): public ml.zhannicholas.reflection.User(java.lang.String) 获取字段信息 **Class**类提供了4种可以获取字段信息的方法：\npublic Field[] getFields(); public Field getField(String name); public Field[] getDeclaredFields(); public Field getDeclaredField(String name); getFields()方法返回类中所有的公有字段(包括继承来的公有字段)，getDeclaredFields()返回类中声明的所有字段(但不包括从继承来的字段)，getField()和getDeclaredField()会返回具有指定name的字段，若不存在name对应的字段，将抛出一个 NoSuchFieldException。\nprivate static void printField(Field field) { System.out.println(field.toGenericString()); } private static void printFields(Field[] fields) { for (Field field: fields) { printField(field); } System.out.println(); } private static void getFields() throws NoSuchFieldException { System.out.println(\u0026#34;Fields of User.class:\u0026#34;); printFields(User.class.getFields()); System.out.println(\u0026#34;Fields of ActiveUser.class\u0026#34;); printFields(ActiveUser.class.getFields()); System.out.println(\u0026#34;Declared fields of User.class:\u0026#34;); printFields(User.class.getDeclaredFields()); System.out.println(\u0026#34;Declared fields of ActiveUser.class:\u0026#34;); printFields(ActiveUser.class.getDeclaredFields()); System.out.println(\u0026#34;Filed [approach] of ActiveUser.class:\u0026#34;); printField(ActiveUser.class.getField(\u0026#34;approach\u0026#34;)); System.out.println(\u0026#34;Declared field [uuid] of User.class:\u0026#34;); printField(User.class.getDeclaredField(\u0026#34;uuid\u0026#34;)); } getFields()方法会尝试获取 **User**和 **ActiveUser**类中满足要求的字段并输出，输出结果如下：\nFields of User.class: Fields of ActiveUser.class public java.lang.String ml.zhannicholas.reflection.ActiveUser.approach Declared fields of User.class: protected java.lang.String ml.zhannicholas.reflection.User.uuid protected java.lang.String ml.zhannicholas.reflection.User.name protected java.lang.String ml.zhannicholas.reflection.User.email Declared fields of ActiveUser.class: public java.lang.String ml.zhannicholas.reflection.ActiveUser.approach Filed [approach] of ActiveUser.class: public java.lang.String ml.zhannicholas.reflection.ActiveUser.approach Declared field [uuid] of User.class: protected java.lang.String ml.zhannicholas.reflection.User.uuid 获取方法信息 **Class**类提供了4种可以获取方法信息的方法：\npublic Method[] getMethods(); public Method getMethod(String name, Class\u0026lt;?\u0026gt;... parameterTypes); public Method[] getDeclaredMethods(); public Method getDeclaredMethod(String name, Class\u0026lt;?\u0026gt;... parameterTypes); getMethods()方法返回类中所有公有方法(包括继承来的)，getDeclaredMethods()方法返回类中所有声明的方法(不包括继承来的),getMethod()和getDeclaredMethod()返回满足指定参数列表并且方法名匹配的方法，可能抛出一个 NoSuchMethodException。这四个方法不会获取类中的构造函数。\nprivate static void printMethod(Method method) { System.out.println(method.toGenericString()); } private static void printMethods(Method[] methods) { for (Method method: methods) { printMethod(method); } System.out.println(); } private static void getMethods() throws NoSuchMethodException { System.out.println(\u0026#34;Methods of User.class:\u0026#34;); printMethods(User.class.getMethods()); System.out.println(\u0026#34;Declared method of User.class:\u0026#34;); printMethods(User.class.getDeclaredMethods()); System.out.println(\u0026#34;Method [equals] without parameter list in User.class:\u0026#34;); printMethod(User.class.getMethod(\u0026#34;equals\u0026#34;, Object.class)); System.out.println(\u0026#34;Declared method [getEmail] without parameter list in User.class:\u0026#34;); printMethod(User.class.getDeclaredMethod(\u0026#34;getEmail\u0026#34;)); } 这段代码会尝试获取 **User**类中满足要求的方法并输出，运行结果为：\nMethods of User.class: public java.lang.String ml.zhannicholas.reflection.User.getName() public void ml.zhannicholas.reflection.User.setName(java.lang.String) public java.lang.String ml.zhannicholas.reflection.User.getEmail() public void ml.zhannicholas.reflection.User.setEmail(java.lang.String) public final native void java.lang.Object.wait(long) throws java.lang.InterruptedException public final void java.lang.Object.wait(long,int) throws java.lang.InterruptedException public final void java.lang.Object.wait() throws java.lang.InterruptedException public boolean java.lang.Object.equals(java.lang.Object) public java.lang.String java.lang.Object.toString() public native int java.lang.Object.hashCode() public final native java.lang.Class\u0026lt;?\u0026gt; java.lang.Object.getClass() public final native void java.lang.Object.notify() public final native void java.lang.Object.notifyAll() Declared method of User.class: protected java.lang.Object ml.zhannicholas.reflection.User.clone() throws java.lang.CloneNotSupportedException public java.lang.String ml.zhannicholas.reflection.User.getName() public void ml.zhannicholas.reflection.User.setName(java.lang.String) public java.lang.String ml.zhannicholas.reflection.User.getEmail() public void ml.zhannicholas.reflection.User.setEmail(java.lang.String) Method [equals] without parameter list in User.class: public boolean java.lang.Object.equals(java.lang.Object) Declared method [getEmail] without parameter list in User.class: public java.lang.String ml.zhannicholas.reflection.User.getEmail() 创建新实例 通过反射，有两种方式可以创建新的实例：\n 使用Class.newInstance()。 使用Constructor.newInstance()。  前者在JDK9中已被弃用，因为它会抛出构造函数中的任何异常(不管是受检异常还是非受检异常)，而后者会使用 **InvocationTargetException**来包裹构造函数抛出的任何异常。此外，和后者相比，前者只支持无参构造函数并要求这个构造函数是可见的，而后者能够调用所有的构造函数(包括被private修饰的构造函数)。前者可以使用clazz.getDeclaredConstructor().newInstance()代替。\nSystem.out.println(\u0026#34;Invoke default constructor:\u0026#34;); System.out.println(User.class.getDeclaredConstructor().newInstance()); System.out.println(\u0026#34;Invoke public constructor:\u0026#34;); User user1 = User.class.getDeclaredConstructor(String.class).newInstance(\u0026#34;Nicholas\u0026#34;); System.out.println(\u0026#34;name=\u0026#34; + user1.name + \u0026#34;, email=\u0026#34; + user1.getEmail()); System.out.println(\u0026#34;Invoke private constructor:\u0026#34;); Constructor\u0026lt;User\u0026gt; userConstructor = User.class.getDeclaredConstructor(String.class, String.class); userConstructor.setAccessible(true); User user2 = userConstructor.newInstance(\u0026#34;Nicholas\u0026#34;, \u0026#34;nicholas@zhannicholas.ml\u0026#34;); System.out.println(\u0026#34;name=\u0026#34; + user2.name + \u0026#34;, email=\u0026#34; + user2.getEmail()); 上面的代码演示了如何使用反射来调用默认构造函数，公有构造函数以及私有构造函数。在调用私有构造函数时，需要先是该构造函数可见，否则会遭遇一个 IllegalAccessException。运行结果如下：\nnvoke default constructor: ml.zhannicholas.reflection.User@5594a1b5 Invoke public constructor: name=Nicholas, email=null Invoke private constructor: name=Nicholas, email=nicholas@zhannicholas.ml 操作数组 **Class**类并没有直接提供操作数组的方法，但我们可以通过它提供的isArray()方法来判断某个 **Class**实例是否代表一个数组。要操作数组，我们可以使用 **java.lang.reflect.Array**类，它提供了动态创建和访问数组的静态方法。\n**Array**类提供了两个创建数组的方法：\npublic static Object newInstance(Class\u0026lt;?\u0026gt; componentType, int length); public static Object newInstance(Class\u0026lt;?\u0026gt; componentType, int... dimensions); 第一个方法用来创建一维数组，第二个方法用来创建多维数组。\n此外，**Array**还提供了获取数组维度(getLength())，设置(setXXX())和读取(getXXX())指定下标值的方法：\nint[] nums = (int[]) Array.newInstance(int.class, 2); System.out.println(Arrays.toString(nums)); Array.set(nums, 0, 1); Array.set(nums, 1, 2); System.out.println(\u0026#34;[\u0026#34; + Array.get(nums, 0) + \u0026#34;, \u0026#34; + Array.get(nums, 1) + \u0026#34;]\u0026#34;); int[][] matrix = (int[][]) Array.newInstance(int.class, 2, 2); System.out.println(\u0026#34;Dimension of matrix: \u0026#34; + Array.getLength(matrix)); System.out.println(\u0026#34;[\u0026#34; + Arrays.toString(matrix[0]) + \u0026#34;, \u0026#34; + Arrays.toString(matrix[1]) + \u0026#34;]\u0026#34;); Array.set(matrix, 0, new int[]{1,2}); Array.set(matrix, 1, new int[]{3,4}); System.out.println(Arrays.toString((int[]) Array.get(matrix, 0))); System.out.println(Arrays.toString((int[]) Array.get(matrix, 1))); 上面这段代码演示了使用 **Array**类进行数组的创建、维度及内容读取，运行结果如下：\n[0, 0] [1, 2] Dimension of matrix: 2 [[0, 0], [0, 0]] [1, 2] [3, 4] 调用对象方法 在获取到类的方法信息之后，我们可以通过 **java.lang.reflect.Method**的public Object invoke(Object obj, Object... args);方法来调用obj的中参数列表为Object... args的方法并得到方法调用结果。对于静态方法，给第一个参数传递null即可。\npublic class Calculator { public static int add(int a, int b) {return a + b;} public int minus(int a, int b) {return a - b;} } public class InvokeCalculator { public static void main(String[] args) { Method add = Calculator.class.getDeclaredMethod(\u0026#34;add\u0026#34;, int.class, int.class); System.out.println(add.invoke(null, 1, 2)); Method minus = Calculator.class.getDeclaredMethod(\u0026#34;minus\u0026#34;, int.class, int.class); System.out.println(minus.invoke(new Calculator(), 1, 2)); } } 上面的代码通过反射获取到了 **Calculator**的静态方法add()和实例方法minus，然后通过 **Method**类的的invoke()方法进行调用，输出结果如下：\n3 -1 修改字段值 在获取到类的字段信息后，我们可以通过 **java.lang.reflect.Field**类提供的getXXX()方法来获取实例字段的值，也可以使用setXXX()方法来改变实例字段的值：\nUser user = new User(\u0026#34;Nicholas\u0026#34;); Field name = User.class.getDeclaredField(\u0026#34;name\u0026#34;); System.out.println(name.get(user)); name.set(user, \u0026#34;Sarkar\u0026#34;); System.out.println(name.get(user)); 这段代码先实例化了一个 **User**类，然后通过反射读取name字段的值，之后又修改了name的值，运行结果如下：\nNicholas Sarkar 参考资料  James Gosling, Bill Joy, Guy Steele, Gilad Bracha, Alex Buckley, Daniel Smith. The Java Language Specification: Java SE 11 Edition. 2018. Tim Lindholm, Frank Yellin, Gilad Bracha, Alex Buckley, Daniel Smith. The Java Virtual Machine Specification: Java SE 11 Edition. 2018. Trail: The Reflection API.  ","href":"/java/java_lang/reflection/","title":"反射"},{"content":"通过克隆（Clone），我们可以快速构建出一个已有对象的副本。\n浅克隆 VS 深克隆 浅克隆（Shadow Clone） 或 浅复制（Shallow Copy） 把原对象中成员变量为值类型的属性都复制给克隆对象，把原对象中成员变量为引用类型的引用地址也复制给克隆对象。当原对象中存在引用类型的成员变量时，该变量的地址会被原对象和克隆对象共享。\n深克隆（Deep Clone） 或 深复制（Deep Copy） 将原对象中的所有类型的成员变量（无论是值类型还是引用类型）都复制一份给克隆对象。\n简单来说，浅克隆和深克隆的区别就在于对引用类型的成员变量的复制：前者复制的是引用对象的地址，而后者复制的是引用对象本身。\nCloneable 接口 在 Java 中，为了实现对象的克隆，我们需要让类实现 Cloneable 接口并重写 Object 类的 clone() 方法。例如：\n@Data @AllArgsConstructor public class CloneableName implements Cloneable { private String firstName; private String lastName; @Override public CloneableName clone() throws CloneNotSupportedException { return (CloneableName) super.clone(); } } @Data @AllArgsConstructor public class CloneableUser implements Cloneable { private String id; private CloneableName name; private int age; @Override public CloneableUser clone() throws CloneNotSupportedException { return (CloneableUser) super.clone(); } } 对于一个类而言，如果我们只重写 clone() 方法而不实现 Cloneable 接口，调用 clone() 方法时就会抛出 CloneNotSupportedException。\nJava 中的数组本身也是可克隆的，根据 JLS11 数组对象既是可克隆的，又是可序列化的。数组可以看成下面这个类：\nclass A\u0026lt;T\u0026gt; implements Cloneable, java.io.Serializable { public final int length = X; public T[] clone() { try { return (T[])super.clone(); } catch (CloneNotSupportedException e) { throw new InternalError(e.getMessage()); } } } clone() 方法 clone() 是一个 native 方法，它在 Object 类中定，其详细内容如下：\n/** * Creates and returns a copy of this object. The precise meaning * of \u0026#34;copy\u0026#34; may depend on the class of the object. The general * intent is that, for any object {@code x}, the expression: * \u0026lt;blockquote\u0026gt; * \u0026lt;pre\u0026gt; * x.clone() != x\u0026lt;/pre\u0026gt;\u0026lt;/blockquote\u0026gt; * will be true, and that the expression: * \u0026lt;blockquote\u0026gt; * \u0026lt;pre\u0026gt; * x.clone().getClass() == x.getClass()\u0026lt;/pre\u0026gt;\u0026lt;/blockquote\u0026gt; * will be {@code true}, but these are not absolute requirements. * While it is typically the case that: * \u0026lt;blockquote\u0026gt; * \u0026lt;pre\u0026gt; * x.clone().equals(x)\u0026lt;/pre\u0026gt;\u0026lt;/blockquote\u0026gt; * will be {@code true}, this is not an absolute requirement. * \u0026lt;p\u0026gt; * By convention, the returned object should be obtained by calling * {@code super.clone}. If a class and all of its superclasses (except * {@code Object}) obey this convention, it will be the case that * {@code x.clone().getClass() == x.getClass()}. * \u0026lt;p\u0026gt; * By convention, the object returned by this method should be independent * of this object (which is being cloned). To achieve this independence, * it may be necessary to modify one or more fields of the object returned * by {@code super.clone} before returning it. Typically, this means * copying any mutable objects that comprise the internal \u0026#34;deep structure\u0026#34; * of the object being cloned and replacing the references to these * objects with references to the copies. If a class contains only * primitive fields or references to immutable objects, then it is usually * the case that no fields in the object returned by {@code super.clone} * need to be modified. * \u0026lt;p\u0026gt; * The method {@code clone} for class {@code Object} performs a * specific cloning operation. First, if the class of this object does * not implement the interface {@code Cloneable}, then a * {@code CloneNotSupportedException} is thrown. Note that all arrays * are considered to implement the interface {@code Cloneable} and that * the return type of the {@code clone} method of an array type {@code T[]} * is {@code T[]} where T is any reference or primitive type. * Otherwise, this method creates a new instance of the class of this * object and initializes all its fields with exactly the contents of * the corresponding fields of this object, as if by assignment; the * contents of the fields are not themselves cloned. Thus, this method * performs a \u0026#34;shallow copy\u0026#34; of this object, not a \u0026#34;deep copy\u0026#34; operation. * \u0026lt;p\u0026gt; * The class {@code Object} does not itself implement the interface * {@code Cloneable}, so calling the {@code clone} method on an object * whose class is {@code Object} will result in throwing an * exception at run time. * * @return a clone of this instance. * @throws CloneNotSupportedException if the object\u0026#39;s class does not * support the {@code Cloneable} interface. Subclasses * that override the {@code clone} method can also * throw this exception to indicate that an instance cannot * be cloned. * @see java.lang.Cloneable */ @HotSpotIntrinsicCandidate protected native Object clone() throws CloneNotSupportedException; 从源码中可以看出：Object 对 clone() 有三条约定：\n 对于所有对象来说，表达式 x.clone() != x 的值为 true ，即克隆对象与原对象不应该同一个对象。 对于所有对象来说，表达式 x.clone().getClass() == x.getClass() 的值应该为 true ，即克隆对象与原对象的类型应该是一样的。 对于所有对象来说，表达式 x.clone().equals(x) 的值应该为 true，即克隆对象与原对象是相等（equal）的。  默认情况下，调用 Object.clone() 进行的是浅克隆。例如：\n@Test public void shallowCopyTest() throws CloneNotSupportedException { CloneableUser user1 = new CloneableUser(\u0026#34;id1\u0026#34;, new CloneableName(\u0026#34;fn\u0026#34;, \u0026#34;ln\u0026#34;), 20); CloneableUser user2 = user1.clone(); assertNotSame(user1, user2); // shallow copy  assertSame(user1.getName(), user2.getName()); user2.getName().setFirstName(\u0026#34;nfn\u0026#34;); assertSame(user1.getName().getFirstName(), user2.getName().getFirstName()); user2.setId(\u0026#34;id2\u0026#34;); assertNotSame(user1.getId(), user2.getId()); } 实现深克隆 在Java中，实现深克隆的方式有很多，例如：\n 为所有引用类型都实现克隆方法。 使用复制构造函数。 通过序列化与反序列化。 通过第三方工具实现深克隆，例如 Apache Commons Lang 、 Jackson 、 Gson 、FastJson 等。  为引用类型实现克隆方法 这种方法需要为每种类型都实现克隆方法，并且这些类型都要实现 Cloneable 接口。例如：\n@Test public void deepCopyTest_implementsCloneable() throws CloneNotSupportedException { @Data @NoArgsConstructor @AllArgsConstructor class Name implements Cloneable { private String firstName; private String lastName; @Override protected Name clone() throws CloneNotSupportedException { return (Name) super.clone(); } } @Data @NoArgsConstructor @AllArgsConstructor class User implements Cloneable { private String id; private Name name; private int age; @Override protected User clone() throws CloneNotSupportedException { User user = (User) super.clone(); user.setName(name.clone()); return user; } } Name newName = new Name(\u0026#34;fn\u0026#34;, \u0026#34;ln\u0026#34;); User user = new User(\u0026#34;id1\u0026#34;, newName, 20); User user2 = user.clone(); user2.getName().setFirstName(\u0026#34;nfn\u0026#34;); assertNotSame(user.getName().getFirstName(), user2.getName().getFirstName()); } 使用复制构造函数 这种方法不需要实现 Cloneable 接口，但是需要为所有的引用类型都定义一个复制构造函数。具体来说，如果构造器的参数为基本数据类型或字符串类型（不可变类型）就直接赋值，否则需要重新 new 一个对象。例如：\n@Test public void deepCopyTest_copyConstructor() { @Data @NoArgsConstructor @AllArgsConstructor class Name { private String firstName; private String lastName; public Name (Name name) { this.firstName = name.getFirstName(); this.lastName = name.getLastName(); } } @Data @NoArgsConstructor @AllArgsConstructor class User { private String id; private Name name; private int age; public User (User user) { this.id = user.getId(); this.name = new Name(user.getName()); this.age = user.age; } } Name newName = new Name(\u0026#34;fn\u0026#34;, \u0026#34;ln\u0026#34;); User user = new User(\u0026#34;id1\u0026#34;, newName, 20); User user2 = new User(user); user2.getName().setFirstName(\u0026#34;nfn\u0026#34;); assertNotSame(user.getName().getFirstName(), user2.getName().getFirstName()); } 序列化与反序列化 这种方法要求引用类型实现 Serializable 接口。思想为：先将要原对象写入到字节流中，然后再从这个字节流中读出刚刚存储的信息，来作为一个新的对象返回，新对象和原对象就不存在任何地址上的共享，这样就实现了深克隆。例如：\n@Test public void deepCopyTest_serDe() { SerializableUser user1 = new SerializableUser(\u0026#34;id1\u0026#34;, new SerializableName(\u0026#34;fn\u0026#34;, \u0026#34;ln\u0026#34;), 20); try { ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(baos); oos.writeObject(user1); oos.close(); ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bais); SerializableUser user2 = (SerializableUser) ois.readObject(); ois.close(); assertEquals(user1, user2); assertNotSame(user1, user2); user2.getName().setFirstName(\u0026#34;nfn\u0026#34;); assertNotSame(user1.getName().getFirstName(), user2.getName().getFirstName()); } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } 使用三方工具 Apache Commons Lang 工具包中的 SerializationUtils 提供了一个 clone() 方法，可以实现对象的深克隆。由于方法内部也是通过序列化与反序列化实现的，因此需要被序列化的类实现 Serialiable 接口。例如：\n@Test public void deepCopy_apacheCommonsLang() { SerializableUser user1 = new SerializableUser(\u0026#34;id1\u0026#34;, new SerializableName(\u0026#34;fn\u0026#34;, \u0026#34;ln\u0026#34;), 20); SerializableUser user2 = SerializationUtils.clone(user1); assertEquals(user1, user2); assertNotSame(user1, user2); user2.getName().setFirstName(\u0026#34;nfn\u0026#34;); assertNotSame(user1.getName().getFirstName(), user2.getName().getFirstName()); } 很多 JSON 工具库也可以实现对象的深克隆，它们的核心思想是先将对象转储为 JSON ，然后用 JSON 重新创建出一个全新的对象，新老对象的地址不同，但内容却是相同的。例如 Google 的 Gson 库：\n@Test public void deepCopy_json() { User user1 = new User(\u0026#34;id1\u0026#34;, new Name(\u0026#34;fn\u0026#34;, \u0026#34;ln\u0026#34;), 20); Gson gson = new Gson(); User user2 = gson.fromJson(gson.toJson(user1), User.class); assertEquals(user1, user2); assertNotSame(user1, user2); user2.getName().setFirstName(\u0026#34;nfn\u0026#34;); assertNotSame(user1.getName().getFirstName(), user2.getName().getFirstName()); } 相关代码 相关代码位于 Github。\n参考资料  The Java® Language Specification (Java SE 11 Edition).  ","href":"/java/java_lang/cloning_objects/","title":"Cloning Objects"},{"content":"Java允许我们在一个类中定义另一个类，后者被称为嵌套类（nested class）。嵌套类可以分为两种：\n 静态内部类（static nested class） 内部类（inner class）  其中内部类又可以分为三种：\n 非静态成员类（non-static member class） 局部内部类（local class） 匿名内部类（anonymous class）  四种嵌套类的定义如下：\nclass OuterClass { static class NestedClass{} // 静态嵌套类  class MemberClass{} // 非静态成员类  void m1() { class LocalClass{} // 局部内部类  new Thread(new Runnable() { // 匿名内部类  @Override public void run() { // nothing  } }); } } 静态内部类 静态内部类指被声明为static的内部类，它可以单独实例化，其它的内部类需要在外部类实例化之后才能实例化。静态内部类不能和外部类同名，不能访问外部类的实例变量，只能访问外部类的类变量和类方法（包括被private修饰的类变量和类方法）。\n非静态成员类 非静态成员类不被static修饰。它可以自由的访问外部类的属性和方法，无论这些属性和方法是静态的还是非静态的。但是成员类是和外部类实例相绑定的，因此不可以定义静态变量和静态方法。只有在外部类实例化之后，这个成员类才能被实例化。\n局部内部类 局部内部类是指定义在一个代码块内的类，它的作用范围只限于其所在的代码块。局部内部类和局部变量类似，不能被private、public、protected和static关键字修饰，并且只能访问方法中定义为final的局部变量。局部类的其它限制和成员类基本相同。\n匿名内部类 匿名内部类没有类名。不能使用关键字class、extends、implements，没有构造函数。一个匿名类一定是在new的后面，并且它必须继承其它类或实现一个接口。成员类的限制也适用于匿名类。\n","href":"/java/java_lang/nested_classes/","title":"Nested Classes"},{"content":"","href":"/tags/jakartaee/","title":"JakartaEE"},{"content":" A servlet is a Java™ technology-based Web component, managed by a container, that generates dynamic content.\n 和其它基于Java的组件一样，Servlet也由Java类组成，这些Java类(.class文件)会被编译成字节码，字节码随后会被动态地加载到Web服务器中并运行。Servlet通过Servlet容器实现的request/response模型来与Web客户端进行交互。\nrequest/response模型：可以简单的将这个模型看成是一种客户端与服务端通过交换各自的消息来交互的一个过程。像浏览器这样的客户端发出的消息叫做requests，而服务端响应的消息就叫做responses。\n什么是Servlet容器  The servlet container is a part of a Web server or application server that provides the network services over which requests and responses are sent, decodes MIME-based requests, and formats MIME-based responses. A servlet container also contains and manages servlets through their lifecycle.\n 容器(Container)，又称Servlet引擎(Servlet engine)。它可以是Web服务器内置的，也可以是Web服务器中的插件。对于所有的Servlet容器而言，除了必须支持的HTTP协议以外，还可能支持向HTTPS这种的基于request/response模型的协议。容器必须实现的HTTP协议版本包括HTTP/1.1和HTTP/2，当支持HTTP/2时，容器必须支持h2和h2c两种协议标识符，即所有的Servlet容器必须支持ALPN。\n此外，Servlet容器还可能会在Servlet的执行环境中加入安全限制。例如，一些应用服务器可能会限制对**Thread**对象的创建，以避免对容器中的其它组件造成负面影响。\n一个例子 下面是一个典型的Servlet事件序列：\n 客户端(比如浏览器)访问Web服务器并发起一个HTTP请求。 Web服务器接收该HTTP请求并将其转交给Servlet容器。 Servlet容器先根据配置信息决定该要调用哪一个Servlet，然后将表示request和response的对象传递给该Servlet并调用它。 Servlet从request对象中获取到远程用户的身份、请求参数和其它相关数据，然后执行处理逻辑，最后生成响应数据(通过response对象)并发回给客户端。 一旦Servlet完成整个请求处理过程，Servlet容器就会刷新响应，然后将控制权归还给Web服务器。  一个简单的Servlet 下面将编写一个非常简单的Servlet，它将展现编写Servlet的所有基本要求。最终该Servlet会在浏览器中输出一些文本：\n Hello, Servlet!\n 我这里使用的开发工具是IDEA，Servlet容器时Tomcat 9.0.35。我直接使用IDEA创建了一个Java Enterprise工程，相关配置如下：\n   Name Value     Java EE version Java EE 8   Application Server Tomcat 9.0.35   Additional Libraries and Frameworks Web Application    创建Servlet类 Servlet是一个Java类，我们创建了一个比较简单的，它直接继承了 **HttpServlet**并重写了service方法：\npublic class HelloServlet extends HttpServlet { @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { PrintWriter writer = resp.getWriter(); writer.println(\u0026#34;Hello, Servlet!\u0026#34;); writer.close(); } } service方法是Servlet容器在Servlet生命周期中调用的最基本的处理方法。在我们的service方法中，我们将Hello, Servlet!写到了响应对象的输出流中。\n配置Web程序 在WEB-INF目录下创建web.xml文件，文件内容如下：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;web-app xmlns=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\u0026#34; version=\u0026#34;4.0\u0026#34;\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;hello-servlet\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;ml.zhannicholas.servletdemos.HelloServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;servlet-mapping\u0026gt; \u0026lt;servlet-name\u0026gt;hello-servlet\u0026lt;/servlet-name\u0026gt; \u0026lt;url-pattern\u0026gt;/hello\u0026lt;/url-pattern\u0026gt; \u0026lt;/servlet-mapping\u0026gt; \u0026lt;/web-app\u0026gt; 该文件向Tomcat描述了我们要发布的Web应用程序。文件中的servlet-name表示所使用的Servlet的名字。servlet-class元素将该名称映射到一个Servlet的实现类，本示例中为 HelloServlet。servlet-mapping元素告诉Tomcat我们的Servlet只处理对/hello的请求。\n设置好web.xml后，就可以启动Tomcat了。\n运行Servlet 在Tomcat成功发布我们的Web应用之后，访问http://localhost:8080/hello 就可以看到我们的Hello, Servlet!信息了。\n示例代码位于Github。\n参考资料  Shing Wai, Chan Ed Burns. Java™ Servlet Specification, Version 4.0. Oracle Corporation, 2017.  ","href":"/java/jakartaee/what_is_servlet/","title":"What is Servlet"},{"content":"**Servlet接口是Java Servlet API的核心抽象，所有的Servlet都直接或间接地实现它。GenericServlet**和 **HttpServlet**就是Java Servlet API中的两个 **Servlet**实现类。很多时候，开发人员只需要继承 **HttpServlet**并实现他们自己的服务就行了。\n请求处理方法 **Servlet**接口中定义的service方法就是用来处理客户端请求的，Servlet容器每将一个请求路由到一个Servlet实例之后，都会调用这个service方法。为了处理并发请求，开发者编写的Servlet通常要能够处理在特定时间内service方法中有多个线程执行的情况。通常情况下，Web容器就是通过在不同线程上并发地执行service方法来将并发请求交给同一个Servlet处理的。\nHTTP请求的处理方法 为了简化HTTP请求的处理过程，抽象类 **HttpServlet**在 **Servlet**的基础上添加了一些额外的方法。这些方法可以自动的被service方法调用，它们是：\n 处理GET请求的doGet方法； 处理POST请求的doPost方法； 处理PUT请求的doPut方法； 处理DELETE请求的doDelete方法； 处理HEAD请求的doHead方法； 处理OPTIONS请求的doOptions方法； 处理TRACE请求的doTrace方法；  在开发基于HTTP的Servlet时，开发者一般只需要关心doGet和doPost方法即可。\nServlet生命周期 Servlet有一个定义非常良好的生命周期，它定义了Servlet是如何被加载并被实例化、如何初始化、如何处理客户端请求以及如何结束服务的。这个生命周期是通过 **Servlet**接口的init、service和destroy方法来定义的，所有的Servlet都必须直接或间接地实现这几个方法。Servlet的生命周期是由Servlet容器控制的，当一个请求被映射到一个Servlet时，容器会进行以下步骤：\n 如果容器中没有该Servlet的实例，容器就会：先加载Servlet，然后创建Servlet实例，再调用init方法对Servlet进行初始化。 容器调用Servlet的service方法并传入request和response对象。  当容器需要移除一个Servlet时，它会调用Servlet的destroy方法进行收尾工作。\n加载与实例化 Servlet的加载与实例化是由Servlet容器负责的。Servlet的加载与初始化可以发生在Servlet容器启动时，也可以延迟到需要处理请求时才发生。Servlet容器使用Java的类加载机制来加载Servlet，被加载的Servlet可能来自本地文件系统，也可能来自远程文件系统或其它网络服务。当Servlet被加载成功之后，容器就会实例化它以备将来使用。\n初始化 当Servlet对象被实例化之后，容器必须在它开始处理客户端请求之前将它初始化。初始化过程可以用来从数据库内读取配置数据，执行一次性活动等等。容器通过 **Servlet**接口的init方法来初始化一个Servlet，init方法 接收一个 **ServletConfig接口的实现类，它允许Servlet从Web应用的配置信息中读取name-value形式的初始化参数。此外，Servlet**还可以通过 **ServletConfig**来访问 ServletConfig（它描述了当前Servlet的运行环境）。\n在初始化过程中，Servlet实例可能会抛出一个 **UnavailableException**或 ServletException。这时，这个Servlet实例就不能被放到现役服务中，Servlet容器必须把它释放掉。这个时候，destroy方法并不会被调用，因为这种情况下Servlet的初始化是不成功的。初始化失败的Servlet在之后的某个时间点还可能再次被初始化。\n请求处理 在Servlet被正确地初始化之后，Servlet容器就可以用它来处理客户端请求了。每个请求都被表示为一个 **ServletRequest**对象，而请求的响应会被表示成一个 **ServletResponse**对象。这两个对象都会被传递给 **Servlet**接口的service方法作为入参。当处理HTTP请求时，容器会使用 **HttpServletRequest**和 **HttpServletResponse**类型的对象。\n当然，容器内处于服务状态的Servlet在整个生存期中一个请求也不处理也是有可能的。\n多线程问题 Servlet容器可能将并发的请求移交给Servlet的service方法。为了处理这些请求，Servlet开发者必须仔细处理好service方法内的多个线程。**SingleThreadModel**在Servlet 4.0中已被弃用。\n请求处理中的异常 在处理请求时，Servlet可能会抛出 **ServletException或 UnavailableException。ServletException**表示处理过程中发生了一些错误，此时容器需要采取一些操作来清除请求。而 **UnavailableException**表示该Servlet临时或永久不可用。如果不可用是永久的，容器必须将这个Servlet移除，调用它的destroy方法并释放它，容器会为所有因此而拒绝的请求返回SC_NOT_FOUND(404)。如果不可用是临时的，容器会在一定时间内不再路由相关的请求到这个Servlet上，容器会为所有在这段时间内因此而拒绝的请求返回SC_SERVICE_UNAVAILABLE(503)。\n异步处理 **AsyncContext**表示的是 **ServletRequest**创建的异步执行上下文。\n待完善。\n终止服务 当容器决定要移除一个Servlet时，它会调用这个Servlet的destroy方法。Servlet可以在这个方法中释放当前占用的资源、保存当前状态到数据库等等。在调用destroy方法之前，容器会等待service方法内的线程执行结束。当等待时间达到某个阈值时，destroy方法也可能被直接调用。在destroy方法完成后，容器必须释放这个Servlet，以便GC将其回收。\n","href":"/java/jakartaee/the_servlet_interface/","title":"The Servlet Interface"},{"content":"Java EE(Java Platform, Enterprise Edition)是构建在Java SE之上的一套企业级标准，早期被称为J2EE(Java 2 Platform Enterprise Edition)，现在则被称为Jakarta EE。\n在Java的第一个版本中，Java企业扩展仅仅只是core JDK的一部分。到了1999年，这些企业扩展被从Java SE中剥离出来，并作为Java 2的一部分，即J2EE(或Java 2 Platform Enterprise Edition)，J2EE这个称呼一致维持到了2006年。在2006年发布的Java 5中，J2EE改名为Java EE，这个名字一直使用到了2017年。2017年发生了一件和Java有关的大事儿：Oracle决定将Java EE捐赠给Eclipse基金会。虽然如此，但Oracle依旧保留了Java语言的所有权。事实上，由于Oracle拥有\u0026quot;Java\u0026quot;的商标权，按照法律规定，Eclipse基金会不能再使用Java EE这个名字。经过社区投票，基金会最终选取了Jakarta EE作为Java EE的新名字。\n简而言之，J2EE的名字在历史上的主要变更如下：\n   Version Date     J2EE 1.2 1999年12月   J2EE 1.3 2001年09月   J2EE 1.4 2003年11月   Java EE 5 2006年05月   Java EE 6 2009年12月   Java EE 7 2013年04月   Java EE 8 2017年08月   Jakarta EE 2018年12月    当前，Java EE的大部分Github仓库已经归档，取而代之的是Eclipse EE4J。\n","href":"/java/jakartaee/about/","title":"About Jakarta EE"},{"content":"Redis中提供了5个和事务相关的命令：MULTI、EXEC、DISCARD、**WATCH key [key ...]**和 UNWATCH。Redis事务一次执行一组命令，并保证：\n 一个事务中的所有命令按顺序串行执行。若某一个连接已开启事务，其它连接提交的命令并不会插入到这个事务中，而是会单独执行，即隔离性——每个会话之间是相互隔离的 要么所有的命令都执行，要么一条命令都不执行，即原子性。Redis保证当某个连接断开后，之前在该连接上所有QUEUED的命令一个也不会执行  使用事务 **MULTI**标志着一个事务块的开始，随后的命令不会立即执行，而是会被放入一个队列。当遇到 **EXEC**时，队列中的命令会以原子的方式按入队顺序执行，队列中的所有命令构成了一个单一的原子操作。而当遇到 **DISCARD**命令时，事务队列会被清空，队列中的所有命令被丢弃，事务结束，连接回到正常状态，如果某些键处于watched状态，它们将不再被watch。\n下面是一个例子，从acount:B划20到acount:A的这个操作是原子的：\n\u0026gt; mset account:A 100 account:B 200 OK \u0026gt; multi OK \u0026gt; decrby account:B 20 QUEUED \u0026gt; incrby account:A 20 QUEUED \u0026gt; exec 1) (integer) 180 2) (integer) 120 \u0026gt; mget account:A account:B 1) \u0026quot;120\u0026quot; 2) \u0026quot;180\u0026quot; 从上面这段代码可以发现，**MULTI**后的命令都被QUEUED，执行 **EXEC**会返回一个数组，数组中的元素就是之前入队的命令的执行结果，它们是顺序对应的。\n当事务中发生错误 在一个事务内，可能发生以下两类错误：\n 调用 **EXEC**之前有命令入队失败。多数情况下是由于编程错误(例如语法错误)，也可能是系统错误(例如内存用尽)，也可能是被当前线程 **WATCH**的键在调用 **MULTI**之后被其它线程修改 调用 **EXEC**之后有命令执行失败。例如在特定类型的数据上执行该类型不支持的操作  对于第一种情况，当前事务会被标记为无效状态，错误的命令并不会入队，其后的命令会继续入队，但当调用 **EXEC**时，并不会开启事务，而是会异常中止。当xx命令由于语法错误而入队失败，事务处于无效状态，调用 **EXEC并不会开启事务，而是直接终止，INCR account:B**也没有执行：\n\u0026gt; multi OK \u0026gt; xx (error) ERR unknown command `xx`, with args beginning with: \u0026gt; incr account:B QUEUED \u0026gt;exec (error) EXECABORT Transaction discarded because of previous errors. 对于第二种情况，当某个命令执行失败时，Redis会继续执行后续命令，因为Redis不支持回滚。继续以account:A和account:B举例，我们先在string上执行 **LPOP**命令，命令本身没有语法错误，因此入队成功，开启事务后，lpop account:A执行失败，而后续命令正常执行：\n\u0026gt; multi OK \u0026gt; lpop account:A QUEUED \u0026gt; incr account:B QUEUED \u0026gt; exec 1) (error) WRONGTYPE Operation against a key holding the wrong kind of value 2) (integer) 181 \u0026gt; mget account:A account:B 1) \u0026quot;120\u0026quot; 2) \u0026quot;181\u0026quot; 第一种情况和第二种情况最大的区别在于：前者会导致 **EXEC**执行失败，而后者不会。\n乐观并发控制 **WATCH为Redis事务提供了基于check-and-set(CAS)的乐观并发控制机制，作用范围为单个连接。WATCH**必须在 **MULTI**之前调用，它在调用 **EXEC命令之前持续watch一组键，若被watch的键中有至少一个被其它客户端修改，整个事务就会中止，EXEC也会返回NULL以表示事务失败(没有任何命令被执行)。WATCH**可以被多次调用，后面的调用不会重置之前被watch的键，而是持续增加新的被watch的键。\n**UNWATCH**会使得当前被watch的所有键不再被监视。如果已经在 **WATCH**之后调用了 **EXEC**或 DISCARD，就不需要再次手动调用 **UNWATCH**了，因为它们会自动unwatch。\n参考资料  Transactions.  ","href":"/posts/redis/redis_transactions/","title":"Redis中的事务"},{"content":"作为Redis 5.0中推出的全新数据结构，stream的行为就像append-only log一样，但它由基数树(radix tree)实现。stream由entry构成。它具有很多特性：stream的entry保存了一组field-value对，和hash十分类似。除了field-value对，每个entry都具有唯一ID，默认情况下，这个ID的形式为：\u0026lt;millisecond-timestamp\u0026gt;-\u0026lt;sequence number\u0026gt;。stream支持基于ID的范围查询，若将时间戳作为ID的前缀，便可以实现基于时间的范围查询。entry一旦被创建，其存储的内容就不能被修改，但我们可以从stream中删除某个entry。若要向stream中添加entry，只能通过在stream的末尾追加这个entry的方式进行。最后，stream同时支持阻塞和非阻塞的消费模式，可以被多个不同的消费组消费或处理，这些消费组相当于stream的订阅者。stream和Pub/Sub最大的区别在于：stream会为了后续客户端的消费而在内存中保存数据，Pub/Sub不保存任何消息。\nStream生产者 通常情况下，producer从一个或多个源读取数据，然后将它们写入到stream。\nRedis Stream的生产者API Redis Stream Producer API允许producer将任何消息(entry)追加到stream的末尾，整个API只包含一个命令——XADD key ID field value [field value ...]。 **XADD命令用于将一个entry追加到stream，当key不存在时，还会新建一个stream：第一个参数key代表一个特定的stream，ID表示消息的ID，通常情况下我们会使用*来告诉Redis我们希望由Redis来生成这个ID，接下来是entry的内容。XADD**会将新加入stream的消息的ID作为返回值返回。\n\u0026gt; xadd numbers * n 0 \u0026quot;1587959118318-0\u0026quot; \u0026gt; xadd numbers * n 1 \u0026quot;1587959128680-0\u0026quot; \u0026gt; type numbers stream 消息ID ID是唯一的，并且每个消息有且只有一个ID，它代表着消息在stream中的位置。也就是说：越接近于stream开头的消息的ID越小，越接近于stream末尾的消息的ID越大。消息的ID是不可变的，一旦被创建，就不能再被修改。\n我们可以在一个命令的前面加上一个数字，然后Redis会多次执行这个命令。这能帮助我们更好的观察Redis自动生成消息ID的特点：\n\u0026gt; 7 xadd letter_number * a 1 \u0026quot;1587960623524-0\u0026quot; \u0026quot;1587960623525-0\u0026quot; \u0026quot;1587960623525-1\u0026quot; \u0026quot;1587960623526-0\u0026quot; \u0026quot;1587960623526-1\u0026quot; \u0026quot;1587960623527-0\u0026quot; \u0026quot;1587960623527-1\u0026quot; 这里重复执行了7次xadd letter_number * a 1。Redis自动生成的消息ID的格式为：\u0026lt;millisecond-timestamp\u0026gt;-\u0026lt;sequence number\u0026gt;。可以发现：当timestamp相同时，sequence number会不断自增(多个消息可能会在同一个时间出现，用自增保证ID的唯一性)。每到一个新的时刻，sequence number会重置为0。\n实际上，ID包括两个数字(64位无符号整数)，数字之间用-分隔。ID是总是自增的，这体现在组成ID的两个数字的自增上。插入一个ID比当前stream中最大entry的ID还小的entry是不被接受的。\n\u0026gt; xadd letter_number 1-1 a 1 (error) ERR The ID specified in XADD is equal or smaller than the target stream top item 有时候，我们可能希望使用自己的ID，这个时候我们需要保证自己的ID是自增的，一个有效的ID必须大于0-0。\n\u0026gt; xadd mystream 0-0 max 100 (error) ERR The ID specified in XADD must be greater than 0-0 \u0026gt; xadd mystream 0-1 max 100 \u0026quot;0-1\u0026quot; \u0026gt; xadd mystream 1-1 max 10 \u0026quot;1-1\u0026quot; 插入消息时，我们也可以将ID指定为一个数字，称为partial ID，这个时候Redis会自动帮我们添加sequence number。\n\u0026gt; xadd mystream 2 max 111 \u0026quot;2-0\u0026quot; // Redis自动添加sequence number 消息内容 **XADD**命令不接收空消息，一个消息至少包含一个field-value对。每一个field上的value都是一个普通的Redis字符串。消息的内容对Redis来说时不透明的，Redis本身也不处理消息的内容。\n管理Stream的长度 **XLEN key**可以用来查看stream中消息(entry)的数量，若key不存在，将返回0(就像stream是空的一样)。\n\u0026gt; xlen ms-1 (integer) 0 \u0026gt; xadd ms-1 0-1 f1 v1 f2 v2 \u0026quot;0-1\u0026quot; \u0026gt; xadd ms-1 1-1 f1 v1 f2 v2 \u0026quot;1-1\u0026quot; \u0026gt; xlen ms-1 (integer) 2 **XDEL key ID [ID ...]**用来从stream中删除消息，返回实际被删除消息的数量。虽然stream是一个append-only的数据结构，但它是保存在内存中的，所以我们可以执行删除操作。当删除某个消息时，并不是真正删除，而是标记删除，当满足一定条件时，才会进行真正的删除操作。即使stream中的所有消息都被删除，stream为空时，stream本身也不会被删除，要删除它，可以使用 **DEL**或 UNLINK。\n\u0026gt; xdel ms-1 0-1 1-1 (integer) 2 \u0026gt; xlen ms-1 (integer) 0 \u0026gt; exists ms-1 (integer) 1 \u0026gt; del ms-1 (integer) 0 使用 **XDEL**可以限制stream的长度，但我们需要知道消息的ID。为了防止stream无限扩大，Redis也允许我们对stream进行修剪(trim)，即限制stream长度的上限。 **XTRIM key MAXLEN [~] count将stream的长度限制为count，当stream长度超过上限，XTRIM**会开始删除stream中ID最小的消息。然而，使用MAXLEN对stream进行修建的开销非常大，这和entry底层的基数树(radix tree)中采用的宏结点(macro node)有关系。命令提供了一个可选参数~，XTRIM mystream MAXLEN ~ 1000表示：并不是真的不能超过1000，消息的数量可以操作1000且至少是1000。当Redis移除整个宏结点时才会进行修剪操作。\n\u0026gt; xadd ms-2 0-1 f1 v1 \u0026quot;0-1\u0026quot; \u0026gt; xadd ms-2 0-2 f1 v1 \u0026quot;0-2\u0026quot; \u0026gt; xadd ms-2 0-3 f1 v1 \u0026quot;0-3\u0026quot; \u0026gt; xtrim ms-2 MAXLEN 2 (integer) 1 // 有一个entry被删除 \u0026gt; xlen ms-2 (integer) 2 如果需要一直限制stream的长度，除了周期性的手动调用 XTRIM，还可以利用 **XADD**提供的MAXLEN来在我们添加entry时自动修剪strean，例如：xadd ms-3 maxlen ~ 2 * f1 v1。\n范围查询 **XRANGE key start end [COUNT count]**查询ID在[start, end]之间的所有entry。\n\u0026gt; 10 xadd ms-3 * f1 v1 // 生成10条消息 \u0026quot;1587969310313-0\u0026quot; \u0026quot;1587969310313-1\u0026quot; \u0026quot;1587969310314-0\u0026quot; \u0026quot;1587969310314-1\u0026quot; \u0026quot;1587969310314-2\u0026quot; \u0026quot;1587969310315-0\u0026quot; \u0026quot;1587969310315-1\u0026quot; \u0026quot;1587969310316-0\u0026quot; \u0026quot;1587969310316-1\u0026quot; \u0026quot;1587969310316-2\u0026quot; \u0026gt; xrange ms-3 1587969310313-0 1587969310313-1 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 2) 1) \u0026quot;1587969310313-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; -和+表示stream可能存在的最小ID和最大ID，-等价于0-0，+等价于18446744073709551615-18446744073709551615。\n\u0026gt; xrange ms-3 - + // 使用-和+ 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 2) 1) \u0026quot;1587969310313-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 3) 1) \u0026quot;1587969310314-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 4) 1) \u0026quot;1587969310314-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 5) 1) \u0026quot;1587969310314-2\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 6) 1) \u0026quot;1587969310315-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 7) 1) \u0026quot;1587969310315-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 8) 1) \u0026quot;1587969310316-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 9) 1) \u0026quot;1587969310316-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 10) 1) \u0026quot;1587969310316-2\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 此外，还可以使用partial ID。例如：\n\u0026gt; xrange ms-3 1587969310313 1587969310314 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 2) 1) \u0026quot;1587969310313-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 3) 1) \u0026quot;1587969310314-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 4) 1) \u0026quot;1587969310314-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 5) 1) \u0026quot;1587969310314-2\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 可选参数count可以限制返回结果数量的上限：\n\u0026gt; xrange ms-3 1587969310313 1587969310314 count 1 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 迭代Stream 结合ID自增的特性和 **XRANGE**的count参数，我们可以完成对整个stream的迭代。首先：\n\u0026gt; xrange ms-3 - + count 2 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 2) 1) \u0026quot;1587969310313-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 然后将当前迭代返回的最后一个ID的sequence number加一，作为下一次迭代的起始ID：\n\u0026gt; xrange ms-3 1587969310313-1 + count 2 1) 1) \u0026quot;1587969310313-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 2) 1) \u0026quot;1587969310314-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 重复这个过程即可。以上演示的是ID从小到大的正向迭代过程，若要从大到小进行反向迭代，可以使用 XREVRANGE key end start [COUNT count]。\n查询单条消息 如果只需要查询单条消息，可以将count参数设为1。\n\u0026gt; xrange ms-3 1587969310313-0 + count 1 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 因为 **XRANGE**和 **XREVRANGE**采用的是闭区间，所以当start和end相同时，查询的结果就是单条消息。\n\u0026gt; xrange ms-3 1587969310313-0 1587969310313-0 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; \u0026gt; xrevrange ms-3 1587969310313-0 1587969310313-0 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 消费者 有时候，我们并不希望主动去查询stream，而是希望当stream中有新的消息时，新的消息可以直接推送给我们，也就是订阅stream。**XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key ...] id [id ...]**正是用来监听stream中新的消息的。例如：\n\u0026gt; xread count 1 streams ms-3 0 1) 1) \u0026quot;ms-3\u0026quot; 2) 1) 1) \u0026quot;1587969310313-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; **XREAD**不支持-和+，id表示消费者上一次收到的消息ID，所以返回的消息ID都会大于id。\n\u0026gt; xread count 1 streams ms-3 1587969310313-0 1) 1) \u0026quot;ms-3\u0026quot; 2) 1) 1) \u0026quot;1587969310313-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 阻塞型消费者 默认情况下，**XREAD**是非阻塞的。通过使用可选的[BLOCK] milliseconds参数，我们可以将 **XREAD变成阻塞操作。在消息到来之前，XREAD**会在新消息到来之前阻塞阻塞一段时间，若在超时之前有新的消息到来，则结束阻塞并返回新的消息，否则超时自动返回。\n\u0026gt; xrevrange ms-3 + - count 2 // 获取最新的两条消息 1) 1) \u0026quot;1587969310316-2\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 2) 1) \u0026quot;1587969310316-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; \u0026gt; xread count 1 block 1000 streams ms-3 1587969310316-1 // 因为有消息，直接返回 1) 1) \u0026quot;ms-3\u0026quot; 2) 1) 1) \u0026quot;1587969310316-2\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; \u0026gt; xread count 1 block 1000 streams ms-3 1587969310316-2 // 没有新的消息，阻塞，超时返回 (nil) (1.03s) BLOCK 0表示新消息到来之前一直阻塞，不会超时返回。在client-2上阻塞，$表示当前stream中最新消息的ID(使用$之后，每个消息最多收到一次)，所以client-2会一直阻塞：\nclient-2:6379\u0026gt; xread count 1 block 0 streams ms-3 $ 这个时候在client-1上向ms-3中写入一条消息：\nclient-1:6379\u0026gt; xadd ms-3 * f1 v1 \u0026quot;1587974321051-0\u0026quot; client-2收到消息，结束阻塞，返回：\nclient-2:6379\u0026gt; xread count 1 block 0 streams ms-3 $ 1) 1) \u0026quot;ms-3\u0026quot; 2) 1) 1) \u0026quot;1587974321051-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; (23.59s) 消费组 **XREAD**可以让多个消费者都获取到stream中的所有消息。将所有的消息同时分发给多个消费者有时候并不能提高效率，因为多个消费者可能做着重复的工作，而我们只需要一份结果。一个比较好的处理方式是：将stream中的消息划分为多个彼此不重叠的子集，然后将不同的子集分发给不同消费者处理。为了处理一个大的任务，多个物理消费者可以联合起来，每个物理消费者处理不同的部分，外界看起来就像只有一个消费者一样，这样，多个物理消费者构成了一个逻辑消费者，外界看到的正是这个逻辑消费者。在Redis里，这个逻辑消费者就是一个消费组(Consumer Group)，物理消费者可以根据实际需要加入或者离开逻辑消费者，逻辑消费者从stream中获取数据，数据由多个物理消费者进行处理，并且：\n 同一个消息不会被分发给多个消费者 消费组中的每一个消费者都通过名字唯一识别，名字是一个大小写敏感的字符串 每个消费组都有first ID never consumed的概念，这样一来，当一个消费者请求新的消息时，它可以提供从未被交付过的消息 使用特定的命令作为某个消息已经被正确处理的确认，此后该消息可以被移出消费组了 消费组追踪每个当前被分发给某个消费者但还未收到处理完成的确认的消息，消息的这种状态被称为pending。Redis使用Pending Entries List (PEL)来追踪这些消息  **XGROUP [CREATE key groupname id-or-$] [SETID key groupname id-or-$] [DESTROY key groupname] [DELCONSUMER key groupname consumername]**是一个用来管理和key上stream相关联的消费组的命令。\n创建消费组 要创建一个消费组，可以使用 **XGROUP CREATE key groupname id-or $**子命令。最后一个参数id-or-$是消费组开始消费的消息的ID(不包括这个ID代表的消息)。如果希望消费组从下一个最新的消息开始消费，可以使用$，$表示stream中的最后一个消息的ID。如果希望消费组消费stream中的所有消息，可以使用0作为传入ID。如果消费组已经存在，命令将放回一个-BUSYGROUP错误，否则创建成功并返回OK。\n如果stream不存在，命令会返回一个错误，可以在ID后面新增一个可选MKSTREAM子命令让Redis自动创建一个stream，这个时候创建出来的stream的长度为0。\n\u0026gt; xgroup create ms-4 group0 0 // ms-4不存在 (error) ERR The XGROUP subcommand requires the key to exist. Note that for CREATE you may want to use the MKSTREAM option to create an empty stream automatically. \u0026gt; xgroup create ms-4 group0 0 MKSTREAM // 创建group0，并让Redis自动创建出stream OK \u0026gt; exists ms-4 (integer) 1 \u0026gt; xadd ms-4 0-1 f1 v1 // 添加一条消息到ms-4 \u0026quot;0-1\u0026quot; \u0026gt; xgroup create ms-4 group1 $ // 创建group1 OK \u0026gt; xadd ms-4 1-1 f1 v1 \u0026quot;1-1\u0026quot; \u0026gt; xinfo groups ms-4 1) 1) \u0026quot;name\u0026quot; 2) \u0026quot;group0\u0026quot; 3) \u0026quot;consumers\u0026quot; 4) (integer) 0 5) \u0026quot;pending\u0026quot; 6) (integer) 0 7) \u0026quot;last-delivered-id\u0026quot; 8) \u0026quot;0-0\u0026quot; 2) 1) \u0026quot;name\u0026quot; 2) \u0026quot;group1\u0026quot; 3) \u0026quot;consumers\u0026quot; 4) (integer) 0 5) \u0026quot;pending\u0026quot; 6) (integer) 0 7) \u0026quot;last-delivered-id\u0026quot; 8) \u0026quot;0-1\u0026quot; 每个消费组都与一个stream以及最近交付的消息ID(即上面的last-delivered-id)相关联。\n添加消费者到消费组 **XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...]**是 **XREAD**的一个特殊版本，它支持Consumer Group。STREAMS选项的ID可以是以下两种形式：\n \u0026gt;。\u0026gt;是一个特殊的ID，表示消费者只想接收那些从未被交付给任何其他消费者的消息，即消费者想收到一个从未被交付过的消息 任何其它的ID，即0，其它有效ID或partial ID。这个时候，我们可以得到消费者的PEL(返回的PEL中的ID比传递的ID都要大)，而BLOCK和NOACK参数会被忽略。  \u0026gt; xreadgroup group group0 consumerA count 1 block 1000 streams ms-4 \u0026gt; 1) 1) \u0026quot;ms-4\u0026quot; 2) 1) 1) \u0026quot;0-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; \u0026gt; xreadgroup group group0 consumerA streams ms-4 0 // 查看consumerA的PEL 1) 1) \u0026quot;ms-4\u0026quot; 2) 1) 1) 0-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 上面这条命令让消费者consumerA加入到了group0中并消费了一条消息。如果这个时候我们再次执行xinfo groups ms-4，会得到和上一次不同的结果：\n\u0026gt; xinfo groups ms-4 1) 1) \u0026quot;name\u0026quot; 2) \u0026quot;group0\u0026quot; 3) \u0026quot;consumers\u0026quot; 4) (integer) 1 5) \u0026quot;pending\u0026quot; 6) (integer) 1 7) \u0026quot;last-delivered-id\u0026quot; // 0-1已被group0中的consumerA消费 8) \u0026quot;0-1\u0026quot; 2) 1) \u0026quot;name\u0026quot; 2) \u0026quot;group1\u0026quot; 3) \u0026quot;consumers\u0026quot; 4) (integer) 0 5) \u0026quot;pending\u0026quot; 6) (integer) 0 7) \u0026quot;last-delivered-id\u0026quot; 8) \u0026quot;0-1\u0026quot; 若要查看某个消费组中的消费者信息，可以使用 XINFO CONSUMERS \u0026lt;key\u0026gt; \u0026lt;group\u0026gt;：\n\u0026gt; xinfo consumers ms-4 group0 1) 1) \u0026quot;name\u0026quot; 2) \u0026quot;comsumerA\u0026quot; 3) \u0026quot;pending\u0026quot; 4) (integer) 1 5) \u0026quot;idle\u0026quot; 6) (integer) 905911 消息处理完成确认 **XACK key group ID [ID ...]**从消费组的PEL移除一个或多个消息，表示他们已被成功处理。假设group0中的consumerA已经将0-1这条消息处理完成，它需要告知Redis服务器它已经这条消息处理完毕：\n\u0026gt; xack ms-4 group0 0-1 // 发送确认 (integer) 1 \u0026gt; xreadgroup group group0 consumerA streams ms-4 0 // 查看consumerA的PEL 1) 1) \u0026quot;ms-4\u0026quot; 2) (empty list or set) 当消息处理失败时，没有 **XACK**确认，消费者可以再次进行消费。有时候，当失败在可接受范围内时，我么可以使用 **XREADGROUP**中的子命令 **NOACK**来告诉Redis不需要进行确认，Redis会认为 **XREADGROUP**返回的所有消息都已确认，因此不用再维护对应的PEL。\n管理消费组 先准备一些测试数据，创建3个消费组，它们都从最新的消息开始消费：\n\u0026gt; xgroup create ms-5 group0 $ MKSTREAM OK \u0026gt; xgroup create ms-5 group1 $ OK \u0026gt; xgroup create ms-5 group3 $ OK 向ms-5中写入7条消息：\n\u0026gt; 7 xadd ms-5 * f1 v1 \u0026quot;1587991174542-0\u0026quot; \u0026quot;1587991174542-1\u0026quot; \u0026quot;1587991174542-2\u0026quot; \u0026quot;1587991174543-0\u0026quot; \u0026quot;1587991174543-1\u0026quot; \u0026quot;1587991174543-2\u0026quot; \u0026quot;1587991174544-0\u0026quot; 修改消费者的位置 **XGROUP SETID key groupname id-or-$**允许我们修改消费组的last-delivered-id的值，进而改变接下来要消费的消息(即消费者的位置)。若要重新消费所有消息，可以使用：\n\u0026gt; xgroup setid ms-5 group1 0 OK 若现在指向消费最新的消息，则可以使用：\n\u0026gt; xgroup setid ms-5 group1 $ OK 删除消费组 因为并不会自动删除未使用的消费组，所以我们需要自己来做这件事情。**XGROUP DESTROY key groupname**会永久删除某个消费组和相关的消费者。\n\u0026gt; xgroup destroy ms-5 group3 // 删除group3 (integer) 1 从消费组删除消费者 **XGROUP DELCONSUMER key groupname consumername**命令用于从消费组里面删除消费者，它还会删除消费者对应的PEL，返回值为被删消费的的PEL中的消息数量。\n\u0026gt; xreadgroup group group0 consumerA count 1 block 1000 streams ms-5 \u0026gt; 1) 1) \u0026quot;ms-5\u0026quot; 2) 1) 1) \u0026quot;1587991174542-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; \u0026gt; xreadgroup group group0 consumerB count 1 block 1000 streams ms-5 \u0026gt; 1) 1) \u0026quot;ms-5\u0026quot; 2) 1) 1) \u0026quot;1587991174542-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; \u0026gt; xinfo consumers ms-5 group0 // 查看group0中的消费者 1) 1) \u0026quot;name\u0026quot; 2) \u0026quot;consumerA\u0026quot; 3) \u0026quot;pending\u0026quot; 4) (integer) 1 5) \u0026quot;idle\u0026quot; 6) (integer) 88581 2) 1) \u0026quot;name\u0026quot; 2) \u0026quot;consumerB\u0026quot; 3) \u0026quot;pending\u0026quot; 4) (integer) 1 5) \u0026quot;idle\u0026quot; 6) (integer) 79710 \u0026gt; xgroup delconsumer ms-5 group0 consumerB // 删除group0中的consumerB (integer) 1 \u0026gt; xinfo consumers ms-5 group0 1) 1) \u0026quot;name\u0026quot; 2) \u0026quot;consumerA\u0026quot; 3) \u0026quot;pending\u0026quot; 4) (integer) 1 5) \u0026quot;idle\u0026quot; 6) (integer) 1540978 在删除消费者的时候一定要消息，因为它可能还有未确认的消息(PEL不为空)。\n消费失败问题 理想情况下，所有交付的消息都会被确认(XACK)。但是有时候，消费者可能在发出 **XACK**之前就已经下线了，这个时候未确认的消息就会一直留在PEL中。\n查看未确认的消息 有两种方式可以查看未确认的消息：**XINFO CONSUMERS key group**和 XPENDING key group [start end count] [consumer]。\n先准备测试数据，创建消费组group0并向ms-6中写入5条消息，然后添加消费者consumerA：\n\u0026gt; xgroup create ms-6 group0 0 MKSTREAM OK \u0026gt; 5 xadd ms-6 * f1 v1 \u0026quot;1587996240331-0\u0026quot; \u0026quot;1587996240331-1\u0026quot; \u0026quot;1587996240332-0\u0026quot; \u0026quot;1587996240332-1\u0026quot; \u0026quot;1587996240333-0\u0026quot; \u0026gt; xreadgroup group group0 consumerA count 1 block 1000 streams ms-6 \u0026gt; 1) 1) \u0026quot;ms-6\u0026quot; 2) 1) 1) \u0026quot;1587996240331-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 查看消息确认状态：\n\u0026gt; xinfo consumers ms-6 group0 1) 1) \u0026quot;name\u0026quot; 2) \u0026quot;consumerA\u0026quot; 3) \u0026quot;pending\u0026quot; 4) (integer) 1 5) \u0026quot;idle\u0026quot; 6) (integer) 44813 \u0026gt; xpending ms-6 group0 1) (integer) 1 2) \u0026quot;1587996240331-0\u0026quot; 3) \u0026quot;1587996240331-0\u0026quot; 4) 1) 1) \u0026quot;consumerA\u0026quot; 2) \u0026quot;1\u0026quot; 结果显示consumerA消费的一条消息还未确认。\n更换消费者 假设consumerA在发送确认消息之前下线了，为了继续处理consumerA在下线前未处理完的消息，我们可以使用 **XCLAIM key group consumer min-idle-time ID [ID ...] [IDLE ms] [TIME ms-unix-time] [RETRYCOUNT count] [FORCE] [JUSTID]**命令让另一个消费者获得未处理完的消息并继续处理。\n先创建一个消费者consumerB：\n\u0026gt; xreadgroup group group0 consumerB count 1 block 1000 streams ms-6 \u0026gt; 1) 1) \u0026quot;ms-6\u0026quot; 2) 1) 1) \u0026quot;1587996240331-1\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; \u0026gt; xack ms-6 group0 1587996240331-1 (integer) 1 \u0026gt; xreadgroup group group0 consumerB streams ms-6 0 1) 1) \u0026quot;ms-6\u0026quot; 2) (empty list or set) // consumerB的消息都已经处理完并确认 将consumerA未处理完的消息1587996240331-0转交给consumerB：\n\u0026gt; xclaim ms-6 group0 consumerB 1000 1587996240331-0 1) 1) \u0026quot;1587996240331-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 查看consumerA和consumerB的PEL：\n\u0026gt; xreadgroup group group0 consumerA streams ms-6 0 1) 1) \u0026quot;ms-6\u0026quot; 2) (empty list or set) \u0026gt; xreadgroup group group0 consumerB streams ms-6 0 1) 1) \u0026quot;ms-6\u0026quot; 2) 1) 1) \u0026quot;1587996240331-0\u0026quot; 2) 1) \u0026quot;f1\u0026quot; 2) \u0026quot;v1\u0026quot; 如果我们运行 XPENDING，也可以发现消息1587996240331-0现在归consumerB所有：\n\u0026gt; xpending ms-6 group0 1) (integer) 1 2) \u0026quot;1587996240331-0\u0026quot; 3) \u0026quot;1587996240331-0\u0026quot; 4) 1) 1) \u0026quot;consumerB\u0026quot; 2) \u0026quot;1\u0026quot; 参考资料  Introduction to Redis Streams.  ","href":"/posts/redis/redis_streams/","title":"Redis Streams"},{"content":"Redis的键是一个字符串。在Redis中，字符串二进制安全的，也就是说：Redis中的字符串可以是任何二进制序列，即可以是任何类型的数据(比如：一张图片、一个序列化后的Java对象……)。像2、2.3、0xff、空字符串(\u0026quot;\u0026quot;)等任何二进制序列都可以作为Redis中的键。由于键是一个二进制序列，所以键是区分大小写的(a和A是两个不同的键)。当前版本的Redis的键最大支持到512MB(未来这个值可能还会更大)。过大的键会消耗更多的内存，因此选择合适的键很重要。\n通常情况下，我们会使用结构良好且有意义的键名，使用冒号(:)作为分隔符。例如：users:1000:friends。\n逻辑数据库 Redis中也有数据库(database)的概念，只是是以命名空间(namespacing)的形式体现的。在每一个逻辑数据库(logical database)内，都存在一个键空间(key space)。逻辑数据库通过下标(从0开始)进行区分。一个逻辑数据库内的键名都是唯一的，但同一个键名可以出现在多个不同的逻辑数据库中，逻辑数据库的一个作用就是对键名进行隔离。\n逻辑数据库的数量也是有限制的，Redis集群只支持database 0。\n获取所有键名 有两个命令可以获取Redis数据库内所有的键名，**KEYS pattern**和 SCAN cursor [MATCH pattern] [COUNT count]。我们可以使用这两个命令来迭代数据库内所有的键，也可以只获取满足给定模式的键。\nKEYS命令会一次性迭代完所有的键，在迭代完所有的键之前，会阻塞所有其它的操作。若数据库内键的数量非常大，执行这个命令则会很耗时，因此在生产环境中使用这个命令时要谨慎，由于 **KYES**的使用比 **SCAN**要简单，因此它在调试时还是很有用的。\nSCAN命令也会阻塞，但它采取的是增量迭代的方式，一次只迭代少量的键，阻塞时间不会过长，因此在生产环境中使用它是安全的。\n删除键 有多种方式可以删除键，每种方式都能保证删除成功，但不同方式的性能不同。\nDEL key [key ...]命令会删除给定的键并回收与键相关联的内存，这个删除操作是以阻塞方式进行的。**UNLINK key [key ...]**命令和 DEL命令类似，但它会使用单独的后台线程来回收内存，因此是非阻塞的。\n检查键的存在性 当我们执行 **SET命令时，若键不存在，将会创建键并设置值。有时候，我们希望只有当某一个键存在时，才给它设置值。EXISTS key [key ...]**命令可以检测给定的键是否存在，若键存在，则返回1，否则返回0。\n键过期 我们可以为某个key设置过期时间(expires)。当达到设置的过期时间时，对应的key就会被自动删除，就像我们显式的执行 DEL 命令一样。过期时间的单位可以是秒，也可以是毫秒，还可以是UNIX时间戳。关于expires的信息都存放在磁盘上，且有备份。这意味着，即使Redis服务器停止运行，过期时间仍然会有效。实际上，Redis保存的是key过期的确切时间，而不是剩余生存时间，虽然参数可以是秒或毫秒。\n有三种类型的命令可以管理键的过期时间：\n 设置过期时间：EXPIRE(单位为秒)、PEXPIRE(单位为毫秒)、EXPIREAT(单位为秒)、PEXPIREAT(单位为毫秒) 查看剩余过期时长：TTL(单位为秒)、PTTL(单位为毫秒) 移除过期时间：PERSIST  下面是一些例子：\n\u0026gt; set s1 v1 OK \u0026gt; expire s1 10 // 设置s1于10秒后过期 1 \u0026gt; ttl s1 // 查看s1的剩余生存时间(单位为秒) 7 \u0026gt; pttl s1 // 查看s1的剩余生存时间(单位为毫秒) 3042 \u0026gt; get s1 // 查看s1的值，未经过10秒 \u0026quot;v1\u0026quot; \u0026gt; get s1 // 查看s1的值，超过10秒，s1已被删除 (nil) \u0026gt; set s2 v2 ex 10 // 在设置值的同时指定过期时间为10秒 OK \u0026gt; set s3 v3 px 10000 // 在设置值的同时指定过期时间为10000毫秒 OK 数据类型 Redis支持多种数据结构(类型)。我们可以使用 **TYPE key**命令来查看key对应的数据类型，例如：\n\u0026gt; set key1 1 OK \u0026gt; rpush key2 a 1 \u0026gt; type key1 string \u0026gt; type key2 list **OBJECT subcommand [arguments [arguments ...]]**允许我们查看某个键对应的Redis对象的内部信息。支持的subcommand有：\n OBJECT REFCOUNT \u0026lt;key\u0026gt;：返回key对应的值的引用数 OBJECT ENCODING \u0026lt;key\u0026gt;：返回key对应的值在Redis内部存储的存储形式 OBJECT IDLETIME \u0026lt;key\u0026gt;：返回key对应的值有多久没有被读/写了，单位为秒 OBJECT FREQ \u0026lt;key\u0026gt;：返回key对应的值被访问频率的对数值 OBJECT HELP：返回简短的帮助信息  Redis中的对象在Redis内部可以被存储为多种形式：\n String可以被编码成raw(普通字符串)和int(64位有符号整数会被这么存储以节省空间) List可以被编码成ziplist(用于小型list，可以节省空间)和linkedlist Set可以被编码成intset(用于只由整数组成的小型集合)和hashtable Hash可以被编码成ziplist(用于小型hash)和hashtable Sorted Set可以被编码成ziplist(用于小型sorted set)和skiplist  当Redis无法继续维持为节省空间而是用的编码格式时，都会将编码格式转为各种类型对应的通用格式。\n下面的例子展示了如何查看数据的编码格式：\n\u0026gt; set foo 1 OK \u0026gt; object encoding foo \u0026quot;int\u0026quot; \u0026gt; append foo a (integer) 2 \u0026gt; object encoding foo \u0026quot;raw\u0026quot; Strings 字符串(string)是Redis中最基本的数据类型，它不仅可以存储文本数据，还可以存储整数、浮点数和二进制数据。string是二进制安全的，最大不能超过512MB。\nRedis提供了20多个操作string类型的命令。下面是一些例子：\n\u0026gt; set s1 \u0026quot;Hello, Redis\u0026quot; // 设置s1的值为\u0026quot;Hello, Redis\u0026quot; OK // 成功返回OK \u0026gt; get s1 // 获取值 \u0026quot;Hello, Redis\u0026quot; \u0026gt; setnx s1 \u0026quot;Redis\u0026quot; // 尝试为s1设置新值 (integer) 0 // s1已有值，不进行操作，返回0 \u0026gt; set s1 2 // 设置值 OK \u0026gt; get s1 \u0026quot;2\u0026quot; \u0026gt; incr s1 // 加1 (integer) 3 \u0026gt; incrby s1 10 // 加10 (integer) 13 \u0026gt; decr s1 // 减1 (integer) 12 \u0026gt; decrby s1 5 // 减5 (integer) 7 \u0026gt; incrbyfloat s1 1.1 // 加1.1 \u0026quot;8.09999999999999964\u0026quot; \u0026gt; incrbyfloat s1 -3.1 // 减3.1 \u0026quot;5\u0026quot; SET 和 GET 分别用来设置和检索key对应的值(这个值是一个字符串)。需要注意的是，如果某个key已经有对应的值，SET 命令会覆盖掉已有的值(不管之前的值是何种类型都会被覆盖)，和这个key关联的TTL也会被丢弃。如果不希望已有key的值被覆盖，可以使用 SETNX 命令，当key存在时，SETNX 不会进行任何操作。\n虽然值是string，但如果这个值是一个数字的字符串形式的话，可以对其进行加减操作。上面的例子中，s1的值被重新修改为了\u0026quot;2\u0026quot;，随后执行了 INCR 和 INCRBY 两个加法命令，值变成了13，之后的减法命令 DECR 和 DECRBY 将值减到了7。更加有趣的是，Redis还提供了 INCRBYFLOAT 以支持浮点加减运算。\n有时候，一次性设置或获取多个值是很有意义的，这可以通过 MSET key value [key value ...] 和 MGET key [key ...] 命令实现。类似的还有 MSETNX 命令。下面是一个例子：\n\u0026gt; mset s1 1 s2 2 s3 a OK \u0026gt; mget s1 s2 s3 1) \u0026quot;1\u0026quot; 2) \u0026quot;2\u0026quot; 3) \u0026quot;a\u0026quot; Lists 在Redis中，列表(list)表示由一系列元素组成的有序序列。通常情况下，List的实现有数组和链表两种方式，两者各有优劣。数组实现的List可以通过索引快速访问元素，但在插入或删除元素的时候较慢；链表实现的List在插入或删除元素时非常迅速，但访问元素时较慢。Redis中的list使用双向链表实现，支持双向操作。基于链表的实现还带来了一个重大优势，那就是可以快速的截取列表中的某一部分。\nRedis提供了10多个操作list的命令，下面是一些例子：\n\u0026gt; lpush list1 a 1 b // 向列表list1的头部依次插入a、1、b这三个元素 (integer) 3 // 成功插入，当前列表list1含有3个元素 \u0026gt; lrange list1 0 -1 // 查看list1中的所有元素 1) \u0026quot;b\u0026quot; 2) \u0026quot;1\u0026quot; 3) \u0026quot;a\u0026quot; \u0026gt; llen list1 // 获取list1的长度 (integer) 3 \u0026gt; rpush list1 c d // 向列表list1尾部依次插入c、d这两个元素 (integer) 5 // 插入成功，当前列表list1含有5个元素 \u0026gt; lrange list1 0 -1 1) \u0026quot;b\u0026quot; 2) \u0026quot;1\u0026quot; 3) \u0026quot;a\u0026quot; 4) \u0026quot;c\u0026quot; 5) \u0026quot;d\u0026quot; \u0026gt; lpop list1 // 移除并返回列表list1中的第一个元素 \u0026quot;b\u0026quot; \u0026gt; rpop list1 // 移除并返回列表list2中的最后一个元素 \u0026quot;d\u0026quot; \u0026gt; lrange list2 0 -1 // 列表list2是空的 (empty list or set) \u0026gt; lpop list2 // 从一个空的列表里面移除元素会返回NULL (nil) \u0026gt; rpop list2 (nil) \u0026gt; lrange list1 0 -1 1) \u0026quot;1\u0026quot; 2) \u0026quot;a\u0026quot; 3) \u0026quot;c\u0026quot; \u0026gt; lindex list1 0 // 查看列表list1中处于位置0处的元素 1 \u0026gt; lindex list1 -1 // 查看列表list1中最后一个元素 \u0026quot;c\u0026quot; \u0026gt; ltrim list1 1 2 // 修剪列表list1，只保留位于区间[1,2]中的元素 OK \u0026gt; lrange list1 0 -1 // 查看list1中所有元素 1) \u0026quot;a\u0026quot; 2) \u0026quot;c\u0026quot; 命令 LPUSH key value [value ...] 和 RPUSH key value [value ...] 分别用来向列表头部(左侧)和尾部(右侧)添加元素，LPOP key 和 RPOP key 分别用来从列表头部(左侧)和尾部(右侧)移除元素。在一个空的列表上执行 LPOP 或 RPOP 将返回 NULL 。\n若要查看列表中的元素，可以使用 LRANGE key start stop 命令，LRANGE 命令需要两个索引作为参数。这两个索引形成一个区间，分别指向待返回的第一个和最后一个元素在列表中的位置。索引可以是负数，0、-1、-2分别代表链表中的第一个、最后一个和倒数第二个元素。除了区间查看外，还可以使用 LINDEX key index 命令查看指定位置的元素。\nLLEN key 用于查看list的长度。很多时候，我们只需要保留列表中的某一部分，这个时候 LTRIM key start end 就可以排上用场了，它和 LRANGE 类似，也接收两个索引作为参数，但它将列表的内容设置为区间内的元素并去除区间外的所有元素。上面的例子中，LTRIM 告诉Redis只保留列表list1处于区间[1,2]中的元素，然后丢弃其它的元素。\n有时候，我们希望只有当列表中有元素时，才执行POP操作，这时候可以使用 BLPOP key [key ...] timeout 和 BRPOP key [key ...] timeout 。两个命令的都是从列表中移除元素，只是方向不一样。以 BLPOP 为例，它接收一个或多个key以及一个超时时间timeout(可阻塞时间，单位为秒，0代表一直阻塞)。在这些key中，如果有key对应的列表不为空，将会返回第一个非空列表的key及POP出的值。举个例子：\n\u0026gt; lrange list1 0 -1 // 列表list1包含两个元素 1) \u0026quot;a\u0026quot; 2) \u0026quot;c\u0026quot; \u0026gt; lrange list2 0 -1 // 列表list2为空 (empty list or set) \u0026gt; blpop list2 list1 0 // list1中的第一个元素被移除 1) \u0026quot;list1\u0026quot; 2) \u0026quot;a\u0026quot; BLPOP 依次检查list2和list1，然后移除并返回list1的头部元素(list2是一个空列表)。\n如果 BLPOP 命令后面给出的的key对应的都是空列表，BLPOP 就会阻塞当前连接。一旦另外一个客户端向某一个key对应的列表插入元素，BLPOP 就会解除阻塞并返回。若阻塞的时间超过了timeout，将返回NULL。\nHashes Redis中的哈希表(hash)是一个字符串类型的field-value映射表(field和value的类型均是string)，不支持多级嵌套，非常适合表示对象。\n下面是一些例子：\n\u0026gt; hset user:1 name Tome // 将哈希表user:1的name字段的值设置为Tome (integer) 1 \u0026gt; hget user:1 name // 获取哈希表user:1中name字段对应的值 \u0026quot;Tome\u0026quot; \u0026gt; hmset user:2 name Bob age 20 education barchelor // 一次向哈希表user:2中插入多个filed-value OK \u0026gt; hmget user:2 name age // 获取哈希表user:2中name和age字段对应的值 1) \u0026quot;Bob\u0026quot; 2) \u0026quot;20\u0026quot; \u0026gt; hgetall user:2 // 获取哈希表user:2中所有字段和值 1) \u0026quot;name\u0026quot; 2) \u0026quot;Bob\u0026quot; 3) \u0026quot;age\u0026quot; 4) \u0026quot;20\u0026quot; 5) \u0026quot;education\u0026quot; 6) \u0026quot;barchelor\u0026quot; \u0026gt; hget user:1 age // 企图获取一个不存在的字段对应的值，会返回NULL (nil) \u0026gt; hexists user:1 age // 检查哈希表user:1中是否存在字段age (integer) 0 \u0026gt; hvals user:2 // 返回哈希表user:2中所有的值 1) \u0026quot;Bob\u0026quot; 2) \u0026quot;20\u0026quot; 3) \u0026quot;barchelor\u0026quot; \u0026gt; hincrby user:2 age 2 // 将哈希表user:2的age字段对应的值加2 (integer) 22 \u0026gt; hget user:2 age \u0026quot;22\u0026quot; Sets Redis中的集合(set)是一个由字符串构成的无序集合，集合中不存在重复元素。除了基本的插入、删除、存在性检测等操作，Redis还支持集合的交、并、差计算。\n下面是基本操作的一些例子：\n\u0026gt; sadd colors red blue white black // 向集合colors中插入4个元素 (integer) 4 \u0026gt; smembers colors // 查看集合colors内容 1) \u0026quot;blue\u0026quot; 2) \u0026quot;white\u0026quot; 3) \u0026quot;red\u0026quot; 4) \u0026quot;black\u0026quot; \u0026gt; sismember colors yellow // 检查yellow是否在colors中 (integer) 0 // 集合colors不包含yellow \u0026gt; scard colors // 查看集合colors的基数(元素个数) (integer) 4 \u0026gt; spop colors // 随机从集合colors中移除一个元素 \u0026quot;black\u0026quot; \u0026gt; srem set1 black // 从集合colors中删除一个不存在的元素 (integer) 0 \u0026gt; srem colors white // 从集合colors中删除一个存在的元素 (integer) 1 \u0026gt; smembers colors 1) \u0026quot;blue\u0026quot; 2) \u0026quot;red\u0026quot; 下面是集合运算：\n\u0026gt; sadd set2 a b c red blue gold (integer) 6 \u0026gt; smembers set2 1) \u0026quot;c\u0026quot; 2) \u0026quot;a\u0026quot; 3) \u0026quot;gold\u0026quot; 4) \u0026quot;b\u0026quot; 5) \u0026quot;red\u0026quot; 6) \u0026quot;blue\u0026quot; \u0026gt; sunion colors set2 // colors ∩ set2 1) \u0026quot;gold\u0026quot; 2) \u0026quot;red\u0026quot; 3) \u0026quot;b\u0026quot; 4) \u0026quot;a\u0026quot; 5) \u0026quot;blue\u0026quot; 6) \u0026quot;c\u0026quot; \u0026gt; sinter colors set2 // colors ∪ set2 1) \u0026quot;blue\u0026quot; 2) \u0026quot;red\u0026quot; \u0026gt; sdiff set2 colors // set - colors 1) \u0026quot;gold\u0026quot; 2) \u0026quot;b\u0026quot; 3) \u0026quot;c\u0026quot; 4) \u0026quot;a\u0026quot; \u0026gt; sadd set3 f (integer) 1 \u0026gt; sdiff set2 colors set3 // set2 - colors - set3 1) \u0026quot;gold\u0026quot; 2) \u0026quot;b\u0026quot; 3) \u0026quot;c\u0026quot; 4) \u0026quot;a\u0026quot; Sorted sets Redis中的有序集合(sorted set)和集合(set)类似，存放不重复的字符串。不过，有序集合中的每个元素都有一个对应的浮点类型的分数(score)，这个分数用来维持集合的有序性。由于是有序的，有序集合又具有哈希表的快速访问的优势。\n考虑有序集合中的两个元素A和B，有序集合的有序性基于以下两点：\n 如果A.score \u0026gt; B.score，那么：A \u0026gt; B 如果A.socre = B.score，A \u0026gt; B的前提是A的字典序大于B。因为集合的元素都是唯一的，所以A和B的内容不可能相同  下面是一些例子：\n\u0026gt; zadd z1 2 a -1 b // 向有序集合z1中添加a(score=2)和b(score=-1)， ZADD还可以用来更新元素对应的分数 (integer) 2 \u0026gt; zrange z1 0 -1 // 查看z1内容(正序)，因为b.score \u0026lt; a.score，所以b在前面 1) \u0026quot;b\u0026quot; 2) \u0026quot;a\u0026quot; \u0026gt; zrange z1 0 -1 withscores // 查看z1内容及对应分数 1) \u0026quot;b\u0026quot; 2) \u0026quot;-1\u0026quot; 3) \u0026quot;a\u0026quot; 4) \u0026quot;2\u0026quot; \u0026gt; zrevrange z1 0 -1 // 查看z1内容(逆序) 1) \u0026quot;a\u0026quot; 2) \u0026quot;b\u0026quot; \u0026gt; zrangebyscore z1 0 inf // 查看z1中分数在[0, inf)内的内容 1) \u0026quot;a\u0026quot; \u0026gt; zrank z1 b // 查看z1中内容b的排名 (integer) 0 \u0026gt; zremrangebyscore z1 -inf -1 // 移除z1中分数位于(-inf,1]的所有元素 (integer) 1 \u0026gt; zrange z1 0 -1 1) \u0026quot;a\u0026quot; 有序集合还支持很多的命令，比如：ZPOPMAX、ZRANGEBYLEX、**ZUNIONSTORE**等等。\nBitmaps 严格地说，Bitmap并不是一种新的数据类型，而是基于string的一种数据类型，它提供了一些基于比特位的操作。由于当前Redis中的string最大支持到512MB，因此位操作能够使用的比特位数最大为$$2^{32}$$，约为4.295亿。\nBit fields Bit field是一种数据结构，它把数据以比特位为单元进行存储，并允许对单个比特位或一组比特位进行操作。\n**BITFIELD**会将string作为位数组对待，支持一次操作一个或多个位。可以理解为：我们可以在string上操作一个或多个可变长度的整数。其完整语法如下，包括三个子命令和三种溢出策略： BITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL]\n这里的type必须是整数类型，它由两部分组成：符号前缀+整数的宽度。前缀由分为两种：i表示有符号(signed)整数，而u表示无符号(unsigned)整数。例如：u8表示宽度为8位的无符号整数，而i5表示宽度为5位的有符号整数。对于无符号整数，最大支持到64位，而对于有符号整数，最大支持到63位。\noffset有两种形式：\n 不带前缀#，采用从0开始的offset去确定bit field的位置 带前缀#，采用整数的宽度乘以offset去确定bit field的位置  例如：\n\u0026gt; bitfield myfield set u8 0 42 1) (integer) 0 \u0026gt; bitfield myfield get u8 0 1) (integer) 42 \u0026gt; type myfield string \u0026gt; object encoding myfield \u0026quot;raw\u0026quot; \u0026gt; bitfield myfield set u8 #1 10 1) (integer) 0 \u0026gt; bitfield myfield get u8 #1 get u8 8 1) (integer) 10 2) (integer) 10 \u0026gt; get myfield \u0026quot;*\\n\u0026quot; // 在ASCII表中，42表示'*'，而10表示'\\n'。 **BITFIELD**支持的三个子命令分别为：\n GET \u0026lt;type\u0026gt; \u0026lt;offset\u0026gt;：返回给定的bit field SET \u0026lt;type\u0026gt; \u0026lt;offset\u0026gt; \u0026lt;value\u0026gt;：设置bit field的值并返回其上的旧值 INCRBY \u0026lt;type\u0026gt; \u0026lt;offset\u0026gt; \u0026lt;increment\u0026gt;：在给定的bit field上执行增减操作，并返回新值  对于 **INCRBY**子命令，还可以指定溢出策略，有三种可选的溢出策略：\n WRAP：wrap around，回绕。例如，对于i8类型的127，加一会得到-128，而对于u7类型的128，加一会得到0 SAT：saturation arithmetic，饱和计算。例如，例如，对于i8类型的127，加一依然是128 FAIL：当发生溢出时，不采取任何运算，而是返回NULL WRAP是默认的溢出策略。  Bit arrays SETBIT key offset value给指定比特位设置值，然后返回该位置上原来的值，如果key不存在，则会先创建一个新的string，这个string会自动扩容以保证offset的有效性。企图使用 **SETBIT**设置0和1以外的值会导致错误。 **GETBIT key offset**获取offset位置的值，若给定offset超出了当前string的长度或key不存在，Redis都会返回0。例如：\n\u0026gt; setbit bits 2 1 (integer) 0 \u0026gt; getbit bits 2 (integer) 1 \u0026gt; getbit bits 1 (integer) 0 有三种命令可以操作一组比特位：\n BITOP operation destkey key [key ...]：进行字符串之间的按位与(AND)、按位或(OR)、按位异或(XOR)以及按位取反(NOT)操作 BITCOUNT key [start end]：进行计数操作，返回设置为1的比特位的个数 BITPOS key bit [start end]：寻找给定bit(0或1)出现的第一个位置  HyperLogLogs Redis使用HyperLogLog进行计数，这个算法是基于统计的。Redis的实现中，标准误差只有1%，并且在最坏的情况下只需要消耗 12KB 的内存。HyperLogLog在技术上是一种不同的数据结构，但也是基于string实现的。\n我们使用 SADD 向集合中添加元素，类似的，我们也可以使用 PFADD 向HyperLogLog中添加元素。实际上，HyperLogLog并不存储我们添加的元素，只是更新内部状态。\n\u0026gt; pfadd hll a b c d // 向hll中加入四个元素 (integer) 1 \u0026gt; type hll // 查看hll类型 string // HyperLogLog实际为string \u0026gt; pfcount hll // 对hll进行计数 (integer) 4 \u0026gt; pfadd hll1 a b c d (integer) 1 \u0026gt; pfadd hll2 c d e f (integer) 1 \u0026gt; pfmerge hll3 hll1 hll2 // 合并hll1和hll2到hll3 OK \u0026gt; pfcount hll3 (integer) 6 Geospatial indexes Redis在3.2.0版本中加入了地理空间(geospatial)这一数据类型，并支持半径查询功能。一个具体的位置信息由三元组(longtitude, latitude, member)确定。对于每一个\u0026lt;latitude, longitude\u0026gt;对，Redis都会计算出一个GeoHash(Redis中的GeoHash是一个52位的整数)。当我们使用 **GEOADD key longitude latitude member [longitude latitude member ...]**向一个key添加地理数据时，数据会被存储为有序集合(member作为有序集合中的member，GeoHash作为有序集合中的score)，这么做为半径查询 **GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]**和 **GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]**提供了支持。有序集合上的所有命令也能用于geospatial类型。\n下面是一些例子：\n\u0026gt; geoadd municipalities 116.4551113869 39.6733986505 beijing 121.6406041505 30.8267595167 shanghai 106.6992091675 29.3055601981 chongqing // 添加3个地理空间数据 (integer) 3 \u0026gt; zrange municipalities 0 -1 withscores // 因为GeoHash使用有序集合存储，所以可以使用zrange命令 1) \u0026quot;chongqing\u0026quot; 2) \u0026quot;4026043269574572\u0026quot; 3) \u0026quot;shanghai\u0026quot; 4) \u0026quot;4054740844391077\u0026quot; 5) \u0026quot;beijing\u0026quot; 6) \u0026quot;4069148402401385\u0026quot; \u0026gt; type municipalities zset \u0026gt; geohash municipalities beijing shanghai // 查看beijing和chongqing的GeoHash 1) \u0026quot;wx4cdn242c0\u0026quot; 2) \u0026quot;wtqrrgzfzs0\u0026quot; \u0026gt; geopos municipalities chongqing // 查看chongqing的地理空间数据 1) 1) \u0026quot;106.69921070337295532\u0026quot; 2) \u0026quot;29.30556015923176716\u0026quot; \u0026gt; geodist municipalities beijing chongqing // 查看beijing和chongqing直接的距离(单位为米) \u0026quot;1457336.8906\u0026quot; \u0026gt; georadius municipalities 116 40 1000 km // 查看以经度116、纬度40为中心，1000km为半径内的所有位置 1) \u0026quot;beijing\u0026quot; \u0026gt; georadiusbymember municipalities chongqing 1500 km // 查看以chongqing为中心、1500km为半径内的所有位置 1) \u0026quot;chongqing\u0026quot; 2) \u0026quot;shanghai\u0026quot; 3) \u0026quot;beijing\u0026quot; Streams 见Redis Streams。\n参考资料  An introduction to Redis data types and abstractions. Bit field.  ","href":"/posts/redis/redis_datastructures/","title":"Redis数据结构"},{"content":"Redis通过 PUBLISH、**SUBSCRIBE**等命令实现了发布-订阅模式，这个功能提供了两种信息机制：simple syndication和pattern syndication。值得注意的是：发布-订阅不保证完成消息的交付。当消息发布时到某个channel上时，只有那些订阅了该channel并且连接上了的客户端才会收到消息。消息一旦发布，就会被丢弃，只有那些当前订阅了的客户端才会收到该消息。也就是说，客户端只有在消息发布前订阅，才能收到该消息。。\nSimple syndication 在simple syndication下，Redis使用 **PUBLISH channel message**发布message到指定channel，然后返回收到该消息的客户端数量。\n客户端使用 SUBSCRIBE channel [channel ...]来监听指定的channel，此后客户端不能再发出除 SUBSCRIBE、PSUBSCRIBE、UNSUBSCRIBE、PUNSUBSCRIBE、**PING**和 **QUIT**以外的命令。对于redis-cli来说，一旦处于订阅模式，就不再接收任何命令并且只能通过Ctrl-C来退出。\n若客户端不想再监听某个channel上的消息，可以执行 **UNSUBSCRIBE [channel [channel ...]]**来停止对指定channel上消息的订阅。若没有给出channel参数，客户端将停止对之前订阅的所有channel的监听。\n下面是一个例子： 首先client-2订阅了ch1上的消息：\nclient-2:6379\u0026gt; subscribe ch1 Reading messages... (press Ctrl-C to quit) 1) \u0026quot;subscribe\u0026quot; 2) \u0026quot;ch1\u0026quot; 3) (integer) 1 然后client-1往ch1上发布消息：\nclient-1:6379\u0026gt; publish ch1 \u0026quot;hello\u0026quot; (integer) 1 // 有一个订阅者收到了消息 收到消息的正是client-2，client-2上应该可以看到这样的信息：\nclient-2:6379\u0026gt; subscribe ch1 Reading messages... (press Ctrl-C to quit) 1) \u0026quot;subscribe\u0026quot; 2) \u0026quot;ch1\u0026quot; 3) (integer) 1 1) \u0026quot;message\u0026quot; 2) \u0026quot;ch1\u0026quot; 3) \u0026quot;hello\u0026quot; 消息的格式 Redis中，channel的订阅者接收到的消息是一个由三个元素组成的数组。其中第一个元素是消息的类型，后两个元素的内容随消息的类型而变化。有三种类型的消息：\n subscribe：成功订阅某个channel，第二个元素为被订阅channel的名字，第三个元素为当前订阅的channel数量 unsubscribe：成功取消对某个channel的订阅，第二个元素为取消订阅的channel的名字，第三个元素为当前订阅的channel数量(为0表示不再订阅任何channel) message：收到了发布的消息，第二个元素为这个消息的来源(channel)，第三个元素为消息的内容  Pattern syndication 有时候我们想接收所有的消息，这个时候把所有的channel都订阅就行了。但有时候我们可能希望对消息的来源进行过滤，只接收满足过滤条件的channel上的信息。这个时候就可以使用 **PSUBSCRIBE pattern [pattern ...]**来订阅满足给定模式的channel了。Redis支持glob-style的pattern：\n h?llo表示订阅hello、hillo、hallo等 h*llo表示订阅hllo、hello、haello等 h[ae]llo表示订阅hallo和hello，而不是hxllo或hbllo等  **PUNSUBSCRIBE [pattern [pattern]]**表示不再订阅满足给定模式的channel。若没有给出pattern参数，客户端将停止对之前订阅的所有channel的监听。\n接着上一部分举例，新开一个client-3订阅ch?模式的channel：\nclient-3:6379\u0026gt; psubscribe ch? Reading messages... (press Ctrl-C to quit) 1) \u0026quot;psubscribe\u0026quot; // 订阅pattern成功 2) \u0026quot;ch?\u0026quot; // 订阅的pattern 3) (integer) 1 // 订阅该pattern的订阅者数量 然后client-1往ch3上发布消息：\nclient-1:6379\u0026gt; publish ch3 \u0026quot;hello, ch3\u0026quot; (integer) 1 // 有一个客户端收到了消息 收到消息的正是client-3，由于client-2只订阅了ch1，因此它不会收到消息。client-3上应该可以看到这样的信息：\nclient-3:6379\u0026gt; psubscribe ch? Reading messages... (press Ctrl-C to quit) 1) \u0026quot;psubscribe\u0026quot; 2) \u0026quot;ch?\u0026quot; 3) (integer) 1 1) \u0026quot;pmessage\u0026quot; // 收到消息 2) \u0026quot;ch?\u0026quot; // pattern 3) \u0026quot;ch3\u0026quot; // 消息对应的channel 4) \u0026quot;hello, ch3\u0026quot; // 消息内容 消息的格式 使用 **PSUBSCRIBE**之后，订阅者接收到的消息和 **SUBSCRIBE**大体类似，但消息内容的含义稍有变化，具体的见上面的注释。\n查看Pub/Sub系统状态 **PUBSUB subcommand [argument [argumeng ...]]命令允许我们查看发布-订阅系统的内部状态，共支持3个子命令：\n PUBSUB CHANNELS [pattern]：查看当前处于活动状态的channel(有至少一个订阅者的channel)。若未提供pattern参数，将返回所有的处于活动状态的channel，否则只返回满足pattern并处于活动状态的channel PUBSUB NUMSUB [channel-1 ... channel-N]：查看给定channel的订阅者数量(不包括订阅pattern的订阅者)，返回结果的格式为channel,count,channel,count,...,。若未提供channel参数，将返回一个空列表 PUBSUB NUMPAT：返回被订阅pattern的数量  接着上面的例子，在client-1上执行 PUBSUB，查看系统状态：\nclient-1:6379\u0026gt; pubsub channels // 不带pattern参数 1) \u0026quot;ch1\u0026quot; client-1:6379\u0026gt; pubsub channels ch? 1) \u0026quot;ch1\u0026quot; client-1:6379\u0026gt; pubsub numsub ch1 // 查看ch1上订阅者的数量 1) \u0026quot;ch1\u0026quot; 2) (integer) 1 client-1:6379\u0026gt; pubsub numsub // 不带参数 (empty list or set) client-1:6379\u0026gt; pubsub numpat (integer) 1 // 当前被订阅的pattern数量为1 参考资料  Pub/Sub. Publish–subscribe pattern.  ","href":"/posts/redis/publish_subscribe/","title":"Publish/Subscribe"},{"content":"Redis支持在服务端使用Lua解释器执行Lua脚本。Redis本身已经提供了非常多的命令，而Lua脚本可以帮助我们使用Lua提供的语言特性来组织这些命令，以达到更好的效果。总的来说，使用Lua脚本带来的好处有：\n 降低网络开销。客户端每次只能操作一个键，客户端每发出一个命令，都会引发一个完整请求-响应过程，当需要发送大量指令时，这会引起巨大的网络开销。因为Lua脚本是直接在在Redis服务端执行的，将多个命令放在同一个Lua脚本中执行或在同一个Lua脚本中操作多个键可以减少网络传输次数，进而降低网络开销 原子操作。Redis会将整个Lua脚本作为一个整体以原子的方式执行，就像事务一样 可复用性。Lua脚本就像是数据库中的存储程序一样，支持传入参数，反复使用。Redis还会缓存已执行的脚本，这意味着其它的客户端可以直接使用已缓存的脚本  执行脚本 执行Lua脚本的命令为 **EVAL script numkeys key [key ...] arg [arg ...]**和 EVALSHA sha1 numkeys key [key ...] arg [arg ...]。\n**EVAL**具有很多参数：script代表待执行的Lua脚本，numkeys表示键的数量.，接下来是键名(可以通过全局变量KEYS访问)和脚本可能需要用到的参数(可以通过全局变量ARGV访问)。\n\u0026gt; hset hash-key f1 hello f2 Redis (integer) 2 \u0026gt; eval \u0026quot;return redis.call('HGET', KEYS[1], ARGV[1])\u0026quot; 1 hash-key f2 \u0026quot;Redis\u0026quot; \u0026gt; eval \u0026quot;return redis.call('HGET', 'hash-key', 'f2')\u0026quot; 0 \u0026quot;Redis\u0026quot; 在上面这段脚本里，创建了两个预定义的数组：KEYS存放传入所有键名，而ARGV存放传入的所有参数。最后一条命令硬编码参数，虽然执行结果和倒数第二条一致，但灵活性非常低。需要注意的是：Lua中的数组下标是基于1的。\nLua和Redis之间的数据类型转换 Redis to Lua：\n   Redis Value Lua Value     Redis Integer Number   Redis bulk reply String   Redis multi bulk reply Table (with other types nested)   Redis status reply Table with \u0026ldquo;ok\u0026rdquo; field containing status   Redis error Table with \u0026ldquo;err\u0026rdquo; field containing error   Redis nil bulk reply and nil multi reply False (boolean type)    Lua to Redis：\n   Lua Value Redis Value     Number Redis Integer   String Redis bulk reply   Table (array) Redis multi bulk reply   Table with \u0026ldquo;ok\u0026rdquo; field Redis status reply   Table with \u0026ldquo;err\u0026rdquo; field Redis error reply   False (boolean type) Redis nil bulk reply   True (boolean type) Redis integer reply with value of 1    还有有三条重要的规则需要注意：\n Lua中没有integer和float之分，取而代之的是number类型。Redis会将Lua中的number转为integer，对于浮点数来说，其小数部分就会丢失。如果需要保留浮点数的小数部分，我们应该采用string来进行存储和查询操作 Lua数组以nil作为结束标志，有效数组元素仅位于第一个nil出现之前。所以Redis在转换Lua数组时，遇到nil就会停止 当Lua数组包含键(和它们的值)时，转换到Redis的数据类型时不会被包含进去  管理脚本 缓存脚本 对于一个Lua脚本，客户端首先通过网络将它发送给Redis，Redis在执行脚本前需要解析(parse)该脚本，脚本执行完成后，执行结果会被传回给客户端。脚本的解析是需要一定开销的，所以Redis会维护一个已编译脚本的缓存。当我们需要反复执行某个脚本时，我们可以使用 **SCRIPT LOAD script**命令，它会解析脚本，将其载入缓存并返回脚本的sha1摘要值，随后可以使用 **EVALSHA**命令通过脚本的sha1摘要调用这个脚本。\n\u0026gt; set s1 \u0026quot;hello\u0026quot; OK \u0026gt; script load \u0026quot;local val=redis.call('GET', KEYS[1]) return val\u0026quot; \u0026quot;dd47ad79bb7b6d7d2b8e0607c344d134412e84e0\u0026quot; \u0026gt; evalsha dd47ad79bb7b6d7d2b8e0607c344d134412e84e0 1 s1 \u0026quot;hello\u0026quot; 只要我们没有手动执行 SCRIPT FLUSH(清空缓存中的所有脚本)或重启Redis，脚本就会一直位于缓存当中。**SCRIPT EXISTS sha1 [sha1 ...]**用于检测脚本是否已在缓存当中。\n\u0026gt; script exists dd47ad79bb7b6d7d2b8e0607c344d134412e84e0 1) (integer) 1 // 缓存中存在该脚本 其它命令 有时候，我们的脚本可能有bug或者由于某种原因导致其不能在预期的时间内执行完，我们可以使用 **SCRIPT KILL**来终止当前正在执行的脚本。 **SCRIPT DEBUG YES|SYNC|NO**可以唤起LDB(Lua Debuger)，进一步帮助我们进行脚本调试。\n当脚本执行时间过长 当一个脚本开始执行，在一段时间内(默认为5秒)，Redis是不会接收其它命令的。 当执行时间超过这个阈值，Redis不会主动终止这个脚本，因为Redis保证以原子的方式执行Lua脚本，主动终止脚本的执行可能会导致数据出现问题。Redis会采取以下方式应对这种情况：\n Redis会在日志里面记录脚本执行时间过长 Redis会开始接收其它命令。对于脚本执行完之前接收到的这些命令，Redis并不会执行它们，而是直接用一个busy error进行响应 如果脚本是只读的，也就是说脚本没有进行任何写操作，可以直接用 **SCRIPT KILL**安全的终止这个脚本 如果脚本已经进行过写入操作，那就只能通过 **SHUTDOWN NOSAVE**命令来停止服务器，并放弃所有的修改。这些修改包括：当前执行的Lua脚本造成的修改和最近一次刷新数据到磁盘之后的其它修改。  为了防止脚本的执行时间过长，我们需要尽可能地考虑脚本地执行时间，争取使用最短的时间完成相同的工作。其次，脚本其实定义了事务边界，知道这一点，我们可以将一个大脚本拆分为多个可以被组织在一起的原子单元。\n参考资料  Redis Lua scripting.  ","href":"/posts/redis/lua_scripting/","title":"Redis Lua 脚本"},{"content":"Jedis是一个Java编写的Redis客户端，提供了完整的Redis API。Redis客户端通常需要具备三种能力：管理Redis连接、实现Redis序列化协议RESP(REdis Serialization Protocol)和实现可供编程人员调用的Redis API(GET、SET等)。\n除了Jedis，还有两个Java编写的Redis客户端非常流行，它们是Lettuce和Redisson。\n连接到Redis服务 Jedis jedis = new Jedis(\u0026#34;127.0.0.1\u0026#34;, 6379); jedis.set(\u0026#34;hello\u0026#34;, \u0026#34;Redis\u0026#34;); assertThat(result, is(\u0026#34;OK\u0026#34;)); String value = jedis.get(\u0026#34;hello\u0026#34;); assertThat(value, is(\u0026#34;Redis\u0026#34;)); jedis.close(); // 关闭TCP连接，防止发生连接泄漏 Jedis API的使用和redis-cli非常类似，例如redis-cli中的 SET 和 **GET**在Jedis中就是jedis.set(...)和jedis.get(...)。\n在多线程环境下使用Jedis Jedis实例会维护一个到Redis的TCP连接，它不是线程安全的。因此，为了避免遇到未知问题，我们不应该让同一个Jedis实例被多个线程共享。为每一个线程都创建一个Jedis实例也不是一个好主意，因为这会创建大量的Socket连接，也可能导致未知问题。为了避免这些问题，我们可以在多线程环境下使用JedisPool，它是一个线程安全的网络连接池。使用JedisPool既可以帮助我们克服非线程安全引发的未知问题，同时，由于复用了连接，应用的性能也会有所提高。为了使用JedisPool，我们需要先初始化它：\nJedisPool jedisPool = new JedisPool(new JedisPoolConfig(), \u0026#34;127.0.0.1\u0026#34;, 6379); JedisPool基于Apache Commons Pool 2实现。JedisPoolConfig也继承于GenericObjectPoolConfig，它包含了一些非常有用的连接池默认值。我们可以这么使用JedisPool:\ntry (Jedis jedis = jedisPool.getResource()) { String result = jedis.set(\u0026#34;hello\u0026#34;, \u0026#34;Redis\u0026#34;); assertThat(result, is(\u0026#34;OK\u0026#34;)); String value = jedis.get(\u0026#34;hello\u0026#34;); assertThat(value, is(\u0026#34;Redis\u0026#34;)); } jedisPool.close(); 选择正确的Java类去连接Redis    Thread-safe Deployment Type Connection     No Single Redis Jedis   No Redis Enterprise Jedis   Yes Single Redis JedisPool   Yes Redis Enterprise JedisPool   Yes Redis Sentinel JedisSentinelPool   Yes RedisCluster JedisCluster    Java-Redis类型映射    Redis Type Java Type     string String   list List   set Set   hash Map\u0026lt;String, String\u0026gt;   float Double   integer Long    Pipelining pipeline内的所有命令都是独立的，它们不是一个原子的整体。\nPipeline p = jedis.pipelined(); Response\u0026lt;Long\u0026gt; hsetResponse = p.hset(statusKey, \u0026#34;available\u0026#34;, \u0026#34;true\u0026#34;); Response\u0026lt;Long\u0026gt; expireResponse = p.expire(statusKey, 1000); Response\u0026lt;Long\u0026gt; saddResponse = p.sadd(availableKey, \u0026#34;1\u0026#34;); p.sync(); // 通过pipeline发送所有的命令并获取返回值 assertThat(hsetResponse.get(), is(1L)); assertThat(expireResponse.get(), is(1L)); assertThat(saddResponse.get(), is(1L)); Transactions 很多客户端用pipeline实现事务，这么做既高效，又能保证原子性。\nTransaction t = jedis.multi(); Response\u0026lt;Long\u0026gt; hsetResponse = t.hset(statusKey, \u0026#34;available\u0026#34;, \u0026#34;true\u0026#34;); Response\u0026lt;Long\u0026gt; expireResponse = t.expire(statusKey, 1000); Response\u0026lt;Long\u0026gt; saddResponse = t.sadd(availableKey, \u0026#34;1\u0026#34;); t.exec(); assertThat(hsetResponse.get(), is(1L)); assertThat(expireResponse.get(), is(1L)); assertThat(saddResponse.get(), is(1L)); 参考资料  Jedis wiki.  ","href":"/posts/redis/jedis/","title":"Jedis"},{"content":" 本文中绝大部分内容和图片都来自 High Performance Java Persistence: Get the most out of your persistence layer。\n 事务（Transaction） 是由一组读写操作组成的一个不可分割的执行单元，这组操作要么全部成功，要么全部失败。根据定义，一个事务必须是原子的（atomic）、一致的（consistent）、隔离的（isolated）和持久的（durable），我们通常把这四大特性称为 ACID。\n原子性 多个操作可以构成一个逻辑工作单元。当且仅当工作单元内的所有操作都成功，这个工作单元才成功，若有操作执行失败，则必须回滚所有已经执行的操作并恢复到未执行时的状态，这就是事务的 原子性（Atomicity）。简而言之，原子性就是“all-or-nothing”。\n一致性 一致性和事务状态的改变紧密相关的，当事务涉及修改操作时，可以将其看作是状态迁移，数据库会从一个有效状态迁移到另一个有效状态。一致性(Consistency） 就是指系统从一个正确的状态迁移到另一个正确的状态。\n那么该如何理解“正确的状态”呢？我们在设计数据库时，通常会规定字段的类型、长度以及是否为空等，这些就是我们定义的规则。此外，我们可能还会给列加上一些约束（constraint），比如主键约束、外键约束、唯一约束和各种自定义约束。只有当数据满足所有的规则和约束时，它才是 有效的，它的状态才是正确的。一致性保证只有有效的数据才会被写入数据库中，若数据在事务执行后处于无效状态（即违反规则或约束），整个事务都会回滚，所有的修改都会被撤销，数据库会回到之前的状态。\n一致性是维护数据完整性（integrity）的关键。\n隔离性 事务的 隔离性(Isolation） 定义的是一个事务产生的影响对其它事务的可见程度。为了提高吞吐量，数据库系统通常会允许多个连接对数据进行并发访问。为了保证数据的完整性（integrity），数据库系统必须对这些并发的读写操作进行控制。\n若几个事务不管以何种顺序执行，最终系统的状态都是一样的，则这几个事务是 可串行化的（serilizable）。如果几个事务并发执行，即使每个事务都能确保一致性和原子性，它们的操作也可能会以我们意料之外的某种顺序交叉执行，最终导致不一致状态。 只有当这些事务是可串行化的，最终的结果才会一致。可串行化是最高的隔离级别，但是此时数据库的性能可能会很低，因为事务都是串行的了，并发的优势荡然无存。\n数据库中允许多个不同的隔离级别，这些隔离级别实际上是在性能与数据完整性之间做权衡的结果。隔离级别越低，并发度越高，数据的完整性就越差，反之，隔离级别越高，并发度越低，数据的完整性就越好。可串行化是唯一一个没有损坏数据完整性的隔离级别。\n并发控制 并发访问很容易出现数据冲突，通常有两种解决数据冲突的基本策略：\n 冲突避免：使用锁控制共享资源，比如两阶段锁（2PL, two-phase locking） 冲突检测：并发性更好，但可能导致数据异常，比如多版本并发控制（MVCC, Multi-Version Concurrency Control）  两阶段锁 为了减少访问共享资源时发生的冲突，关系型数据库支持多粒度锁。数据库对象天生就是层次结构的，一个逻辑表空间可能被映射到多个数据库文件，而一个数据库文件又由多个数据页构成，每页又包含很多行…… 因此，不同的数据库对象都可以获得锁。\n低级别的锁（比如：行级锁）粒度较小，减小了竞争的可能，可以提供更好的并发控制。然而，每个锁都是会消耗资源的，维持大量的低级别锁，也会带来可观的资源开销。因此，数据库可能会决定用一个稍微高级别的锁代替这些低级别的锁，这就是锁升级。这就是可并发性和资源消耗之间的一个权衡点。\n不同数据库的锁都有自己的层级结构，最常见的锁有两种：\n 共享锁（读锁）：加锁后，只允许并发读，不允许写 互斥锁（写锁）：加锁后，既不允许写也不允许读  两阶段锁保证了可串行性，协议要求每个事务分为两个阶段：\n 增长阶段：只可获取锁，不可释放锁 缩减阶段：只可释放锁，不可获取锁 最初，事务处于增长阶段，事务可以根据需要获取锁。一旦事务开始释放锁，便进入缩减阶段，并且不能再发出加锁请求。  多版本并发控制 最初，所有的数据库系统都采用两阶段锁实现可串行化的事务。后来，很多数据库厂商改用了多版本并发控制(MVCC）。\n尽管锁可以提供可串行化的事务调度，但是锁竞争还是会对事务的响应时间和可扩展性造成影响。为了克服这个缺点，数据库厂商们选择了乐观并发控制机制。如果说两阶段锁使用的是冲突避免策略，那么 MVVC 使用的就是冲突检测策略。\n为了防止阻塞，数据库进行版本控制，然后可以重建数据库记录的早期版本，未提交的改变可能对后来的用户不可见。没有了锁的协助，可串行化调度的实现变得更加困难，数据库引擎必须分析当前的交叉操作并检测影响串行化的异常操作。\nMVCC 是通过保存数据在某个时间点的快照来实现的。也就是说，不管执行多长时间，每个事务看到的数据都是一致的。事务开始时间的不同，即使是同一时刻的同一张表，不同事物看到的数据也可能是不一样的。\n数据库中允许多个不同的隔离级别，这些隔离级别实际上是在性能与数据完整性之间做权衡的结果。隔离级别越低，并发度越高，数据的完整性就越差，反之，隔离级别越高，并发度越低，数据的完整性就越好。可串行化是唯一一个没有损坏数据完整性的隔离级别。\n数据库异象 严格保证数据完整性的代价可能会非常高，因此有必要降低对事务串行化的保证程度，使用多个隔离级别，但这会导致数据库异象（Phenomena）的出现。\nSQL-92 标准提出了三种异象：\n 脏读（dirty read） 不可重复读（non-repeatable read，也叫 fuzzy read） 幻读（phantom read）  现实中，还有其它的异象：\n 脏写（dirty write） 读偏（read skew） 写偏（write skew） 更新丢失（loss update）  脏写 当两个并发事务被允许同时修改同一行时，脏写就发生了。\n发生脏写时，若两个事务分别提交或回滚，则事务的结果是无法确定的。如果两个事务都提交，后提交的事务会覆盖先提交的事务的操作结果，造成更新丢失。如果第一个事务选择回滚，数据库引擎则不得不进行抉择：\n 将数据恢复到第一个事务做出改变之前的那一个版本（上图中即：title:'Transactions'），这回导致第二个事务未提交的变更被覆盖。 它知道数据有一个较新的版本（上图中即：title:'BASE'），但如果第二个事务决定回滚，回滚的结果将是第一个事务还未提交的变更（上图中即：title:'ACID'）。  如果数据库引擎不能阻止脏写，就不能保证回滚，因为如果没有可靠的回滚机制，原子性是无法实现的。因此，所有的数据库都必须阻止脏写。\n最低的隔离级别——读取未提交 就可以避免脏写。\n脏读 解决脏写之后，我们面临的问题是脏读。若一个事务可以读取另一个事务未提交的改变，就会发生脏读。也就是说，读取未提交这一隔离级别不能防止脏读。使用未提交的数据进行决策是有风险的，因为未提交的数据可能回滚。\n为了防止脏读，数据库引擎必须防止其他的并发事务读取当前事务未提交的改变。一种防止脏读的简单方法是对未提交的记录加锁，但如果获得锁的事务运行时间过长的话，其它事务只有等到锁被释放后才能读取到相关的记录。所以实践中一般不采用这种方式，因为锁会引起竞争，而竞争会对可伸缩性。既然 undo log 已经保存了每个未提交记录的之前版本，那么数据库引擎就可以在其它并发事务查询中使用它去恢复之前版本的数据。\n隔离级别——读取已提交可以防止脏读。读取未提交的使用场景很少（仅适用于允许脏读的要求没那么严格的查询），所以在实际应用中，读取已提交通常是最低的隔离级别。\n不可重复读 解决了脏读，我们面临的问题是不可重复读。若一个事务要多次读取数据库中的数据 A 又没有给 A 加 共享锁 时，其它事务可能在第一个事务结束之前修改掉 A 的值，最终导致第一个事务多次读取到的内容不一致，这就是不可重复读。\n当事务基于第一次读取到的值进行决策时，就可能遇到问题。因为这条记录可能在决策期间被修改，可以认为事务基于错误的数据做出了决策。\n现在很多数据库都转向了 MVCC 模型，共享锁不再是防止不可重复读的唯一途径了。默认情况下，可重复读 和 可串行化 这两个隔离级别都可以避免这个问题。在 读取已提交 这一隔离级别下，显式获取共享锁（比如：SELECT FOR SHARE）也可以避免不可重复读。\n可重复读的含义是：只允许读取已提交的数据，而且在一个事务两次读取同一个数据项期间，不允许其它事务修改这个数据项.\n一些 ORM 框架（比如 JPA/Hibernate）提供了应用级的可重复读。所有检索出来的实体的第一个快照都会被缓存在当前运行的持久化上下文（Persistence Context）中，后续查询操作都会返回已经缓存的对象。\n幻读 当我们解决了不可重复读问题之后，还会碰到幻读的情况。 幻读发生在正在执行的事务 T1 在读取一个范围内的数据时（包括统计查询）没有加锁，另外一个事务 T2 执行了和之前范围有交集的插入操作。比如 T1 在 T2 之前读到的 A 的值是 3，但 T2 执行时有交集，插入了新的数据，这个时候 A 变成了 4，如果 T1 再次读取 A 的话，就会发现 A 变成了 4 而不是之前的 3，这就是幻读。\n不可重复读和幻读的区别：不可重复读的重点是数据的修改，比如多次读取同一条记录发现其中某些列被修改；而幻读的重点在于数据的插入或删除，比如多次读取一个集合内的数据，会发现该集合内的数据变多了或者变少了。\n隔离级别——可串行化可以避免幻读。但是，一旦事务完全串行化，并发性就非常弱了。需要注意的是：行锁解决不了幻读，因为即使锁住所有的记录，也阻止不了插入新的数据。MySQL 的 InnoDB 引擎引入了 间隙锁（Gap Lock），它能锁住记录之间的“间隙”，解决幻读。\n读偏 读偏发生在当两张数据表之间有一个总的约束的时候。比如表 A 和表 B 必须同步改变，当事务 T1 在读取表 A 的数据项 a 和表 B 的数据项 b 时，事务 T2 插进来更新了数据项 a 和 b，最终导致事务 T1 读到了 a 的旧值和 b 的新值，这就是读偏。\n为了解决读偏，事务 T1 在读取数据是可以 使用共享锁 防止事务 T2 更新数据，也可以在检测到数据违反约束时进行回滚。\n写偏 写偏和读偏类似。假设表 A 的数据项 a 和表 B 的数据项 b 在单个事务内必须同步改变，若允许写偏，则事务 T1 更新了表 A 的数据项 a，事务 T2 也更新了表 B 的数据项 b，T1 和 T2 两个事务独立的更新了这两条记录，最终打破了二者必须同时改变的约束。\n和不可重复读类似，有两种方式可以解决写偏：\n 第一个事务获取两个数据项上的共享锁，这能防止第二个事务更新这两个数据项。 数据库引擎在检测到第二个事务修改了相关记录之后，强制回滚第一个事务。  更新丢失 当事务 T1 读取某个数据项 a 然后修改 a 时，另一个事务 T2 却在事务 T1 读取 a 之后修改 a 之前修改了 a，这个时候事务 T2 所作的更新就丢失了，T1 根本察觉不到 T2 的存在。\n先分析下更新丢失的情况：首先，没有脏写，因为 T2 已经提交。其次，没有脏读，因为在写之后没有读操作。可重复读就可避免更新丢失问题。\n隔离级别 SQL 标准定义了 4 个隔离级别：\n READ UNCOMMITTED： 最低的隔离级别，允许读取未提交的数据。可能导致脏读、不可重复读或幻读。 READ COMMITTED：允许读取并发事务已经提交的数据。可以防止脏读，但不可重复读和幻读仍然有可能发生。 REPEATABLE READ：除非数据被事物本身修改，对同一记录的多次读取结果都是一致的。可以防止脏读和不可重复读，但可能发生幻读。 SERIALIZABLE：事务串行执行，可以防止脏读、不可重复读和幻读。  下表展示了各隔离级别及其能够防止的异象：\n    脏读(Dirty Read） 不可重复读(Non-repeatable Read） 幻读(Phantom Read）     读取未提交(Read Uncommitted） 是 是 是   读取已提交(Read Committed） 否 是 是   可重复读(Repeatable Read） 否 否 是   可串行化(Serializable） 否 否 否    MySQL 默认的隔离级别为可重复读，而 Oracle、SQL Server 和 PostgreSQL 默认的隔离级别为读取已提交。\n持久性 事务一旦完成，它对数据库的改变就必须是永久的，即使系统故障也不会对数据的改变造成影响，这就是 持久性(Durability）。\n参考资料  Vlad Mihalcea. High Performance Java Persistence: Get the most out of your persistence layer. 2015. Abaham Siberschatz, Henry F.Kort, S.Sudarshan. 数据库系统概念(第六版）. 杨冬青, 李红燕, 唐世渭, 译. 机械工业出版社, 2013.  ","href":"/posts/databases/database_transactions/","title":"事务"},{"content":"","href":"/tags/http/","title":"HTTP"},{"content":"","href":"/categories/networks/","title":"Networks"},{"content":"Web资源 Web资源（Web resource），或资源（resource）是一个非常宽泛的概念，它代表一个可以被识别的东西。WikiPedia是这么描述它的：\n A web resource, or simply resource, is any identifiable thing, whether digital, physical, or abstract.\n 在早期的Web中，资源就是可寻址的静态文档（documents）或文件（files）。随着Web的发展，资源变得越来越宽泛和抽象，现在的资源包括一切可以被识别的东西或实体。\nWeb资源保存在Web服务器上，通过 统一资源标识符（Uniform Resource Identifier, URI） 进行识别。\nURI  A Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource.\n RFC2396对组成了URI的三个单词进行了定义：\n Uniform: 规定统一的格式可方便处理多种不同类型的资源，而不用根据上下文 环境来识别资源指定的访问方式。另外，加入新增的协议方案（如 http: 或 ftp:）也更容易。 Resource: 资源的定义是“可标识的任何东西”。除了文档文件、图像或服务（例 如当天的天气预报）等能够区别于其他类型的，全都可作为资源。另 外，资源不仅可以是单一的，也可以是多数的集合体。 Identifier: 表示可标识的对象。也称为标识符。  简而言之，URI可以唯一标识并定位Web上的资源，它有URL和URN两种形式。\nURI的语法 为了保证一致性（uniformity），所有的URI都遵循了一个预定义的规则集。虽然如此，URI也可以通过 命名方案（naming scheme） 进行扩展。\nURI的通用语法定义如下，它由5部分组成：\nURI = scheme:[//authority]path[?query][#fragment] authority = [userinfo@]host[:port] 上面的语法定义也可以用语法树来表示：\nURI包括：\n 一个非空的方案（scheme）（后面跟着:）：scheme描述的是客户端在访问服务器并获取资源时需要使用哪种协议。常见的scheme如：http、ftp、mailto、https等。scheme对大小写不敏感，因此HTTP与http指的是同一个scheme。 一个可选的 权限（authority） 部分（以//开始），它包括：  一个可选的 用户信息（userinfo） 部分，userinfo包括一个用户名（username）和一个可选的密码（password）。用户名与密码之间用:分隔，即username:password，由于安全原因，这一部分通常被弃用。userinfo后面会跟着一个@。 一个 主机（host） 部分：表示宿主服务器的主机名或IP地址（IPv4地址必须采用点分十进制形式，IPv6地址则必须用[]括起来）。 一个可选的 端口（port） 部分，以:开头：表示宿主服务器的监听端口，很多scheme都有默认的端口号。   一个 路径（path） 部分：表示服务器上资源的本地名，path可能由很多/分隔的片段组成。 一个可选的 查询（query） 部分：以?开头，query的语法并没有明确定义，但它通常是很多键-值对组成，这些键-值对会被某个分隔符分隔。例如：key1=value1\u0026amp;key2=value2或key1=value1;key2=value2中分别采用了\u0026amp;和;作为分隔符。 一个可选的 片段（fragment） 部分：以#开始，通常包括一个fragment标识符。有些资源（比如HTML）会在内部作进一步划分，fragment就可以用来引用部分资源或资源的一个片段。当客户端向服务器请求内容时，fragment并不会被发送给服务器，因为fragment是给客户端内部使用的。例如：浏览器从服务器拿到整个资源后，可以将滚动页面指定的fragment部分。  例子 下面是一些URI的例子：\nftp://ftp.is.co.za/rfc/rfc1808.txt https://tools.ietf.org/html/rfc3986#section-3.5 ldap://[2001:db8::7]/c=GB?objectClass?one mailto:John.Doe@example.com news:comp.infosystems.www.servers.unix tel:+1-816-555-1212 telnet://192.0.2.16:80/ urn:oasis:names:specification:docbook:dtd:xml:4.1.2 URI的使用 当应用引用一个URI时，它们并不总是使用URI语法规则中所定义的完整形式（绝对URL）。很多网络协议和媒体类型都允许使用URI的缩略形式，例如相对URI。一些浏览器还支持URL的“自动扩展”，即用户输入URL的一部分，浏览器会填充URL的剩余部分。\nURI引用 资源标识符最常见的使用方式就是URI引用（URI Reference）。URI引用不是一个URI就是一个相对引用。\n相对引用 如果一个URI引用的前缀匹配不上URI语法中定义的scheme（以:结尾），那么它就是一个相对引用（relative reference）。相对URI是不完整的，要想从相对URI中获得访问资源所需的全部信息，就必须在基URI（base URI）的基础上进行解析。相对引用所指向的URI又称目标URI（target URI）。\n相对引用有几种不同的形式：\n 以两个斜线（//）开头的相对引用是被称为网络路径引用（network-path reference），这一类引用很少被使用。 以单个斜线（/）开头的相对引用被称为绝对路径应用（absolute-path reference）。 不以斜线开头的相对引用被称为相对路径应用（relative-path reference）。若某个路径片段中包含:（例如this:that），那么这个路径片段不能直接用作相对路径引用中的第一部分，因为它会被误认为是scheme名。这种路径片段前面必须加上./（例如./this:that）以消除歧义。  绝对URI 和相对引用不同，绝对URI（Absolute URI） 包含访问资源所必需的所有信息。绝对URI的开头包括访问资源所采用的scheme。\n引用解析 为了访问相对引用指向的资源，我们需要先解析去它对应的绝对URI。解析过程见：Reference Resolution.\nURL  A Uniform Resource Locator (URL), colloquially termed a web address, is a reference to a web resource that specifies its location on a computer network and a mechanism for retrieving it.\n **统一资源定位符（Uniform Resource Locator, URL）**是资源标识符最常见的形式，描述了特定服务器上特定资源的位置。现在，几乎所有的URI都是URL。\nURN  A Uniform Resource Name (URN) is a Uniform Resource Identifier (URI) that uses the urn scheme. URNs are globally unique persistent identifiers assigned within defined namespaces so they will be available for a long period of time, even after the resource which they identify ceases to exist or becomes unavailable.\n 统一资源名（Uniform Resource Name, URN） 是URI的另一种形式，作为特定内容的唯一名字使用，与资源目前的所在地无关。和URL不同，URN是与位置无关的，我们可以四处移动资源，但依然可以通过URN来访问它。例如，不论RFC 2141位于何处（甚至是被复制到多个地方），我们都可以用urn:ietf:rfc:2141这个URN来访问它。\n参考资料  Web resource. Uniform Resource Identifier. Uniform Resource Locator. Uniform Resource Name. RFC 3986. David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002. 【日】上野宣, 图解HTTP, 人民邮电出版社, 2014.  ","href":"/computer_networks/http/uri_and_resources/","title":"URI与资源"},{"content":"Web代理（Proxy） 服务器是网络中的中间实体，位于客户端与服务器之间，扮演的是“中间人”的角色，负责在各端点之间来回传送HTTP报文。\nWeb的中间实体 Web代理服务器是代表客户端完成事务处理的中间人。如果没有Web代理，HTTP客户端就要直接与服务器进行对话。而有了Web代理之后，客户端就可以与代理进行对话，然后代理代表客户端与服务器进行交流。客户端仍然可以完成事务处理，但代理可以为它带来更加优质的服务和更加灵活的扩展功能。\nHTTP代理服务器既是Web客户端，又是Web服务器。对于客户端而言，Web代理扮演的是服务器的角色，它必须正确处理请求并进行响应。同时，对于服务器而言，Web代理扮演的是客户端的角色，它向服务器发起请求并获得响应。\n代理vs网关 严格来说，代理连接的是两个或多个使用相同协议的应用程序，而网关连接的是两个或多个使用不同协议的端点。网关的主要作用是进行协议转换，即使客户端与服务器使用了不同的协议，客户端也可以借助它完成与服务器之间的事务处理。\n代理的使用场景 代理服务器可以监视并修改所有经过的HTTP流量，因此可以用它来实现很多有用的功能。例如：\n 进行内容的过滤、访问控制、路由和转换。 作为防火墙使用。 作为Web缓存使用，维护常用内容的副本，以加快访问速度。 假扮Web服务器，接收发给Web服务器的真实请求并处理。这种代理被称为替代物（surrogate）或反向代理（reverse proxy）。与普通Web服务器不同的是，反向代理可以发起与其它服务器的通信，按需获取请求内容。反向代理还可以用来提高访问慢速Web服务器上公共内容时的速度，通常将这些反向代理称为服务器加速器（server accelerator）。 匿名代理。主动从HTTP报文中删除访问者的身份信息，从而提供高度的私密性和匿名性。  如何将请求导向代理 客户端通常会直接与Web服务器进行通信。为了让代理派上用场，我们需要将到客户端的HTTP流量导向代理，常见的方式有四种：\n 修改客户端（图a）。很多Web客户端都支持手工和自动的代理配置，可以将客户端配置为使用代理服务器。 修改网络（图b）。网络基础设施可以在客户端不知情的情况下通过一些技术手段将客户端的流量拦截并导入代理，这种代理通常被称为*拦截（intercepting）*代理。 修改DNS的命名空间（图c）。放在Web服务器之前的代理可以直接假扮Web服务器的名字和IP地址，这样，请求就可以直接被发给代理而不是服务器了。要实现这一点，我们可以手动修改DNS列表，也可以采用特殊的动态DNS服务器。 修改Web服务器（图d）。可以将某些Web服务器配置为向客户端发送一个HTTP重定向命令，将客户端的请求重定向到一个代理上去。  客户端代理设置 现在几乎所有的浏览器都允许用户对代理的使用进行配置。配置的方式大概有下面几种:\n 手工配置。显示地设置使用的代理。 预先配置浏览器。浏览器厂商预先对浏览器进行配置。 代理的自动配置（Proxy Auto-Configuration, PAC）。提供一个指向JavaScript编写的PAC文件的URI，客户端会去获取这个文件并运行，从而决定是否使用以及使用哪个代理服务器。PAC文件的后缀通常是.pac，MIME类型通常是application/x-ns-proxy-autoconfig。 使用Web代理自动发现协议（Web Proxy Autodiscovery Protocol, WPAD）。WPAD协议的算法会使用发现机制逐级上升策略自动地为浏览器发现合适的PAC文件。协议会使用一系列的资源发现技术来判定出适当的PAC文件。  追踪报文 Web请求在从客户端传送到服务器的这个过程中，经过两个或多个代理是非常常见的。随着代理的流行，我们要能够追踪经过代理的报文流，以检测出各种问题，其重要性就跟追踪经过不同交换机和路由器传输的IP数据报流一样。\nVia首部 Via首部字段列出了报文途径的每个中间节点（代理或网关）的相关信息，报文每经过一个节点，都必须将这个中间节点添加到Via列表的末尾。例如，下面的报文经过了两个代理：第一个代理为proxy1.com，它实现了HTTP/1.1协议；第二个代理为proxy2.com，它实现了HTTP/1.0：\nVia: 1.1 proxy1.com, 1.0 proxy2.com Via首部被用来追踪报文的转发过程、诊断报文路由循环，以及识别整个请求/响应链上所有发送方的协议处理能力。\nVia的语法 Via首部包含一个由逗号分隔的路径点（waypoint）。每个路径点都对应一个独立的代理服务器或网关，并且含有与该中间节点的一些信息（比如所采用的协议、节点地址等）。Via首部的语法如下：\nVia = \u0026quot;Via\u0026quot; \u0026quot;:\u0026quot; 1#( waypoint ) waypoint = ( received-protocol received-by [ comment ] ) received-protocol = [ protocol-name \u0026quot;/\u0026quot; ] protocol-version received-by = ( host [ \u0026quot;:\u0026quot; port ] ) | pseudonym Via的请求与响应路径 请求和响应报文都会经过代理进行传输。因此，请求和响应报文中都要有Via首部。由于请求和响应通常是在同一条TCP连接上传送的，所以响应报文会在请求报文的路径上反向传输。因此，响应报文的Via首部几乎总是与请求报文的Via首部相反。\nTRACE方法 通过HTTP/1.1提供的TRACE方法，用户可以追踪请求报文经过的代理链，观察请求报文经过了哪些代理以及每个代理都对请求报文做了哪些修改。\nTRACE对代理流的调试非常有用，当TRACE请求到达服务器时，整条请求报文都会被封装在一条HTTP响应的body中会送给客户端。客户端通过检查响应报文，便可以知道报文所经过的代理列表。TRACE响应的Content-Type为message/http，状态码为200 OK。\n通常，不管客户端与服务器之间存在多少代理，TRACE报文都会沿着这个代理链传到服务器。但我们可以使用Max-Forwards首部来限制TRACE和OPTIONS请求所经过的代理数。Max-Forwards请求首部包含了一个整数，这个整数代表当前请求报文还可以被转发多少次。当值为0时，不管接收者是谁，它都必须停止转发并将TRACE报文会送给客户端。当值大于0时，接收者会将其减一并转发。\n代理的互操作性 客户端、服务器和代理通常是由不同厂商构建的，实现的HTTP规范版本可能会存在差异。通过HTTP提供的OPTIONS方法，客户端或代理可以发现Web服务器或其上某个特定资源所支持的功能。如果OPTIONS请求的URI是个星号（*），那么请求的是整个服务器所支持的功能。如果OPTIONS请求的URI是个实际的资源地址，那么请求的就是特定资源的可用特性。如果OPTIONS请求成功，客户端会收到一个包含了各种首部字段的200 OK响应，这些首部字段就描述了服务器或特定资源所支持的各种可选特性。\nHTTP/1.1在响应中唯一指定的首部字段是Allow，它描述了服务器或特定所支持的各种方法，例如：Allow: GET, HEAD, PUT。\n参考资料  David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002.  ","href":"/computer_networks/http/proxies/","title":"代理"},{"content":"用于HTTP协议交互的信息被称为HTTP报文（HTTP Messages）。客户端发出的HTTP叫做请求报文（或Requests），服务端发出的报文叫做响应报文（或Responses）。\n报文流 作为HTTP应用程序之间发送的数据块，HTTP报文以一些描述了报文内容及含义的文本形式的 元信息（meta-information） 开头，后面跟着可选的数据部分。这些报文在客户端、服务器和代理之间流动，术语流入（inbound）、流出（outbound）、上游（upstream） 和 下游（downstream） 就是用来描述报文的方向的。\nInbound与Outbound HTTP使用术语inbound和outbound来描述事务处理的方向。报文流入源端服务器，处理完成之后又会流出到用户的Agent代理中：\nUpstream与Downstream 不管是请求报文还是响应报文，所有的HTTP报文都会向下游（downstream）流动，所有的报文发送者都在接收者的上游（upstream）。报文只会从上游向下游流动，不存在从下游向上游流动的情况：\n报文的组成 HTTP报文本身是由多行（用CRLF作换行符）数据构成的字符串文本，包括三个部分：对报文进行描述的起始行（start line）、包含报文属性的首部（headers）和可选的包含数据的主体（body）。例如:\n起始行和首部都是由行分隔的ASCII文本，每行都以一个有两个字符组成的行终止符结束。这个行终止符包括一个回车符（Carriage Return, CR, ASCII码为13）和一个换行符（Line Feed, LF, ASCII码为10），CR和LF合起来就是CRLF。需要注意的是，尽管HTTP规范中说明应该用CRLF来表示行终止，但有些老的或不完整的应用程序并不总是同时发送CR和LF，所以应用程序也应该能处理这种情况。\n报文的主体是一个可选的数据块。与起始行和首部不同的是，主体中可以包含文本或二进制数据，也可以为空。\n报文的语法 所有的HTTP报文都可以分为两类：请求报文（request message）和响应报文（response message）。请求报文请求服务器执行一个操作，而响应报文则会将请求的结果返回给客户端。请求报文和响应报文的基本机构相同，但它们的格式有些许差异。\n这是请求报文的格式：\n\u0026lt;method\u0026gt; \u0026lt;request-URL\u0026gt; \u0026lt;version\u0026gt; \u0026lt;headers\u0026gt; \u0026lt;entity-body\u0026gt; 这是响应报文的格式（它和请求报文只有起始行不同）：\n\u0026lt;version\u0026gt; \u0026lt;status\u0026gt; \u0026lt;reson-phrase\u0026gt; \u0026lt;headers\u0026gt; \u0026lt;entity-body\u0026gt; 下面是对报文格式中各组成部分的一个简短说明：\n 方法（method）：告知服务器要做什么。 请求URL（request-URL）：指定了所请求资源或URL里面path部分的完整URL。 版本（version）：报文所使用的HTTP版本，格式为：HTTP/\u0026lt;major-version\u0026gt;.\u0026lt;minor-version\u0026gt;。 状态码（status-code）：描述请求过程种发生的情况，每个状态码的第一位数字都对应一类状态。 原因短语（reason-phrase）：这是状态码的可读版本，包含行终止符之前的所有文本。 首部（header）：首部的数量可以为0个、1个或多个，每个首部都包含一个名字，名字后面跟着一个:，然后是一个可选的空格，接着是一个值，最后是一个CRLF。首部总是以一个空行（单个CRLF）结束。 主体（entity-body）：主体部分包含一个由任意数据组成的数据块。这是一个可选的部分，并不是所有的报文都有主体部分，有时，报文只是以一个CRLF结束。  起始行 所有的HTTP报文都以一个起始行作为开始。请求报文的起始行说明了要做什么，而响应报文的起始行则说明了发生了什么。\n请求行 请求报文的起始行称为请求行（request line）。请求行包括三部分，即[报文的语法]部分介绍的方法、请求URL和版本，每个部分之间用空格分隔。\n响应行 响应报文的起始行称为响应行（response line），它包含了响应报文所使用的HTTP版本、数字状态码和描述操作状态的原因短语。\n方法 请求行以方法开始，方法告知服务器要做什么。下面描述了7种常用的HTTP方法：\n   方法 描述 是否包含body     GET 从服务器获取一份文档 否   HEAD 只从服务器获取文档的首部 否   POST 向服务器发送需要处理的数据 是   PUT 将请求的body部分存储在服务器上 是   TRACE 对可能经过代理传送到服务器上的报文进行追踪 否   OPTIONS 决定可以在服务器上执行哪些方法 否   DELETE 从服务器上删除一份文档 否    并不是所有的服务器都实现了以上7种方法。此外，由于HTTP在设计上是易扩展的，一些服务器可能还会实现一些自己的请求方法。\n状态码 方法是客户端用来告知服务器做什么，而状态码是服务器用来告知客户端发生了什么的。状态码位于响应行中，状态码为数字，便于程序进行处理。而原因短语为文本，方便人们理解。状态码共有5个类别：\n   总范围 已定义范围 分类     100 ~ 199 100 ~ 101 信息提示   200 ~ 299 200 ~ 206 成功   300 ~ 399 300 ~ 305 重定向   400 ~ 499 400 ~ 415 客户端错误   500 ~ 599 500 ~ 505 服务器错误    原因短语 原因短语是响应行中的最后一个部分，它为状态码提供了文本形式的解释，方便人们理解。状态码和原因短语总是成对出现。\n版本号 版本号会以HTTP/\u0026lt;major\u0026gt;.\u0026lt;minor\u0026gt;的形式出现在起始行中，HTTP应用程序用它来告诉对方自己所遵循的协议版本，以便双方互相理解。版本号说明了应用程序支持的最高HTTP版本。\n首部 HTTP的起始行后会有0个、1个或多个首部字段，首部内容为客户端和服务器分别处理请求和响应提送所需要的信息（例如：报文主体的大小、所使用的语言、编码、认证信息等）。首部字段本质为一个键-值对列表。\nHTTP规范定义了几种类型的首部：\n 通用首部：既可以出现在请求报文中，也可以出现在响应报文中。 请求首部：提供更多有关请求的信息。 响应首部：提供更多有关响应的信息。 实体首部：描述body的长度和内容，或资源本身。 扩展首部：规范中没有定义的新首部。  除了HTTP规范定义的几种首部字段外，应用程序也可以随意发明自己所用的首部。\n主体 作为HTTP报文的最后一部分，body就是HTTP报文要传输的内容。\n内容协商 同一个Web网站可能存有多份表示相同内容的页面（比如同一个页面可能存在英语版本和中文版本）。当浏览器默认语言为英文时，访问网站时可能展示的是英文页面，而当浏览器语言设置为中文时，访问网站展示的可能又是中文页面。这种机制就是内容协商（Content Negotiation），即客户端和服务器就响应的资源内容进行交涉，然后返回给客户端最为合适的资源，决策时可能会将语言、编码、字符集等作为判别条件。内容协商可能涉及的首部有：Accept、Accept-Charset、Accept-Encoding、Accept-Language、Content-Language等。\n有三种不同的内容协商类型：客户端驱动协商（client-driver negotiation）、服务端驱动协商（server-driven negotiation）和透明协商（transparent negotiation）。\n客户端驱动协商 客户端先发起请求，服务器返回一个可供选择的列表，然后客户端从列表中选择一个合适的。这种方式对服务端来说更容易实现，并且客户端可以做出最合适的选择。但是，这回使得整个过程变得更长，因为至少需要两次请求才能获取到正确的内容。\n服务端驱动协商 服务端通过检查客户端的请求头，从而决定要返回的内容。这种方式比客户端驱动协商更快，HTTP提供了一个质量值机制，它可以帮助服务器找到最合适的选择。此外，Vary首部也会帮助服务器进行猜测。服务端驱动协商有一个缺点：如果请求头对服务器没啥帮助，服务器必须猜测出最佳内容。\n内容协商相关首部 客户端可能会使用以下首部告诉服务器它们的内容偏好：\n   Header Description     Accept 告诉服务器可接受的媒体类型   Accept-Language 告诉服务器可接受发送的语言   Accept-Charset 告诉服务器可接受的字符集   Accept-Encoding 告诉服务器可接受的编码    当收到的请求含有以上的首部时，服务端会将对应的实体首部添加到响应报文中：\n   Accept Header Entity header     Accept Content-Type   Accept-Language Content-Language   Accept-Charset Content-Type   Accept-Encoding Content-Encoding    内容协商首部中的质量值 为了允许客户端列出每种偏好类别的多种选项以及每个偏好选项的优先次序，HTTP协议定义了质量值（Quality Value）。例如，客户端可以发送以下形式的Accept-Language首部：\nAccept-Language: en;q=0.5, fr;q=0.0, zh;q=1.0, tr;q=0.0 其中q值的范围是0.0 ~ 1.0（0.0的优先级最低，而1.0的优先级最高）。上面的首部寿命客户端最愿意接收的中文（zh）文档，但英语（en）文档也可以，并且不愿意接收文档的法语（fr）或土耳其（tr）语版本。\n透明协商 由一个中间设备（通常是一个代理缓存）代表客户端进行内容协商。这种方式比客户端驱动协商更快，但目前关于如何协商没有一个明确的标准。\n参考资料  David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002. 【日】上野宣, 图解HTTP, 人民邮电出版社, 2014.  ","href":"/computer_networks/http/http_messages/","title":"HTTP 报文"},{"content":"Web连接着不计其数的资源，但并不是所有的资源都是可以随意访问的。某些资源只对部分特定的用户开放，为了达到这个目的，需要对用户进行认证（authentication）。通过认证，服务器可以知道用户是谁，进而判定用户可以访问哪些资源。简而言之，认证就是向服务器证明你是谁，而**授权（authorization）**就是服务器授予用户访问特定资源的权限。\nHTTP1.1使用的认证方式如下：\n BASIC认证（基本认证） DIGEST认证（摘要认证） SSL客户端认证 FormBase认证（表单认证） 此外，还有NTLM、Kerberos、Windows Live ID等认证方式。  HTTP访问认证框架 HTTP提供了一个简单的 质询-响应（challenge-response） 认证机制，服务器可以用它来对客户端请求进行质询，要求用户提供认证信息。下图展示了HTTP的认证模型：\nHTTP认证模型由4个步骤组成：\n 请求（Request）：客户端发送请求。 质询（Challenge）：当客户端请求的资源需要认证时，服务器会以401 Unauthorized状态码拒绝请求，同时在响应报文中添加WWW-Authenticate首部字段。WWW-Authenticate首部字段会对保护区域进行描述并指定认证算法。 授权（Authorization）：客户端重新发起请求，但这次会给请求报文附加一个Authorization首部。Authorization首部说明了认证算法、用户名和密码。 成功（Success）：如果授权成功，服务器就会返回客户端所请求的内容。有些授权算法会在Authentication-Info首部中返回一些与授权会话相关的附加信息。  HTTP允许服务器给不同的资源指定不同的访问权限，这是通过安全域来实现的。Web服务器会将受保护的文档组织成一个安全域（security realm），每个安全域都可以有不同的授权用户。\nBASIC认证 BASIC认证早在HTTP/1.0规范中就被提出来了，后来被移到了RFC2617中。现在仍然有一部分网站使用这种传统的认证方式。BASIC认证中会使用WWW-Authenticate和Authorization两个首部，但并不会使用Authentication-Info首部：\n 质询（服务器发往客户端）：网站的不同部分可能使用不同的密码，Realm是一个字符串，告诉用户该使用哪个账号和密码。首部格式为：WWW-Authenticate: Basic realm=quoted-realm。 响应（客户端发往服务端）：客户端用:将用户名和密码连接起来，然后用Base-64进行编码。首部格式为：Authorization: Basic base64-username-and-password。  BASIC认证的步骤 下面是BASIC认证的一个例子：\nBASIC认证的步骤如下：\n 客户端请求的资源需要BASIC认证的资源。 服务器会随状态码401 Authorization Required返回带WWW-Authenticate首部字段的响应，该字段内包含了认证的方式（BASIC）和请求资源所属安全域（realm=\u0026quot;xxx\u0026quot;）。 客户端向用户询问用户名和密码，然后用:连接二者，再进行Base-64编码处理。编码结果会放在Authorization首部中回送给服务器。 服务器对用户名和密码进行解码，验证它们的正确性。如果验证通过，则返回一条包含请求资源的响应。  代理认证 位于客户端和服务器之间的代理也可以实现认证功能。代理认证的步骤和BASIC认证的身份验证步骤相同，但首部和状态码都有所不同。下表列出了Web服务器和代理在认证中使用的状态码和首部的差异：\n   Web Server Proxy Server     Unauthorized status code: 401 Unauthorized status code: 407   WWW-Authenticate Proxy-Authenticate   Authorization Proxy-Authorization   Authentication-Info Proxy-Authentication-Info    BASIC认证的缺陷 BASIC认证虽然简单便捷，但并不安全。只能用它来防止非恶意用户的无意间访问，或配合SSL等加密技术一起使用。BASIC认证存在一些缺陷：\n 通过网络发送用户名和密码。虽然用户名和密码经过了Base-64编码，但可以轻松解码。 攻击者可以嗅探到用户名和密码，然后实施重放攻击。 缺乏针对代理或中间人等中间节点的防护措施。 服务器很容易被假冒。  DIGEST认证 BASIC认证虽然简单，但极不安全：用户名和密码是明文传输的，报文也容易被篡改，保证安全的唯一方式就是配合SSL进行BASIC认证。由于BASIC认证无法达到大多数Web网站期望的安全等级，它并不常用。为了弥补BASIC认证的不足，HTTP/1.1引入了DIGEST认证。DIGEST认证同样采用了质询/响应的方式，但不会发送明文密码，因此更加安全。\nDIGEST认证步骤 由于DIGEST认证和BASIC认证都采用了HTTP的质询/响应框架，所以DIGEST认证的步骤与BASIC认证的步骤基本相同。下图展示了二者再语法上的差异：\nDIGEST认证的步骤如下：\n 客户端请求受保护的资源。 服务器会随状态码401 Authorization Required返回带WWW-Authenticate首部字段的响应，该字段内包含了认证的方式（DIGEST）、对应的保护域（realm）、临时质询码（nonce，一个随机数）等。realm和nonce这两个字段是必须要有的，客户端需要依靠向服务端回送这两个值进行认证。 客户端再Authorization首部中返回DIGEST认证所必须的信息，包括username、realm、nonce、uri、response的字段信息等。其中realm和nonce就是上一步中从服务器响应中收到的字段。 服务器对客户端提供的信息进行验证。若验证通过，则返回受保护的资源。  参考资料  David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002. RFC2617.  ","href":"/computer_networks/http/http_authentication/","title":"HTTP 认证"},{"content":"Web上所有的资源都可以使用HTTP协议，并且其它应用和应用协议也可以利用HTTP来完成它们的任务。开发者可以将HTTP作为一个框架来使用其它协议并进行应用通信，例如：\n 用网关作为HTTP与其它协议和应用程序之间的接口。 用应用程序接口来实现Web上不同类型资源之间的相互通信。 用隧道在HTTP连接上发送非HTTP流量。 用中继逐跳转发数据。  网关 网关（gateway） 是一种在服务器之间充当中间实体的特殊服务器，通常用来将HTTP流量转换为其它协议流量。网关总是像源服务器一样接收请求，客户端可能并不知道自己在和一个网关通信。\n客户端和服务端网关 Web网关在一侧使用HTTP协议，而在另一侧使用另一种协议。\n网关可以用一个/分隔的客户端协议和服务端协议进行描述：\u0026lt;client-protocol\u0026gt;/\u0026lt;server-protocol\u0026gt;。例如，一个连接HTTP客户端与FTP服务器的网关就可以被称为HTTP/FTP网关。可以用术语 客户端网关（clent-side gateway） 和 服务端网关（server-side gateway） 来描述对话是在网关的哪一侧进行的：\n 客户端网关与客户端使用HTTP进行对话，而与服务器使用其它协议（HTTP/*）。 服务端网关与客户端使用其它协议进行对话，而与服务器使用HTTP协议（*/HTTP）。  协议网关 我们可以使用与将流量导向代理相同的方法来将流量导入网关。例如，显式配置浏览器使用网关、对流量进行透明拦截或将网关标记配置为反向代理。\nHTTP/*：服务端Web网关 请求流入源服务器时，服务端Web网关会将客户端HTTP请求转换为其它协议。\nHTTP/HTTPS：服务端安全网关 一个组织可以通过网关对所有的输入Web请求加密，以提供额外的隐私和安全性保护。客户端可以用普通的HTTP浏览Web内容，但是网关会自动加密用户的对话。\nHTTPS/HTTP：客户端安全加速器网关 HTTPS/HTTP网关位于Web服务器之前，通常作为不可见的拦截网关或反向代理使用。它们接收安全的HTTPS流量，对安全流量进行解密，并向Web服务器发送普通的HTTP请求。这些网关通常包含专用的解密硬件，可以高效地解密安全流量，从而降低源服务器的负荷。但是，网关和源服务器之间发送的是未加密的流量，所以需要确保网关和源服务器之间的安全性。\n资源网关 应用服务器是网关最常见的形式。应用服务器是服务端网关，它与客户端之间通过HTTP通信，并与服务端的某个应用程序相连。应用服务器可能会将HTTP请求通过**应用编程接口（Application Programming Interface, API）**传递给运行在服务端上的应用程序。\n通用网关接口 第一个流行的应用程序网关API就是通用网关接口（Common Gateway Interface, CGI）。CGI是一个标准接口集，Web服务器可以用它来装载程序以处理对特定URL的HTTP请求，收集程序的输出数据并将其放入HTTP响应中回送。\n上图展示了服务器与网关应用程序之间的基本交互机制。这个协议（请求、转交、响应）就是CGI模型的基本运行机制。\nCGI应用程序是独立于服务器的，因此几乎可以采用任意语言实现。CGI的处理对用户是不可见的。客户端完全不知道服务器和CGI应用程序之间的转接过程，URL中出现字母cgi或可能的?是客户端发现使用了CGI应用程序的唯一线索。\nCGI在服务器和众多资源类型之间提供了一个简单的对接方式，可以处理各种需要的转换。此外，他还能很好地保护服务器，防止一些扩展破坏服务器。但是这种分离会造成性能的损失，因为为每个CGI请求都创建一个新进程这一做法的代价是非常昂贵的。\n服务器扩展API 大多数流行的服务器都会为开发者提供一个或多个扩展API，这些扩展通常与服务器自身的架构有关系，开发者开一通过这些接口改变服务器的行为或定制实现某些需求。\n隧道 隧道（tunnel） 是一种HTTP应用程序，当隧道建立后，它就会在两条连接之间对原始数据进行盲转发。HTTP隧道不会窥探数据，它通常用来在HTTP连接上传输非HTTP数据。利用HTTP隧道，用户可以通过HTTP应用程序访问使用非HTTP协议的应用程序。\n隧道的建立 Web隧道是用HTTP的CONNECT方法建立起来的。CONNECT方法请求隧道网关创建一条通向任意目标服务器和端口的TCP连接，并对客户端和服务器之间的后续数据进行盲转发。\n上图显示了使用CONNECT方法建立一条到网关的隧道的过程：\n 客户端发送一条CONNECT请求给隧道网关（图a），请求隧道打开一条TCP连接（这里打开的是一条标准SSL连接）。 建立网关与服务器之间的TCP连接（图b和图c）。 一旦TCP连接建立成功，网关就会发送一条HTTP 200 Connection Established响应给客户端（图d）。 隧道建立完成（图e）。客户端通过HTTP隧道发送的所有数据都会被直接转发给输出TCP连接，服务器发送的所有数据也会通过HTTP隧道转发给客户端。  中继 HTTP中继（relay） 是一个简单的HTTP代理，它没有完全遵循HTTP规范。中继做的事情很简单：建立连接，然后对字节进行盲转发。\n参考资料  David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002.  ","href":"/computer_networks/http/gateway_tunnels_and_relays/","title":"网关、隧道和中继"},{"content":"HTTP连接是HTTP报文传输的关键通道。\nTCP连接 TCP/IP是全球计算机及网络设备都在使用的一个分层包交换协议集，几乎所有的HTTP通信都是由TCP/IP承载的。TCP连接时可靠的，它为HTTP提供了一条可靠的比特传输管道，从TCP连接一端传入的字节会在另一端以原有的顺序被正确地传送出来。\nTCP会将数据切分为小的数据块发送，这些小的数据块就是IP数据包（IP packets）或IP数据报（IP datagrams）。这个时候，HTTP就位于HTTP over TCP over IP这个协议栈的最顶层，其安全版本HTTPS只是在HTTP和TCP之间插入了一个加密层（称为TLS或SSL）：\nHTTP在传送一条报文时，会以流的形式将报文内容通过一条打开的TCP连接按序传输。TCP在收到数据流之后，会先将数据流分割成小数据块（这些数据块被称为段（segments）），然后将段封装在IP数据包中，通过Internet进行传输。所有的这些工作都是由TCP/IP软件来处理的，每个TCP段都由IP数据报承载，从一个IP地址发送到另一个IP地址。每个IP数据包中都包括：\n 一个IP数据报首部（通常为20字节）。 一个TCP段首部（通常为20字节）。 一个TCP数据块（0或多个字节）。  TCP通过端口号来保持当前打开的TCP连接的正确运行。IP地址帮助我们找到正确的主机，而端口号帮助我们找到该主机上正确的应用程序。一条TCP连接由\u0026lt;source-IP-address, source-port, destination-IP-address, destination-port\u0026gt;四个值唯一标识。\nHTTP连接处理 HTTP允许客户端和最终的源端服务器中间存在一个中间实体（intermediary）（代理、缓存等）链。HTTP报文从客户端开始在中间设备间逐跳转发，最终到达源端服务器。或者从源端服务器出发，经过中间设备间的逐跳转发，最终达到客户端。\nConnection Header HTTP的Connection首部是一个通用首部，它允许发送方或客户端声明一些只用于某个特定连接的选项，并且这些选项不会（也不能）被其它的连接使用。这些选项就是Connection首部字段中的一个由逗号分隔的连接标签列表。例如：可以用Connection: close来表面发送方希望在响应报文发送之后关闭连接。\nConnection首部可以承载3中不同类型的标签：\n HTTP首部字段名：列出只与此连接有关的首部。 值close：说明当前的request/response操作完成之后就关闭这条连接。这是HTTP/1.0中的默认值。 任意标签值：描述此连接的非标准选项。  如果连接标签列表中包含了一些HTTP首部字段的名称，那么这些首部字段就包含了一些与连接有关的信息，这些信息是不能被转发出去的。在将HTTP报文被转发出去之前，代理必须删除Connection首部中列出的所有HTTP首部字段。Connection首部是一个逐跳首部，只适用于单各链路并且不应该被顺着链路向下传输。\n串行事务处理与短连接 HTTP最早期的模型是短连接（Short-lived connections）。每一个HTTP请求都由它自己独立的连接完成，这意味着每发起一个HTTP请求之前都会进行一次TCP握手。这些连接的生命周期很短：每发起一个请求时都会创建一个新的连接，并在收到响应时立即关闭。\nTCP协议握手本身就是很耗时的，所以TCP可以通过保持更多的热连接来适应负载。而短连接破坏了TCP具备的能力，因为新的冷连接降低了其性能。如果每个事务都需要（串行地建立）一条新的连接，那么TCP的连接时延和慢启动时延就会叠加起来。\n短连接是HTTP/1.0的默认模型（如果没有指定Connection头，或它的值被设置为close）。而在HTTP/1.1中，只有当Connection被设为为close时才会用到短连接模型。\n短连接对TCP性能有先天的限制：每打开一个TCP连接都是相当耗费资源的操作。为了解决短连接模型的低效率问题，出现了几种新的模型：\n 并行连接（Parallel connections）：通过多条TCP连接发起并发的HTTP请求。 持久连接（Persistent connections）：重用TCP连接，以消除打开及关闭TCP连接时的时延。 管道化连接（Pipelined connections）：通过共享的TCP连接发起并发的HTTP请求。 复用连接（Multiplexed connections）：交替传送请求和响应报文。  并行连接 HTTP允许客户端打开多条连接，并行地执行多个HTTP事务，每个事务都有自己的TCP连接。\n例如，当一个网页里面包含多个组件的时候，如果并行连接能够克服单条连接的空载时间和宽带限制，网页的加载速度就会有所提高。时延可以重叠起来，如果单条连接没有充分利用带宽，空闲的带宽可以用来加载其它组件。\n需要注意的是：并行连接可能会更快，但不一定总是更快。当一个较慢的客户端连接到较快的服务器上时，慢客户端的带宽可能很快被耗尽，接着就是带宽资源的竞争，这可能并不会带来性能的提升。因为，打开大量连接会消耗很多的内存资源，从而引发自身的性能问题。\n很多浏览器都使用了并行连接，但它们一般都会将并行连接的总数限制在一个较小的值，而服务器可以随意关闭来自特定客户端的过量连接。\n持久连接 Web客户端经常会打开到同一个站点的连接，并且可能会在将来的一段时间内对该站点发起更多的请求，这种性质就是站点的局部性（site locality）。\n因此，HTTP/1.1（以及HTTP/1.0的各种增强版本）允许HTTP设备在事务处理结束之后保持TCP连接的打开状态，以便将来的HTTP请求可以重用现有的连接。在事务结束之后仍然保持在打开状态的TCP连接被称为持久连接（persistent connections）。非持久连接会在每个事务结束之后关闭，而持久连接会在不同事务之间保持打开状态，直到客户端或服务器决定将其关闭位置。\n重用已经对目标服务器打开的空闲持久连接，客户端就可以避开建立连接阶段的高昂开销。此外，已经打开的连接可以避免慢启动的拥塞适应阶段，以便更快速地进行数据的传输。\n持久连接有两种类型：\n 比较老的HTTP/1.0+中的keep-alive连接。 HTTP/1.1中的persistent连接。  并行连接 vs 持久连接 并行连接虽然可能比较快，但并行连接也有一些缺点：\n 每个事务都会打开/关闭一条新的连接，这会耗费时间和带宽。 由于TCP的慢启动特性，每条新连接的实际性能会有所降低。 可打开的并行连接的最大数量实际上是有限的。  持久连接有一些比并行连接更好的地方。持久连接可以有效的降低时延和建立连接的开销，还可以很方便地控制打开连接的数量。但是，一直打开的大量空闲连接会带来新的问题，它们会消耗客户端与服务器的资源。因此，管理持久连接时要特别仔细。\nHTTP/1.0+中的Keep-Alive连接 和短连接相比，持久连接节省了打开/关闭连接和TCP慢启动阶段的开销。\n在当前的HTTP/1.1规范中，keep-alive已经被弃用了。但是，keep-alive握手操作仍然在浏览器与服务器之间广泛使用。\n实现了HTTP/1.0 keep-alive连接的客户端可以通过在请求头里面包含Connection: Keep-Alive来告诉服务器它希望连接保持打开状态。若服务器也愿意为下一条请求而保持连接的打开状态，它也会将Connection: Keep-Alive放入响应头。如果响应报文中没有Connection: Keep-Alive，那么客户端就会认为服务器不支持keep-alive并且服务器会在发回响应报文之后关闭连接。\nKeep-Alive选项 需要注意的是：keep-alive首部只是请求将连接保持在打开状态，而客户端和服务器不一定会同意进行keep-alive会话。它们可以在任何时候关闭空闲的keep-alive连接，也可以随意限制在keep-alive连接上处理的事务数量。\n我们通过调整Keep-Alive通用首部中的一些由逗号分隔的参数来改变keep-alive行为：\n timeout参数是在Keep-Alive响应头中发送的。它代表着一个服务器可能会将连接保持在打开状态的时间的估计值。这不是一个承诺值。 max参数也是在Keep-Alive响应头中发送的。它表示服务器还希望为多少个事务而将连接表示在打开状态，事务的数量也是一个估计值，不是承诺值。 Keep-Alive首部还支持任意未经处理的属性，这些属性主要用于诊断和调试，语法为name [=value]。  Keep-Alive首部是可选的，只有在Connection: Keep-Alive出现时才会被使用。例如下面这两行表示服务器最多还会为另外5个事务保持连接的打开状态，或者将连接的打开状态再保持120秒：\nConnection: Keep-Alive Keep-Alive: max=5, timeout=120 Keep-Alive连接的限制与规则 使用keep-alive连接时有一些限制和需要特别说明的地方：\n 在HTTP/1.0中，keep-alive不是默认使用的。客户端必须发送一个Connection: Keep-Alive请求头来激活keep-alive连接。 Connection: Keep-Alive首部必须随所有希望保持持久连接的报文一起发送。如果客户端没有发送它，服务器会在完成该请求之后关闭连接。 客户端可以根据响应报文中是否有Connection: Keep-Alive首部来判断服务器是否已经在发出响应报文之后关闭连接。 只有在无需检测连接关闭就能确定报文的实体主体的长度时，连接才能被保持在打开状态。 代理和网关必须执行Connection首部上的规则。代理和网关必须在转发或缓存报文之前移除Connection首部中出现的首部字段及Connection首部本身。 一般来说，不应该在代理服务器无法保证支持Connection首部的情况下建立keep-alive连接，这么做是为了避免**哑代理（dumb proxies）**问题的出现。 从技术上说，应该忽略所有来自HTTP/1.0设备的Connection首部字段，因为它们可能是由比较老的代理服务器误转发的。 除非重复的请求会产生副作用，客户端必须准备好在接收到响应之前连接就关闭时重发请求。  HTTP/1.0持久连接 HTTP/1.0逐渐停止了对keep-alive连接的支持，并用一种名为**持久连接（persistent connection）**的设计取代了它。\n与HTTP/1.0+的keep-alive连接不同，HTTP/1.1的持久连接在默认情况下是激活的。也就是说，除非特别指明，否则HTTP/1.1会假定所有的连接都是持久的。若要在事务处理结束之后关闭连接，HTTP/1.1应用程序必须在报文中显式地添加一个Connection: close首部。但是，客户端和服务器是可以在任何时候关闭空闲连接的，所以不发送Connection: close并不意味着服务器会永远将连接保持在打开状态。\n持久连接的限制与规则 在使用持久连接时，有一些点需要注意：\n 在发送Connection: close请求头之后，客户端就不能在该连接上发送其它的请求了。 如果客户端不想在某条连接上发送另一个请求，那么它应该在当前的最后一个请求中发送Connection: close首部。 只有当连接上的所有报文都是正确的自定义的消息长度，连接才能被保持在打开状态。 HTTP/1.1代理必须能够区分关系客户端和服务器的持久连接，每条持久连接都只适用于单跳传输。 HTTP/1.1服务器不应该与HTTP/1.0客户端建立持久连接。 不管Connection首部内容如何，HTTP/1.1设备都可以在任意时刻关闭连接。 除非重复发起请求会产生副作用，否则如果客户端在收到整个响应之前连接就关闭了，客户端必须重新发起请求。 一个用户客户端对任何服务器或代理最多只能维持两条持久连接，以防止服务器过载。  管道化连接 HTTP/1.1允许在持久连接之上使用可选的管道化连接（Pipelined connections）：在响应到达之前，可以将多条请求放入队列，当第一条请求经过网络流向服务器时，其它的请求也可以开始发送了。在高延迟的网络条件下，管道化连接可以降低网络的环回时间，提高性能。\n在使用管道化连接时，有一些地方需要注意：\n 如果HTTP客户端无法确认连接是持久的，它就不应该使用管道。 服务器必须按照与请求相同的顺序回送HTTP响应。因为HTTP报文中没有序列号标签，所以响应不能失序。 HTTP客户端必须做好连接可能在任意时刻关闭的准备，并重发所有未完成的管道化请求。 HTTP客户端不应该使用管道化的方式发送会产生副作用的请求。  连接的关闭 异常关闭连接 所以的HTTP客户端、服务器或代理都可以在任意时刻关闭一条TCP传输连接，HTTP应用程序必须做好正确处理连接非正常关闭的情况。\n每条HTTP响应都应该有精确的Content-Length首部，用来描述响应主体的大小。一些老的HTTP服务器会省略这个首部，这样就要通过服务器发出的连接关闭来确定数据的真实末尾。\n正常关闭连接 TCP连接是双向的。TCP连接的每一端都有一个输入队列和一个输出队列，分别用于数据的读写。放在一端输出队列中的数据最终会出现在另一端的输入队列中。\n完全关闭与半关闭 应用程序可以关闭TCP输入与输出信道中的任意一个，或将两个都关闭。套接字调用close()会将TCP连接的输入与输出信道都关闭，这就是完全关闭。而套接字调用shutdown()只会单独关闭输入或输出信道，这就是半关闭。\n关闭连接的输出信道总是安全的：另一端的对等实体会在读取完缓存区中的所有数据之后得到一个流结束的通知，从而知道信道已经关闭；除非应用程序知道另一端不会再发送其它数据了，关闭连接的输出信道是比较危险的：另一端的对等实体向已关闭的输入信道发送数据时，操作系统会回送一条\u0026quot;connection reset by peer\u0026quot;的消息。大部分操作系统都会将这种情况作为严重错误来处理，并删除对端还未读取的所有缓存数据。\n参考资料  David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002. HTTP/1.x 的连接管理.  ","href":"/computer_networks/http/connection_management/","title":"HTTP连接管理"},{"content":"HTTP 最初是一个匿名、无状态的请求/响应协议。服务器接收客户端请求，处理并回送响应，Web 服务器几乎没有什么信息可以用来判断请求来自于哪个用户。通常情况下，Web 服务器可能同时和上千个客户端进行通信。现代的Web站点希望能够为不同用户提供个性化体验，因此服务器需要通过某种方式识别出当前对话的用户。常见的用户识别机制有：\n 使用承载有用户身份信息的 HTTP 首部。 追踪客户端的 IP 地址。 通过登录认证来识别用户。 使用 Fat URL，在 URL 中嵌入识别信息。 使用 cookie 。  承载有用户信息的HTTP首部 常见的用来承载用户信息的HTTP首部有：\n   首部名称 首部类型 描述     From Request 用户的Email地址   User-Agent Request 用户的浏览器   Referer Request 用户是从这个页面上面的链接跳转过来的   Authorization Request 用户名和密码   Client-ip Extension (Request) 客户端IP地址   X-Forward-For Extension (Request) 客户端IP地址   Cookie Extension (Request) 服务端生成的ID标签    From 首部包含了用户的 E-mail 地址，这在理想情况下可以用来识别用户。但某些服务器可能会收集这些 E-mail 地址，用于散发垃圾邮件，所以很少有浏览器会发送 From 首部。From 首部一般是由向机器人或蜘蛛这样的自动化程序发送的。\nUser-Agent 可以将用户浏览器的相关信息发送给服务器。这些信息包括程序的名称、版本，甚至操作系统的相关信息等。例如：\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36 Referer 首部可以告诉服务器用户是从哪个页面过来的。服务器可以通过它更好地理解用户的浏览行为。\nFrom、User-Agent、Referer 首部均不足以实现可靠的用户识别。\n通过客户端IP地址识别用户 早期的 Web 服务器尝试将客户端IP地址作为一种识别用户的方式。如果每个用户都有不同的IP地址，并且IP地址很少变化的话，这种方案还是可行的。虽然 Client-IP 不是 HTTP 标准的一部分，但很多代理都会添加这个首部，Web服务器可以通过它找到承载 HTTP 请求的 TCP 连接另一端的IP地址。\n虽然如此，使用客户端IP地址来识别用户还是存在诸多缺点：\n 客户端IP地址描述的是用户所用机器的IP，而不是用户。当多个用户共享同一台计算机时，就无法区分用户了。 很多 ISP 都会为用户动态分配IP地址。用户的IP地址不固定，识别失效。 为了提高安全性和管理稀缺的IP地址资源，很多用户都是通过NAT技术来浏览网络的。这个时候用户的真实IP就被NAT隐藏了，客户端IP成为了NAT设备的IP。 HTTP 代理和网关通常会打开一些新的连接到服务器的TCP连接，这个时候服务器看到的就是代理或网关的地址。有些代理或网关会在特殊的 Client-IP 或 X-Forwarded-For 扩展首部中存放原始的IP地址，但并不是所有的代理都会这么做。  通过登录识别用户 Web服务器除了被动地根据IP来猜测用户身份外，还可以要求用户进行认证来显式地询问用户是谁。\n为了帮助简化Web站点的登录，HTTP 包含了内置了一种将用户信息传送给Web站点的机制，即使用 WWW-Authenticate 和 Authorization 首部。一旦登录，浏览器就会在接下来访问该站点的每个请求中携带登录信息。\n如果服务器希望在访问站点内容之前先登录，它会先给浏览器回送一个 401 Login Required 响应状态码并添加 WWW-Authenticate 首部到响应中。浏览器接着弹出一个登录对话框要求用户输入用户名和密码。用户输入用户名和密码之后，浏览器就会添加一个 Authorization 首部（携带加密后的用户名和密码）并重发原来的请求，服务器将利用它们对用户进行认证。通常，浏览器会保存用户输入的信息，并在往后需要使用用户名和密码的时候自动发送出去。这样一来，只需要登录一次，就可以在整个会话期间维持用户的身份信息了。下图展示了这个过程：\nFat URL 有些Web站点会为每个用户生成特定版本的 URL ，从而追踪用户的身份。通常的做法是：扩展原来的 URL ，往其中加入一些状态信息。当用户浏览站点时，Web服务器会动态生成一些超链接，继续维护 URL 中的状态信息。这些扩展后的包含了用户状态信息的URL就被称为 Fat URL。\n虽然 Fat URL 能够识别用户的身份，但它也存在不少缺点，例如：\n URL 可能会变得又长又丑陋，可能会给用户带来困惑。 当包含了用户状态信息的 URL 被分享给别人时，用户的个人信息也被分享出去了。 生成URL可能会给服务器带来额外的负担。  Cookie Cookie是一种识别用户，实现持久会话的方式，使用非常广泛，也非常重要。\nCookie的类型 Cookie可以粗略地分为两类：会话cookie和持久cookie：\n 会话cookie是一种临时cookie，他记录了用户访问站点时的设置和偏好，会在用户退出浏览器时被删除。 持久cookie的生存时间比较长，因为它们存储在磁盘上。即使浏览器退出，计算机被重启也依然存在。  会话cookie和持久cookie之间的唯一区别就是它们的过期时间。\nCookie的工作原理 当用户首次访问Web站点时，Web服务器对用户一无所知。如果Web服务器希望用户再次回来的话，它就会通过 Set-Cookie 首部给用户返回一个独有的cookie。只要用户在以后的访问中带上这个cookie，服务器就可以识别出用户了。cooKie中包含了一个由name=value信息构成的任意列表。Cookie中可以包含任何信息，浏览器会记住服务器返回报文首部中Set-Cookie的内容并将cookie保存在浏览器的cookie数据库中。当用户在之后再次访问某个站点时，浏览器就会选出该站点分配给用户的cookie并放在Cookie首部中传给服务端。\nCookie的基本思想就是让浏览器维护一组服务器特有的信息，每次访问服务器时都将这些信息提供给它。因为浏览器需要负责cookie的存储，所以cookie也称客户端状态（client-side state），而cookie在HTTP规范中的正式名称则是HTTP state management mechanism。\nCookie的属性 Domain 产生cookie的服务器可以向Set-Cookie首部中添加一个Domain属性来告诉浏览器这个cookie是给哪些站点使用的。例如，Set-Cookie: user=\u0026quot;Tom\u0026quot;; domain=\u0026quot;example.com\u0026quot;告诉浏览器将Cookie: use=\u0026quot;Tom\u0026quot;发给example.com下的所有站点。\nPath Path标识了主机下的那些路径可以接受该Cookie（该路径必须出现在请求URL中）。例如当Path=/docs时，/docs/、/docs/Web/、/docs/web/HTTP都会被匹配。\n参考资料  David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002. HTTP Cookies.  ","href":"/computer_networks/http/client_certification_and_cookies/","title":"客户端身份识别与Cookie"},{"content":"Web缓存指的是一些可以自动保存常用文档副本的HTTP设备。当Web请求经过缓存时，如果缓存设备本地存在一个可用的“已缓存”副本，就不需要再去**源服务器（origin server, 即持有资源实体的服务器）**获取这个文档了，直接将缓存设备本地存储中的文档副本返回给客户端即可。缓存具有不少优点：\n 减少冗余的数据传输，从而节省网络费用。 缓解网络瓶颈问题。有了缓存之后，可以更快地加载页面而不需要更多的带宽。 降低对源服务器的要求。服务器不仅可以更快地响应，还能避免过载的出现。 降低距离时延。因为从较远的地方加载页面会更慢。  缓存命中与不命中 缓存是很有用的，但一个缓存也不可能存储下世界上所有文档的副本。\n当请求到达缓存时，若缓存中存在被请求内容的副本（副本应该是有效的），则可以直接将副本返回给客户端，这种情况被称为缓存命中（cache hit）。否则，缓存中没有可用副本，请求会被转发给源服务器，这种情况被称为缓存不命中（cache miss）。源服务器上的内容可能会发生变化，缓存需要不时地对其进行检测，看看它们保存的副本是否仍然是服务器上的最新内容，这些*新鲜度检测（freshness checks）*就被称为HTTP再验证（HTTP revalidation）。\n再验证 HTTP定义了一些特殊的请求，它们可以不从服务器获取整个对象，并快速检测出内容是否依然是新鲜的。缓存可以在任意时刻，以任意频率对副本进行再验证。由于网络带宽非常珍贵，大部分缓存只有在客户端发起请求并且副本旧得足以需要检测时，才会对副本进行再验证。\n缓存在对副本进行再验证时，会向源服务器发送一个小的再验证请求。如果内容没有发生变化，服务器就会以 304 Not Modified 进行响应。只要缓存确认了副本仍然是有效的，它机会再次将副本标记为新鲜的并将副本提供给客户端，这被称为再验证命中（revalidation hit）或缓慢命中（slow hit）。因为这种方式需要与源服务器进行核对，所以比单纯的缓存命中要慢，但它没有从服务器获取对象数据，所以比缓存不命中要快一些。\nHTTP提供了一些再验证缓存对象的工具，其中最常用的就是 If-Modified-Since 首部。当它被添加到GET请求中时，它告诉服务器只有当对象的副本被缓存并且对象已经被修改后，才发送对象。当服务器收到GET If-Modified-Since请求时，可能处于三种状态:\n 服务器内容未修改（再验证命中）。服务器向客户端发送 304 Not Modified 作为响应。 服务器内容已修改但未删除。这是服务器对象与缓存副本已经不同了，服务器会向客户端发送一条带有完整内容的普通 200 OK 作为响应。 服务器对象已被删除。这个时候服务器会回送一个 404 Not Found 响应，缓存收到后会将对应的副本删除。  缓存的处理步骤 Web缓存对一条HTTP GET报文的处理过程包括以下7个步骤：\n 接收——缓存从网络中读取抵达的请求报文。 解析——缓存对报文进行解析，提取出URL和各种首部。 查询——缓存查询本地是否存在可用副本，如果没有就从服务器获取一份并保存在本地。 新鲜度检测——缓存查看已缓存的副本是否足够新鲜，如果不是，就询问服务器该内容是否有更新。 创建响应——缓存使用新的首部和已缓存的主体来构建一条响应报文。 发送——缓存将响应报文发送给客户端。 日志——缓存向日志文件中新增一个描述这个事务的条目（可选）。  下面的流程图展示了缓存是如何处理GET请求的：\n保持副本的新鲜 HTTP提供了一些机制用来保持已缓存数据与服务器数据之间的一致性。这些机制被称为文档过期（document expiration）和服务器再验证（server revalidation）。\n文档过期 通过HTTP的 Cache-Control 和 Expires 首部，源服务器可以为每个文档附加一个“过期时间”，从而说明在多长时间内可以认为这些内容是新鲜的。\n在缓存文档过期之前，除非客户端请求中包含阻止提供已缓存或未验证资源的首部，缓存可以不与服务器联系并任意使用这些副本。一旦文档过期，缓存就必须与服务器进行核对。\n服务器用HTTP/1.0+的 Expires 首部或HTTP/1.1的 Cache-Control: max-age 首部来指定过期时间。二者所做的事情本质上是一样的，区别在于：前者使用绝对时间，后者使用相对时间。\n服务器再验证 已缓存文档过期了并不意味着它与源服务器上目前处于活跃状态的文档有本质区别，这只是意味着到了核对时间了，这种情况就是服务器再验证，缓存需要询问源服务器文档是否发生了变化：\n 若再验证显示内容发生了变化，则缓存需要获取一份新的副本，替换掉旧的数据，然后发送给客户端。 若再验证显式内容没有变化，则缓存只需要获取新的首部（首部中包含了新的过期日期）并更新缓存中的首部。  使用条件方法进行再验证 HTTP的条件方法可以高效地实现再验证。HTTP允许缓存向源服务器发送一个“条件GET”，请求服务器只有在文档与缓存中的文档副本不同时，才回送对象主体。通过这种方式，单个条件GET就能完成新鲜度检测和对象获取。要发起条件GET请求，需要向GET请求报文的首部中加入以下特殊的条件首部，Web服务器只有在条件为真时才返回对象。\n和缓存相关的条件首部有两个：\n If-Modified-Since: \u0026lt;date\u0026gt;：如果文档在指定日期 \u0026lt;date\u0026gt; 之后被修改了，就执行请求的方法。它一般和 Last-Modified 响应首部配合使用，只有当内容被修改之后才去获取新的内容。 If-None-Match: \u0026lt;tags\u0026gt;：服务器可以为文档提供特殊的标签（ETag），这些标签就像序列号一样，如果已缓存标签与服务器文档中的标签不同，就执行请求方法。  控制缓存 HTTP为服务器定义了几种声明文档在过期前可以被缓存多长时间的方式。按照优先级递减的顺序，服务器可以：\n 附加一个 Cache-Control: no-store 首部到响应中去。 附加一个 Cache-Control: no-cache 首部到响应中去。 附加一个 Cache-Control: must-revalidate 首部到响应中去。 附加一个Cache-Control: max-age 首部到响应中去。 附加一个 Expires 首部到响应中去。 不附加过期信息，让缓存自己确定过期时间。  no-store与no-cache HTTP/1.1提供了几种标记对象不可被缓存的方式。从技术上说，这些不可缓存的页面永远都不应该被存储在缓存中。下面就是几个标记文档不可被缓存的首部：\nPragma: no-cache Cache-Control: no-cache Cache-Control: no-store no-cache 并不意味着\u0026quot;don\u0026rsquo;t cache\u0026quot;，当响应被标记为 no-cache 时，缓存在与源服务器进行新鲜度再验证之前，不能将其提供给客户端。标记为 no-store 的响应禁止缓存保持响应的副本。HTTP/1.1中的 Pragma: no-cache 首部的目的是兼容 HTTP/1.0+。\nmust-revalidate must-revalidate 并不意味着\u0026quot;must revalidate\u0026quot;。Cache-Control: must-revalidate 响应首部告诉缓存，当本地资源的存活时间没有超过 max-age 时，可以直接返回。否则，必须跟源服务器进行再验证。\nmax-age Cache-Control: max-age 首部表示从服务器将文档传来之时起，可以认为该文档处于新鲜状态的秒数。还有一个 s-maxage 首部，其行为与 max-age 类似，但只用于共享（公有）缓存：\nCache-Control: max-age=3600 Cache-Control: s-maxage=3600 服务器可以将 max-age 或 s-maxage 的值设置为 0，从而要求缓存不缓存某个文档或每次访问该文档时都刷新：\nCache-Control: max-age=0 Cache-Control: s-maxage=0 Expires Expires 响应首部已被弃用，因为它声明的是实际的过期时间而不是相对时长。当机器的时钟不同步时，会有问题。\n参考资料  David Gourley, Brian Totty. HTTP: The Definitive Guide. O\u0026rsquo;Reilly Media, 2002.  ","href":"/computer_networks/http/caching/","title":"Web缓存"},{"content":"","href":"/tags/networks/","title":"Networks"},{"content":"网络层的主要功能就是将数据包从源主机路由到目标主机。在大多数网络中，数据包需要经过多跳才能到达目的地。路由算法负责为入境数据包选择一条输出线路。路由算法主要分为两大类：自适应算法和非自适应算法。非自适应算法不会根据当前测量或估计的流量和拓扑结构来调整路由决策，整个过程是静态的。而自适应算法会改变路由决策，路由决策的改变会反映网络拓扑结构的变化，通常也会反映网络流量的变化情况，其过程是动态的。\n最短路径算法 构造一张网络图，图中的每个节点代表一个路由器，每条边代表一条通信线路。为了选择一对给定路由器之间的路由线路，算法只需要找出这对给定路由器之间的最短路径即可。\nDijkstra 算法是一种非常有名的最短路径算法，它能够找出图中源节点到全部目标节点的最短路径。算法要求所有边的权重都为非负值，恰好网络拓扑中不可能存在非负值。\n算法在运行过程中维护了一组节点集合 S，并且从源点 s 出发到集合 S 中的每一个节点之间的最小距离都已经被找到。用 V 表示由图中所有节点构成的集合，算法重复的从节点集 V-S 中选出到源点 s 路径最短节点 u，将 u 加入集合 S，然后更新源点 **s**到集合 V-S 中各节点的估计距离。重复这个过程，直到遍历完所有顶点(集合 V-S 为空)。下图展示了这个过程：\n最短路径算法的缺点在于其需要知道整个网络的拓扑图。\n泛洪算法 泛洪(flooding)算法的思想是：将每一个入境的数据包都发送到除该数据包入境线路以外所有的线路。算法的鲁棒性非常好，只要源主机和目标主机之间存在一条可用通路，数据包便可到达目的地。算法能够选择每一条可能的路径（包括最短路径），但算法会产生大量的重复数据包。\n距离矢量算法 **距离矢量路由(distance vector routing)**算法是一种动态路由算法。算法要求每个路由器都维护一张表，表中记录了当前已知的到网络中其它路由器的最佳距离及所使用的链路。每个路由器回合所有的相邻的其它路由器交换信息并结合获取的信息不断更新自己的表，最终算法收敛时，每个路由器都可以得到到达其它路由器的最佳链路。路由器之间交换信息采用的就是 RIP(路由信息协议(Router Information Protocol)。\n距离矢量路由算法虽然总能收敛到正确的答案，但这个收敛速度可能非常慢。当网络拓扑结构发生变化之后，可能出现无穷计数问题。算法对好消息(一条更近的链路)的反应非常迅速，对坏消息(某条链路不可用时，尝试切换到其它链路)的反应却异常迟缓。问题的核心在于，当B发现自己联系不上C时，A告诉B它有一条通往C的路径，B无从判断自己是否就在A到C的这个路径上面。\n链路状态路由 由于距离矢量算法存在无穷计数问题，它后来被**链路状态路由(link state routing)**算法取代了。\n链路状态路由算法可以用五个部分来描述，每一个路由器必须完成这五个部分：\n 发现邻居节点，并了解邻居节点的网络位置 设置到每个邻居节点的距离或成本度量值 构造一个包含所有刚刚获知的链路状态信息包 将这个包发给所有其他的路由器，并接收来自其它路由器的信息包 计算出到每个其它路由器的最短路径  实际上，算法将完整的网络拓扑结构发给了每一个路由器，然后每个路由器运行Dijstra算法就可以找出自己到其它路由器的最短路径。\n链路状态路由算法在实际的网络中使用非常广泛，典型的有 IS-IS(Intermediate System-Intermediate System) 和 OSPF(Open Shortest Path First) 。\n参考文献  ANDREW S. TANENBAUM, DAVID J. WETHERALL. Computer Networks, Fifth Edition. Pearson, 2011. Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein. Introduction to Algorithms, third Edition. The MIT Press, 2009.  ","href":"/computer_networks/fundamentals/routing_algorithms/","title":"路由算法"},{"content":"延迟（Latency） 和**带宽（Bandwidth）**是衡量网络性能的两个关键指标。\n延迟 延迟是指一个数据包(或一条消息)从发送到被接收这个过程中所花费的时间。\n延迟的构成 对于通过因特网连接的一对客户端-服务器而言，延迟包含以下四部分：\n 传播延迟(Propagation delay)：数据包从发送方到接收方所经过的时间，取决于发送方与接收方之间的距离与信号的传播速率 发送延迟(Transmission delay)：将整个数据包传送到链路上所需时间，取决于包的长度和链路的传输速率 处理延迟(Processing delay)：路由器处理包头，检测错误，以及决定包的目的地所花费的时间 排队延迟(Queuing delay)：当包到达的速度超过了路由器的处理速度时，新抵达的数据包会在一个缓冲区中排队。数据包在队列中排队直到可以被处理所经过的时间就是排队延迟  一般而言，源到目标的距离越长，传播延迟就越长；源到目标之间的路由器越多，发送延迟和处理延迟就越长；路径上的负载越重，出现排队的可能性就越大；\n数据包在传输介质中传播。一旦选定传输介质，包的传播速度其实也就定下来了。虽然难以使数据包传送得更快，但却可以把服务器放到离用户更近的地方，从而减少用户的等待时间，这其实就是CDN的工作原理。\n最后一英里延迟 有趣的是，大量的传播延迟并不是出现在当数据包跨越大洋从一个大陆到另一个大陆的时候，而通常是出现在最后的几英里中。要能够连上Internet，ISP需要路由线缆、聚合信号并转发至路由节点。受连接类型、路由策略、发布技术等影响，数据包从用户到ISP的主路由器会耗费大量的时间。因此，选择一个合适的ISP有时候可以显著降低延迟。\n带宽 带宽是指一个逻辑或物理信道上的最大吞吐率，通常用单位时间内传输的比特数(bit/s)来表示。\n采样定理(Nyquist–Shannon sampling theorem) 采样是将一个信号转换为数字序列的过程。Nyquist曾经推导出一个公式，用来表示一个有限带宽的无噪声信道的最大数据传输速率。后来，Shannon进一步把Nyquist的工作扩展到了有随机噪声(热动力引起)的信道的情形。\nNyquist证明，对于一个任意一个信号，如果这个信号通过了一个带宽为 B 的低通滤波器，那么只要进行每秒 **2B**次确切采样，就可以完全重构出被过滤的信号。如果信号包含了 V 个离散等级，那么Nyquist推导出的公式可以表示为：\n$$ maximum \\ data \\ rate = 2Blog_2V \\ bits/sec $$\n通过Nyquist定理，我们可以知道：在无噪声的1kHz信道上不可能以超过2000bit/s的速率传输二进制(离散等级为2)信号。\n对于任何一个信道，由于系统中分子的运动，随机(热)噪声总是存在的。信号功率与噪声功率的比值被称为 信噪比(SNR, Signal-to-Noise Ratio)。如果将信号功率和噪声功率分别记作 S和 N，那么信噪比为：S/N。通常情况下，为了表示更大范围，信噪比会被写成对数形式\n$$ 10log_{10}S/N $$\n对数的取值单位称为分贝(dB, decibel)。例如，10 的信噪比为 10 分贝，而 100 的信噪比为 20 分贝。\nShannon的成果为：对于一个带宽为 **B**Hz，信噪比为 S/N 的有噪声信道，其最大速率为：\n$$ maximum \\ number \\ of \\ bits/sec = Blog_2(1 + S/N) $$\n这告诉了我们实际信道能够获得的最大容量。\n参考资料  Ilya Grigorik. High Performance Browser Networking. O\u0026rsquo;Reilly Media, 2013. ANDREW S. TANENBAUM, DAVID J. WETHERALL. Computer Networks, Fifth Edition. Pearson, 2011.  ","href":"/computer_networks/fundamentals/latency_and_bandwidth/","title":"延迟与带宽"},{"content":"IP地址 IPv4采用的是32位地址，而Ipv6采用的是128位地址。Internet上的每台主机和路由器都有IP地址，但一个IP地址并不真正指向一台主机或路由器，而是指向一个网络接口。如果一台主机在两个网络上，那么它必须拥有两个IP地址。不过，大多数主机都在一个网络内，只有一个IP地址，但路由器有多个接口，从而有多个IP地址。\nIPv4 IPv4的书写格式是点分十进制表示法，将4个字节中的每个字节都写成一个0~255之间的整数，然后用.分隔。例如：32位的16进制地址80D00297可以写成128.208.2.151。\n分类和特殊寻址 在1993年以前，IP地址被分为5类，这种分配被称为 分类寻址(classful addressing)。一个IPv4地址又32比特组成，其中包括一个网络号(唯一标识互联网中的某个子网)、一个主机号(唯一标识网络内的一台主机)。\nIP地址被划分为了上图中的A、B、C、D、E五个类别。今天，表明一个IP地址是否属于A、B或C类网络的标志位已不再使用。\n有一些地址有着特俗的含义。全0这个地址(0.0.0.0)在主机启动的时候用，表示“这个网络”或“这个主机”。全1(255.255.255.255)这个地址表示指定网络中的所有主机，它允许在本地网络上进行广播。网络号为全0的地址表示本地网络的所有主机。主机号为全1并具有正确网络号的地址允许在该网络内进行广播。所有127.xx.yy.zz形式的地址保留，给本地环路测试用。\n到了1990前后，按照当时IP地址的分配速度，到1996年前后就可能用完所有的IP地址，当时采取了3种解决方案：\n 开发新的IP协议和寻址方案，结果有了现在的IPv6 修改当前IP地址的分配方案，结果有了CIDR(Classless Inter Domain Routing, 无类域间路由) 让未注册的计算机通过NAT的方式访问互联网  CIDR CIDR中已经没有IP地址分类的概念了，它允许使用任意长度的地址前缀，提高了IP地址空间的利用率。\n每个32位的地址由高位的可变长网络和低位的主机两部分组成。同一个网络上的所有主机地址中的网络值是相同的，这意味着一个网络对应一块连续的IP地址空间，这块地址空间就被称为地址的 前缀(prefix)。前缀长度相当于网络部分中1的二进制掩码，例如前缀长度为24，其可以写成255.255.255.0或/24，这种写法被称为 子网掩码(subnet mask)。子网掩码可以和一个IP地址进行与运算，运算结果为该IP的网络部分。\n将多个小前缀的地址块合并成一个大前缀的地址块的过程称为 路由聚合(route aggregation)。\nCIDR的工作原理：当一个数据包到达时，路由器扫描路由表以便确定目的地是否在前缀的地址块内。这个时候有可能匹配到多个具有不同前缀的表项，这种情况下使用具有最长前缀的表项。\nNAT NAT(Network Address Translation) 的思想是ISP为每个家庭或每个公司分配一个IP地址(或者尽可能少的分配IP地址)，用这个分配的IP地址来传输Internet流量。在网络内部，每台主机都有其唯一的IP地址，该地址用来进行网络内部路由。当一个数据包需要从网络内部去往其它网络时，必须进行地址转换，把其内部IP地址换成共享的那个IP地址。\n有三类保留的IP地址可供内部网络主机使用：\n   范围 数量     10.0.0.0 ~ 10.255.255.255/8 16777216   172.16.0.0 ~ 172.31.255.255/12 1048576   192.168.0.0 ~ 192.168.255.255/16 65536    最常用的NAT寻址算法的工作流程如下：\n 当内部网络上的主机发送一个TCP或UDP包给网络外的主机时，路由器将数据包中的源IP地址和端口号保存为地址转换表中的一条记录 路由器用自己的IP替换掉数据包中的源IP地址，用一个虚拟的端口号替换源端口，这个虚拟的端口号指向包含发送主机地址信息的那条记录 路由器将修改了源IP地址和源端口的数据包转发给目标主机 当路由器从外部主机接收到一个TCP或UDP包时，它使用包中的目的端口号访问地址转换表中的记录，并用记录的内容替换数据包中的目的地址和端口号，然后将修改后的数据包转发给内部网络中的主机  IPv6 IPv6不仅解决了IPv4地址耗尽的问题，还做了很多的改进。IPv6地址由16个字节组成，16个字节被分成8组来书写，每一组4个16进制数字，组之间用冒号隔开。\n参考资料  ANDREW S. TANENBAUM, DAVID J. WETHERALL. Computer Networks, Fifth Edition. Pearson, 2011.  ","href":"/computer_networks/fundamentals/ip_address/","title":"Ip寻址"},{"content":"","href":"/tags/zookeeper/","title":"ZooKeeper"},{"content":" ZooKeeper是一个开源的分布式协调服务，可以为分布式计算环境维护配置信息、提供命名、同步、分组等服务。\n 单机部署 准备Java环境 ZooKeeper是由Java编写的，可以在JDK 7及以上版本中运行。在正式运行ZooKeeper之前，需要配置好Java环境以及JAVA_HOME。\n下载 首先下载最版本的ZooKeeper，然后解压。写这篇文章的时候，最新的稳定版是3.5.5，所以我这里下载的也是这个版本。为了加快下载速度，我使用了清华大学的镜像站。\nwget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-3.5.5-bin.tar.gz tar -xzvf apache-zookeeper-3.5.5-bin.tar.gz 创建配置文件 为了启动ZooKeeper，我们需要一份配置文件。启动时如果不指定配置文件名，ZooKeeper默认会去安装目录下的conf目录中寻找一个名为zoo.cfg的文件，并从中读取配置信息。先创建一份配置文件：\ncd apache-zookeeper-3.5.5-bin vim conf/zoo.cfg 下面是zoo.cfg文件内容的一个示例，文件内容参考自ZooKeeper Getting Started Guide:\ntickTime=2000 dataDir=/opt/Apache/apache-zookeeper-3.5.5-bin/data/standalone # 数据目录 clientPort=2181 其实压缩包里附带了一份配置文件的样本zoo_sample.cfg，也可以参考这个文件进行修改。默认情况下，ZooKeeper内置的AdminServer使用了8080端口，如果安装机器上的8080端口已被使用，需要新增一个配置项，例如：\nadmin.serverPort=9090 # 使用9090端口作为AdminServer通信端口 还可以自定义ZooKeeper的日志文件夹，比如：\ndataLogDir=/opt/Apache/apache-zookeeper-3.5.5-bin/logs/standalone 启动ZooKeeper 准备好配置文件之后，就可以启动ZooKeeper了:\nbin/zkServer.sh start 若想显式指定使用配置文件zoo.cfg，则可运行：\nbin/zkServer.sh start zoo.cfg 若看到出错信息，则ZooKeeper启动成功。可进一步查看当前运行状态：\nbin/zkServer.sh status 连接ZooKeeper bin/zkCli.sh -server 127.0.0.1:2181 停止ZooKeeper bin/zkServer.sh stop 伪集群部署 很多时候，单机版的ZooKeeper已经能够满足我们的测试要求了。有时候，我们也需要模拟集群环境。ZooKeeper推荐在集群中使用奇数台机器。下面将部署的是3节点的伪集群。\n准备配置文件 集群模式的配置文件在单机的基础上增加了一些和集群相关的信息，下面是一个例子，在conf目录中建立文件zoo1.cfg：\ntickTime=2000 dataDir=/opt/Apache/apache-zookeeper-3.5.5-bin/data/cluster/zk1 clientPort=2182 initLimit=10 syncLimit=5 dataLogDir=/opt/Apache/apache-zookeeper-3.5.5-bin/logs/cluster/zk1 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 上面的server.X中，X代表服务器的id。服务器在启动时，会去读取dataDir目录下一个名为myid的文件。myid文件的内容即服务器的id。\n同理，zoo2.cfg的内容为：\ntickTime=2000 dataDir=/opt/Apache/apache-zookeeper-3.5.5-bin/data/cluster/zk2 clientPort=2183 initLimit=10 syncLimit=5 dataLogDir=/opt/Apache/apache-zookeeper-3.5.5-bin/logs/cluster/zk2 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 zoo3.cfg:\ntickTime=2000 dataDir=/opt/Apache/apache-zookeeper-3.5.5-bin/data/cluster/zk3 clientPort=2184 initLimit=10 syncLimit=5 dataLogDir=/opt/Apache/apache-zookeeper-3.5.5-bin/logs/cluster/zk3 server.1=127.0.0.1:2887:3887 server.2=127.0.0.1:2888:3888 server.3=127.0.0.1:2889:3889 因为是伪集群，三个配置文件中的各个端口不一样，如果是真集群，端口可以是一样的。\n准备myid文件 分别在上面三个配置文件指定的dataDir中建立一个名为myid的文件，文件内容为服务器的id：\necho \u0026#34;1\u0026#34; \u0026gt;\u0026gt; /opt/Apache/apache-zookeeper-3.5.5-bin/data/cluster/zk1/myid echo \u0026#34;2\u0026#34; \u0026gt;\u0026gt; /opt/Apache/apache-zookeeper-3.5.5-bin/data/cluster/zk2/myid echo \u0026#34;3\u0026#34; \u0026gt;\u0026gt; /opt/Apache/apache-zookeeper-3.5.5-bin/data/cluster/zk3/myid 启动伪集群 通过指定配置文件的方式启动各个节点：\nbin/zkServer.sh start zoo1.cfg bin/zkServer.sh start zoo2.cfg bin/zkServer.sh start zoo3.cfg 查看各节点状态：\nbin/zkServer.sh status zoo1.cfg bin/zkServer.sh status zoo2.cfg bin/zkServer.sh status zoo3.cfg 停止集群：\nbin/zkServer.sh stop zoo1.cfg bin/zkServer.sh stop zoo2.cfg bin/zkServer.sh stop zoo3.cfg ","href":"/posts/getting_started/zookeeper%E5%8D%95%E6%9C%BA%E4%B8%8E%E4%BC%AA%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","title":"ZooKeeper单机与伪集群部署"},{"content":" Redis是一个位于内存中的数据结构存储系统，由 ANSI C 语言编写。可用作数据库、缓存和消息中间件。它支持的数据结构有：string、hash、list、set、sorted set with range quries、bitmap、hyperloglogs、geospatial indexes with radius queries以及stream。Redis支持复制、Lua脚本、基于LRU的键驱逐、事务和不同级别的磁盘持久化，并通过哨兵和Redis集群的自动分区机制来保证高可用。\n Redis 是 Remote dictionary server 的缩写，字母意思即远程字典服务。一个 Redis 可以有多个存储数据的字典，客户端可以通过 select 来选择字典（DB）进行存储。\n Redis的存储是基于key-value的，但这里的key-value不是简单的key-value，因为value的类型可以有很多种。在我们传统的key-value存储中，key和value都是字符串，而在Redis中，value不仅可以是字符串，还有可能是像列表、集合这样的复杂的数据结构。\n 本文主要内容包括：Redis的安装、键和几种常见的数据类型的使用，以及和数据生存期相关的一些操作。\n安装Redis 参见Redis Quick Start。\nRedis键 Redis的键是一个字符串。在Redis中，字符串二进制安全的，也就是说：Redis中的字符串可以包含任何类型的数据(比如：一张图片、一个序列化后的Java对象……)，这些也可以成为Redis中的键。\n注意：空字符串(\u0026quot;\u0026quot;)也是一个有效的键。\n数据类型 Strings 字符串(string)是Redis最基本的数据类型，最大不能超过512MB。\nRedis提供了20多个操作string类型的命令。下面是一些例子：\n\u0026gt;\u0026gt; set s1 \u0026quot;Hello, Redis\u0026quot; // 设置s1的值为\u0026quot;Hello, Redis\u0026quot; OK // 成功返回OK \u0026gt;\u0026gt; get s1 // 获取值 \u0026quot;Hello, Redis\u0026quot; \u0026gt;\u0026gt; setnx s1 \u0026quot;Redis\u0026quot; // 尝试为s1设置新值 0 // s1已有值，不进行操作，返回0 \u0026gt;\u0026gt; get s1 \u0026quot;Hello, Redis\u0026quot; \u0026gt;\u0026gt; set s1 2 // 设置值 OK \u0026gt;\u0026gt; get s1 \u0026quot;2\u0026quot; \u0026gt;\u0026gt; incr s1 // 加1 3 \u0026gt;\u0026gt; incrby s1 10 // 加10 13 \u0026gt;\u0026gt; decr s1 // 减1 12 \u0026gt;\u0026gt; decrby s1 5 // 减5 7 \u0026gt;\u0026gt; incrbyfloat s1 1.1 // 加1.1 8.1 \u0026gt;\u0026gt; incrbyfloat s1 -3.1 // 减3.1 5 SET 和 GET 分别用来设置和检索key对应的值(这个值是一个字符串)。需要注意的是，如果某个key已经有对应的值，SET 命令会覆盖掉已有的值(不管之前的值是何种类型都会被覆盖)，和这个key关联的TTL也会被丢弃。如果不希望已有key的值被覆盖，可以使用 SETNX 命令，当key存在时，SETNX 不会进行任何操作。\n虽然值是string，但如果这个值是一个数字的字符串形式的话，可以对其进行加减操作。上面的例子中，s1的值被重新修改为了\u0026quot;2\u0026quot;，随后执行了 INCR 和 INCRBY 两个加法命令，值变成了13，之后的减法命令 DECR 和 DECRBY 将值减到了7。更加有趣的是，Redis还提供了 INCRBYFLOAT 以支持浮点加减运算。\n\u0026gt;\u0026gt; mset s1 1 s2 2 s3 a OK \u0026gt;\u0026gt; mget s1 s2 s3 [ \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;a\u0026quot; ] 有时候，一次性设置或获取多个值是很有意义的，这可以通过 MSET 和 MGET 命令实现。类似的还有 MSETNX 命令。\nLists 在Redis中，列表(list)表示由一些元素组成的有序序列。通常情况下，List的实现有数组和链表两种方式，两者各有优劣。数组实现的List可以通过索引快速访问元素，但在插入或删除元素的时候较慢；链表实现的List在插入或删除元素时非常迅速，但访问元素时较慢。Redis中的list使用链表实现，因为对于一个数据库系统来说，快速向一个非常长的列表中添加元素至关重要。基于链表的实现还带来了一个重大优势，那就是可以快速的选取列表中的某一部分。\nRedis提供了10多个操作list的命令，下面时一些例子：\n\u0026gt;\u0026gt; lpush list1 a 1 b // 向列表list1的头部依次插入a、1、b这三个元素 3 // 成功插入，当前列表list1含有3个元素 \u0026gt;\u0026gt; lrange list1 0 -1 // 查看list1 1) \u0026quot;b\u0026quot; 2) \u0026quot;1\u0026quot; 3) \u0026quot;a\u0026quot; \u0026gt;\u0026gt; rpush list1 c d // 向列表list1尾部依次插入c、d这两个元素 5 // 插入成功，当前列表list1含有5个元素 \u0026gt;\u0026gt; lrange list1 0 -1 1) \u0026quot;b\u0026quot; 2) \u0026quot;1\u0026quot; 3) \u0026quot;a\u0026quot; 4) \u0026quot;c\u0026quot; 5) \u0026quot;d\u0026quot; \u0026gt;\u0026gt; lpop list1 // 移除并返回列表list1中的第一个元素 b \u0026gt;\u0026gt; rpop list1 // 移除并返回列表list2中的最后一个元素 d \u0026gt;\u0026gt; lrange list2 0 -1 // 列表list2是空的 \u0026gt;\u0026gt; lpop list2 // 从一个空的列表里面移除元素会返回NULL (nil) \u0026gt;\u0026gt; rpop list2 (nil) \u0026gt;\u0026gt; lrange list1 0 -1 1) \u0026quot;1\u0026quot; 2) \u0026quot;a\u0026quot; 3) \u0026quot;c\u0026quot; \u0026gt;\u0026gt; lindex list1 0 // 查看列表list1中处于位置0处的元素 1 \u0026gt;\u0026gt; lindex list1 -1 // 查看列表list1中最后一个元素 c 命令 LPUSH 和 RPUSH 分别用来向列表头部(左侧)和尾部(右侧)添加元素，LPOP 和 RPOP 分别用来从列表头部(左侧)和尾部(右侧)移除元素。在一个空的列表上执行 LPOP 或 RPOP 将返回 NULL 。有时候，我们希望只有当列表中有元素时，才执行POP操作，这时候可以使用 BLPOP 和 BRPOP 。两个命令的都是从列表中移除元素，只是方向不一样。以 BLPOP 为例，BLPOP 的完整命令如下：\nBLPOP key [key ...] timeout BLPOP 接收一个或多个key以及一个超时时间timeout(可阻塞时间，单位为秒，0代表一直阻塞)。在这些key中，如果有key对应的列表不为空，将会返回第一个非空列表的key及POP出的值。举个例子：\n\u0026gt;\u0026gt; lrange list1 0 -1 // 列表list1包含两个元素 1) \u0026quot;a\u0026quot; 2) \u0026quot;c\u0026quot; \u0026gt;\u0026gt; lrange list2 0 -1 // 列表list2为空 \u0026gt;\u0026gt; blpop list2 list1 0 // list1中的第一个元素被移除 1) \u0026quot;list1\u0026quot; 2) \u0026quot;a\u0026quot; BLPOP 依次检查list2和list1，然后移除并返回list1的头部元素(list2是一个空列表)。\n如果 BLPOP 命令后面给出的的key对应的都是空列表，BLPOP 就会阻塞当前连接。一旦另外一个客户端向某一个key对应的列表插入元素，BLPOP 就会解除阻塞并返回。若阻塞的时间超过了timeout，将返回NULL。\n若要查看列表中的元素，可以使用 LRANGE 命令，LRANGE 命令需要两个索引作为参数。这两个索引形成一个区间，分别指向待返回的第一个和最后一个元素在列表中的位置。索引可以是负数，0、-1、-2分别代表链表中的第一个、最后一个和倒数第二个元素。除了区间查看外，还可以使用 LINDEX 命令查看指定位置的元素。\n\u0026gt;\u0026gt; llen list1 // 查看列表list1的长度 3 \u0026gt;\u0026gt; ltrim list1 1 2 // 修剪列表list1，只保留位于区间[1,2]中的元素 OK \u0026gt;\u0026gt; lrange list1 0 -1 // 查看list1中所有元素 1) \u0026quot;a\u0026quot; 2) \u0026quot;c\u0026quot; LLEN 用于查看list的长度。很多时候，我们只需要保留列表中的某一部分，这个时候 LTRIM 就可以排上用场了，它和 LRANGE 类似，也接收两个索引作为参数，但它将列表的内容设置为区间内的元素并去除区间外的所有元素。上面的例子中，LTRIM 告诉Redis只保留列表list1处于区间[1,2]中的元素，然后丢弃其它的元素。\nHashes Redis中的哈希表(hash)是一个字符串类型的field-value映射表，非常适合用来表示对象。\n下面是一些例子：\n\u0026gt;\u0026gt; hset user:1 name Tome // 将哈希表user:1的name字段的值设置为Tome 1 \u0026gt;\u0026gt; hget user:1 name // 获取哈希表user:1中name字段对应的值 Tome \u0026gt;\u0026gt; hmset user:2 name Bob age 20 education barchelor // 一次向哈希表user:2中插入多个filed-value OK \u0026gt;\u0026gt; hmget user:2 name age // 获取哈希表user:2中name和age字段对应的值 [ \u0026quot;Bob\u0026quot;, \u0026quot;20\u0026quot; ] \u0026gt;\u0026gt; hgetall user:2 // 获取哈希表user:2中所有字段和值 { \u0026quot;name\u0026quot;: \u0026quot;Bob\u0026quot;, \u0026quot;age\u0026quot;: \u0026quot;20\u0026quot;, \u0026quot;education\u0026quot;: \u0026quot;barchelor\u0026quot; } \u0026gt;\u0026gt; hget user:1 age // 企图获取一个不存在的字段对应的值，会返回NULL (nil) \u0026gt;\u0026gt; hexists user:1 age // 检查哈希表user:1中是否存在字段age 0 \u0026gt;\u0026gt; hvals user:2 // 返回哈希表user:2中所有的值 [ \u0026quot;Bob\u0026quot;, \u0026quot;20\u0026quot;, \u0026quot;barchelor\u0026quot; ] \u0026gt;\u0026gt; hincrby user:2 age 2 // 将哈希表user:2的age字段对应的值加2 22 \u0026gt;\u0026gt; hget user:2 age 22 Sets Redis中的集合(set)是一个由字符串构成的无序集合。除了基本的插入、删除、存在性检测等操作，Redis还支持集合的交、并、差计算。\n下面是基本操作的一些例子：\n\u0026gt;\u0026gt; sadd set1 red blue white black // 向集合set1中插入4个元素 4 \u0026gt;\u0026gt; smembers set1 // 查看集合set1内容 [ \u0026quot;black\u0026quot;, \u0026quot;white\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot; ] \u0026gt;\u0026gt; sismember set1 yellow // 检查yellow是否在set1中 0 // 集合set1不包含yellow \u0026gt;\u0026gt; scard set1 // 查看集合set1的基数(元素个数) 4 \u0026gt;\u0026gt; spop set1 // 随机从集合set1中移除一个元素 white \u0026gt;\u0026gt; srem set1 white // 从集合set1中删除一个不存在的元素 0 \u0026gt;\u0026gt; srem set1 black // 从集合set1中删除一个存在的元素 1 \u0026gt;\u0026gt; smembers set1 [ \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot; ] 下面是集合运算：\n\u0026gt;\u0026gt; sadd set2 a b c red blue gold 6 \u0026gt;\u0026gt; smembers set2 [ \u0026quot;red\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;gold\u0026quot; ] \u0026gt;\u0026gt; sunion set1 set2 // set1 + set2 [ \u0026quot;a\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;gold\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;b\u0026quot; ] \u0026gt;\u0026gt; sinter set1 set2 // set1 x set2 [ \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot; ] \u0026gt;\u0026gt; sdiff set2 set1 // set2 - set1 [ \u0026quot;a\u0026quot;, \u0026quot;gold\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;b\u0026quot; ] \u0026gt;\u0026gt; sadd set3 f 1 \u0026gt;\u0026gt; sdiff set2 set1 set3 // set2 - set1 - set3 [ \u0026quot;a\u0026quot;, \u0026quot;gold\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;b\u0026quot; ] Sorted sets Redis中的有序集合(sorted set)和集合(set)类似，存放不重复的字符串。不过，有序集合中的每个元素都有一个对应的浮点分数(score)，这个分数用来维持集合的有序性。由于是有序的，有序集合又具有哈希表的快速访问的优势。\n考虑有序集合中的两个元素A和B，有序集合的有序性基于以下两点：\n 如果A.score \u0026gt; B.score，那么：A \u0026gt; B。 如果A.socre = B.score，A \u0026gt; B的前提是A的字典序大于B。因为集合的元素都是唯一的，A和B的内容不能相同。  下面是一些例子：\n\u0026gt;\u0026gt; zadd z1 2 a -1 b // 向有序集合z1中添加a(score=2)和b(score=-1)， ZADD还可以用来更新元素对应的分数 2 \u0026gt;\u0026gt; zrange z1 0 -1 // 查看z1内容(正序)，因为b.score \u0026lt; a.score，所有b在前面 [ \u0026quot;b\u0026quot;, \u0026quot;a\u0026quot; ] \u0026gt;\u0026gt; zrange z1 0 -1 withscores // 查看z1内容及对应分数 [ \u0026quot;b\u0026quot;, \u0026quot;-1\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;2\u0026quot; ] \u0026gt;\u0026gt; zrevrange z1 0 -1 // 查看z1内容(逆序) [ \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot; ] \u0026gt;\u0026gt; zrangebyscore z1 0 inf // 查看z1中分数在[0, inf)内的内容 [ \u0026quot;a\u0026quot; ] \u0026gt;\u0026gt; zrank z1 b // 查看z1中内容b的排名 0 \u0026gt;\u0026gt; zremrangebyscore z1 -inf -1 // 移除z1中分数位于(-inf,1]的所有元素 1 \u0026gt;\u0026gt; zrange z1 0 -1 [ \u0026quot;a\u0026quot; ] 有序集合还支持很多的命令，比如：ZPOPMAX、ZRANGEBYLEX、**ZUNIONSTORE**等等。\nBitmaps 严格地说，Bitmap并不是一种新的数据类型，而是基于string的一种数据类型，它提供了一些基于比特位的操作。\n相关的命令分为两种：操作单个比特位的和操作一组比特位的。\n比特位的设置和检索使用的是 BITSET 和 BITGET 命令:\n\u0026gt;\u0026gt; setbit bits 2 1 0 \u0026gt;\u0026gt; getbit bits 2 1 \u0026gt;\u0026gt; getbit bits 1 0 BITSET给指定比特位设置值，然后返回该位置上原来的值，企图使用 BITSET 设置0和1以外的值会导致错误。 BITGET 获取指定位置的值， 如果给定的位置超出了存储用的字符串的长度，将会返回0。\n操作一组比特位的命令分为三种：\n BITOP 进行字符串之间的按位与、按位或、按位异或以及按位取反操作。 BITCOUNT 进行计数操作，返回设置为1的比特位的个数。 BITPOS 寻找给定的0或1出现的第一个位置。  HyperLogLogs Redis使用HyperLogLog进行计数，这个算法是基于统计的。Redis的实现中，标准误差只有1%，并且在最坏的情况下只需要消耗 12KB 的内存。HyperLogLog在技术上是一种不同的数据结构，但也是基于string实现的。\n我们使用 SADD 向集合中添加元素，类似的，我们也可以使用 PFADD 向HyperLogLog中添加元素。实际上，HyperLogLog并不存储我们添加的元素，只是更新内部状态。\n\u0026gt;\u0026gt; pfadd hll a b c d // 向hll中加入四个元素 1 \u0026gt;\u0026gt; type hll // 查看hll类型 string // HyperLogLog实际为string \u0026gt;\u0026gt; pfcount hll // 对hll进行计数 4 \u0026gt;\u0026gt; pfadd hll1 a b c d 1 \u0026gt;\u0026gt; pfadd hll2 c d e f 1 \u0026gt;\u0026gt; pfmerge hll3 hll1 hll2 // 合并hll1和hll2到hll3 OK \u0026gt;\u0026gt; pfcount hll3 6 Geospatial indexes Redis在3.2.0版本中加入了地理空间(geospatial)这一数据类型，并支持索引半径查询功能。一个具体的位置信息由三元组(longtitude, latitude, name)确定，当向某一个key添加数据时，数据会被存储为有序集合，这么做为半径查询 GEORADIU 提供了支持。\n下面是一些例子：\n\u0026gt;\u0026gt; geoadd municipalities 116.4551113869 39.6733986505 beijing 121.6406041505 30.8267595167 shanghai 106.6992091675 29.3055601981 chongqing // 添加3个地理空间数据 3 \u0026gt;\u0026gt; geodist municipalities beijing chongqing // 查看beijing和chongqing直接的距离(单位为米) 1457336.8906 \u0026gt;\u0026gt; georadius municipalities 116 40 1000 km // 查看以经度116、纬度40为中心，1000km为半径内的所有位置 [ \u0026quot;beijing\u0026quot; ] \u0026gt;\u0026gt; geohash municipalities beijing shanghai // 查看beijing和chongqing的Geohash表示 [ \u0026quot;wx4cdn242c0\u0026quot;, \u0026quot;wtqrrgzfzs0\u0026quot; ] \u0026gt;\u0026gt; geopos municipalities chongqing // 查看chongqing的地理空间数据 [ [ \u0026quot;106.69921070337295532\u0026quot;, \u0026quot;29.30556015923176716\u0026quot; ] ] \u0026gt;\u0026gt; georadiusbymember municipalities chongqing 1500 km // 查看以chongqing为中心、1500km为半径内的所有位置 [ \u0026quot;chongqing\u0026quot;, \u0026quot;shanghai\u0026quot;, \u0026quot;beijing\u0026quot; ] Streams Stream是Redis 5.0中新增加的数据类型，它以更加抽象的方式模拟了日志结构，通常实现为一个仅以追加模式打开的文件。stream涉及到的内容比较多，后面会详细了解。这里先跳过。\n数据过期 我们可以为某个key设置过期时间(expires)。当expires达到时，对应的key就会被自动删除，就像我们显式的执行 DEL 命令一样。过期时间的单位可以是秒，也可以是毫秒。关于expires的信息都存放在磁盘上，且有备份。这意味着，即使Redis服务器停止运行，过期时间仍然会有效。实际上，Redis保存的是key过期的确切时间，而不是剩余生存时间，虽然参数是秒或毫秒。\n下面是一些例子：\n\u0026gt;\u0026gt; set s1 v1 OK \u0026gt;\u0026gt; expire s1 10 // 设置s1于10秒后过期 1 \u0026gt;\u0026gt; ttl s1 // 查看s1的剩余生存时间(单位为秒) 7 \u0026gt;\u0026gt; pttl s1 // 查看s1的剩余生存时间(单位为毫秒) 3042 \u0026gt;\u0026gt; get s1 // 查看s1的值，未经过10秒 \u0026quot;v1\u0026quot; \u0026gt;\u0026gt; get s1 // 查看s1的值，超过10秒，s1已被删除 (nil) \u0026gt;\u0026gt; set s2 v2 ex 10 // 在设置值的同时指定过期时间为10秒 OK \u0026gt;\u0026gt; set s3 v3 px 10000 // 在设置值的同时指定过期时间为10000毫秒 OK ","href":"/posts/redis/redis_at_first_sight/","title":"初见Redis"},{"content":"快速方法 使用root操作\nStep 1: 添加一个用户 adduser username Step 2: 授予root权限 usermod -aG sudo username 但是\u0026hellip;\u0026hellip; 有些时候，这并不管用\n在vultr新租的Debian VPS上，我想添加sarkar这个用户，并授予它root权限。依然执行了上面的命令，但可怜的sarkar还是一个普通用户。解决方案如下：\nStep1: 安装sudo # apt-get install sudo Step2: 修改/etc/sudoers # cd /etc # chmod +w sudoers # vim sudoers 找到下面内容：\n# User privilege specification root\tALL=(ALL:ALL) ALL 在后面添加一行，声明sarkar的权限：\n# User privilege specification root\tALL=(ALL:ALL) ALL sarkar ALL=(ALL:ALL) ALL 保存并退出。然后：\n# chmod -w sudoers 使/etc/sudoers再次成为不可写的状态。\n现在，sarkar就具有root权限了。\n","href":"/posts/linux/linux%E6%B7%BB%E5%8A%A0%E7%94%A8%E6%88%B7%E5%B9%B6%E6%8E%88%E4%BA%88root%E6%9D%83%E9%99%90%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/","title":"Linux添加用户并授予root权限的简单方法"},{"content":" 代理模式（Proxy Pattern）:给某一个对象提供一个代理，并由代理对象控制对原对象的引用。\n 代理模式中引入了一个新的代理对象，代理对象可以在客户端和目标对象之间起到中介作用，去掉客户端不应该看到的内容或服务或者给客户端提供额外的服务。\n结构图 代理模式的结构图如下：\n从结构图可以看出，代理模式主要包含3个角色：\n Suject（抽象主题类）：它声明了真实主题和代理主题共有的接口，似的在任何使用真是主题的地方都可以使用代理主题。 RealSubject（真实主题类）：它定义了代理角色所代表的真是对象，包含真实的业务操作，客户端可以通过代理对象间接的调用真实主题角色中定义的操作。 Proxy（代理类）：代理类包含一个对真实主题类的引用，从而可以在任何时候操纵真实主题对象。通常在代理类中，客户端在调用真实主题的方法的前后还会进行一些其它的操作，这可以由代理类提供。  模式实现 根据代理模式的结构图，可以写出如下的示例代码：\n// 抽象主题接口 public interface Subject { void request(); } // 真实主题类 public class RealSubject implements Subject{ @Override public void request() { System.out.println(\u0026#34;Request of RealSubject instance.\u0026#34;); } } // 代理主题类 public class Proxy implements Subject { // 维持一个对真实主题类的引用  private RealSubject realSubject = new RealSubject(); public void preRequest(){ System.out.println(\u0026#34;PreRequest.\u0026#34;); } public void postRequest(){ System.out.println(\u0026#34;PostRequest.\u0026#34;); } @Override public void request() { preRequest(); System.out.println(\u0026#34;request of Proxy.\u0026#34;); realSubject.request(); postRequest(); } } 测试用的客户端类：\n// 客户端类 public class Client { public static void main(String[] args){ Proxy proxy = new Proxy(); proxy.request(); } } 运行结果：\nPreRequest. request of Proxy. Request of RealSubject instance. PostRequest. 可以发现，客户端通过代理类调用了真实主题类的方法，同时还调用了一些其它的方法。\n几种常用的代理  远程代理（Remote Proxy）: 又称Ambassador，它为一个位于不同地址空间的对象提供一个本地的代理对象。 虚拟代理（Virtual Proxy）： 如果要创建一个资源消耗较大的对象，可以先创建一个资源消耗较小的对象来代替，真实的对象只有在被需要的时候才创建。 保护代理（Protect Proxy）： 控制对一个对象的访问，可以给不同的用户提供不同的反问权限。 缓冲代理（Cache Procy）： 为某一个目标操作的结果提供存储空间，供多个客户端共享。 智能引用代理（Smart Rererence Proxy）： 当一个对象被引用的时候，提供一些额外的操作。  总结 可以根据不同的应用场合选择不同的代理类型。代理模式能够协调调用者和被调用者，在一定程度上降低了系统的耦合读，符合迪米特法则。客户端可以针对抽象接口编程，增加和更换代理类无须修改原有代码，符合开闭原则，利于系统的扩展。不同类型的代理为不同的应用场景提供合适的解决方案。\n由于代理对象出现在客户端和真实对象之间，这加大了客户端和真实对象之间的距离，可能会降低请求的处理速度。同时，代理模式的实现也需要额外工作，这可能加大系统的复杂性。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%81%E4%B8%89%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之十三：代理模式"},{"content":"","href":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式"},{"content":" 享元模式（Flyweight Pattern）：运行共享技术有效的支持大量 细粒度 对象的复用，又称轻量级模式。\n 享元模式以共享的方式高效的支持大量细粒度对象的重用。实现共享的关键是区分了 内部状态（Intrinsic State） 和 外部状态（Extrinsic State） 。\n 内部状态：内部状态是享元内部不会随着外部条件改变而改变的状态，是可以共享的。 外部状态：外部状态会随着外部条件的改变而改变的状态，是不可共享的。  一旦区分了内部状态和外部状态，就可以将具有相同内部状态的对象存储到享元池中。当需要新的对象的时候，就可以先查看享元池，如果享元池有符合要求的对象，就可以将其取出，再注入不同的外部状态，就可以得到一系列相似的对象，而这些对象在内存中只有一份。\n结构图 享元模式的结构图如下所示：\n从图中可以看出，享元模式主要包含4个角色：\n Flyweight（抽象享元类）：它声明了具体享元类共有的方法，这些方法可以向外部提供对象的内部状态，也可以用来设置对象的外部状态。 ConcreteFlyweight（具体享元类）：它实现了Flyweight类声明的方法，并为具体的享元类提供了存储内部状态需要的空间。 UnsharedConcreteFlyweight（非共享具体享元类）：并不是所有的抽象享元类的子类都要被共享，不能被共享的子类就是非共享具体享元类。当需要一个相关对象的时候，就直接实例化创建了。 FlyweightFactory（享元工厂类）：它被用来创建和管理享元对象，针对Flyweight抽象享元类进行编程。  实际上，由于具体享元类是需要被共享的，所以可以结合单例模式来设计享元类，为每一个享元类都提供一个唯一的享元对象。享元工厂一般也是结合工厂模式来进行设计的。\n模式实现 来看一下中国围棋，围棋的棋盘上只有黑子和白子，不同的是不同棋子的位置不同，所以可以将黑子和白子作为享元对象，对应的坐标作为外部状态，使用享元模式来设计棋盘。结构图如下：\n实现的代码如下：\n// 坐标类 public class Coordinate { private int x; private int y; public int getX() { return x; } public int getY() { return y; } public Coordinate(){} public Coordinate(int x, int y) { this.x = x; this.y = y; } } // 抽象棋子类 public abstract class GoChessman { public abstract String getColor(); public void display(Coordinate coordinate){ System.out.println(this.getColor() + \u0026#34;: (\u0026#34; + coordinate.getX() + \u0026#34;, \u0026#34; + coordinate.getY() + \u0026#34;).\u0026#34;); } } // 白子 public class WhiteGoChessman extends GoChessman { @Override public String getColor() { return \u0026#34;white\u0026#34;; } } // 黑子 public class BlackGoChessman extends GoChessman { @Override public String getColor() { return \u0026#34;black\u0026#34;; } } // 棋子工厂 public class GoChessmanFactory { private static GoChessmanFactory instance = new GoChessmanFactory(); private static HashMap\u0026lt;String, GoChessman\u0026gt; chessmanMap; private GoChessmanFactory(){ chessmanMap = new HashMap\u0026lt;\u0026gt;(); GoChessman blackGoChessman, whiteGoChessman; blackGoChessman = new BlackGoChessman(); whiteGoChessman = new WhiteGoChessman(); chessmanMap.put(\u0026#34;black\u0026#34;, blackGoChessman); chessmanMap.put(\u0026#34;white\u0026#34;, whiteGoChessman); } public static GoChessmanFactory getInstance() { return instance; } public GoChessman getGoChessman(String color){ return chessmanMap.get(color); } } 测试用的客户端代码：\n// 客户端 public class Client { public static void main(String[] args){ // 获取享元工厂对象  GoChessmanFactory goChessmanFactory = GoChessmanFactory.getInstance(); // 产生棋子  GoChessman black1 = goChessmanFactory.getGoChessman(\u0026#34;black\u0026#34;); GoChessman black2 = goChessmanFactory.getGoChessman(\u0026#34;black\u0026#34;); // 判断两颗黑子是否相同  System.out.println(\u0026#34;black1 == black2: \u0026#34; + (black1 == black2)); // 获取两颗白子  GoChessman white1 = goChessmanFactory.getGoChessman(\u0026#34;white\u0026#34;); GoChessman white2 = goChessmanFactory.getGoChessman(\u0026#34;white\u0026#34;); // 判断两颗黑子是否相同  System.out.println(\u0026#34;white1 == white2: \u0026#34; + (white1 == white2)); // 设置坐标并展示  black1.display(new Coordinate(1, 1)); black2.display(new Coordinate(2, 2)); white1.display(new Coordinate(3, 3)); white2.display(new Coordinate(4, 4)); } } 运行结果：\nblack1 == black2: true white1 == white2: true black: (1, 1). black: (2, 2). white: (3, 3). white: (4, 4). 可以发现：使用同一个关键字从享元池取出来的对象是都是同一个，后期可以设置不同对象的外部状态，使得所有的对象具有差异。\n总结 当系统中存在大量相同或者相似的对象的时候，这会浪费大量的内存，可以使用享元模式处理这个问题。通过享元池，可以实现相同或者相似的细粒度对象的复用，这能节省内存空间，提高系统性能。相同的对象因为内部状态的相同而相同，相似的对象因为外部状态的不同而不同，并且它们都是独立的。但是，使用享元模式会使系统变得复杂，将对象的部分状态外部化可能使得系统的运行时间变长。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%81%E4%BA%8C%E4%BA%AB%E5%85%83%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之十二：享元模式"},{"content":" 外观模式（Facade Pattern）：外部与一个子系统的通信通过一个统一的外观角色进行，为子系统中的一组接口提供一个统一的入口。外观模式又称门面模式，它定义了一个高层接口，这个接口使得子系统的使用更加容易。\n 外观模式隐藏了系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。\n结构图 为了更好的理解外观模式，先来看一下示意图：\n从上图可以看出：如果没有外观角色，每个客户端都可能需要和多个子系统之间进行复杂的交互，系统的耦合度非常大。引入外观角色facade之后，客户端就只需要和外观角色交互了，外观角色代替客户端和多个子系统进行交互，这降低了系统的耦合度。\n外观模式结构图：\n从上图可以看出，外观模式主要包含两个角色：\n Facade（外观角色）：它知道相关的子系统的功能和职责。客户端调用它的方法的时候，它通常会把客户端发来的请求委派到相应的子系统中去，由子系统的对象进行处理。 System（子系统角色）：它们被外观角色或者客户端调用。实际上，子系统并不知道外观角色的存在，外观角色对她们来说就是一个客户端而已。  模式实现 根据结构图可以写出下面的示例代码：\n// 子系统接口 public interface Subsystem { void method(); } // 子系统A public class SystemA implements Subsystem { @Override public void method() { System.out.println(\u0026#34;调用systemA的方法\u0026#34;); } } // 子系统B public class SystemB implements Subsystem { @Override public void method() { System.out.println(\u0026#34;调用systemB的方法\u0026#34;); } } // 子系统C public class SystemC implements Subsystem { @Override public void method() { System.out.println(\u0026#34;调用systemC的方法\u0026#34;); } } // 外观类 public class Facade { private Subsystem systemA; private Subsystem systemB; private Subsystem systemC; public Facade(){ System.out.println(\u0026#34;调用外观类的方法\u0026#34;); systemA = new SystemA(); systemB = new SystemB(); systemC = new SystemC(); } public void method(){ systemA.method(); systemB.method(); systemC.method(); } } 测试用的客户端类：\n// 客户端类 public class Client { public static void main(String[] args){ Facade facade = new Facade(); facade.method(); } } 运行结果：\n调用外观类的方法 调用systemA的方法 调用systemB的方法 调用systemC的方法 可以发现，客户端通过外观角色调用了子系统的功能。\n总结 外观模式的主要目的在于降低系统的复杂度。通过引入外观角色，简化了客户端与子系统之间的交互，为复杂子系统的调用提供了一个统一的接口，使得子系统和客户端之间的耦合度降低了，但这并不影响子客户端直接使用子系统的目的。一个子系统的修改不会影响到其它的子系统，子系统的内部变化也不会影响到外观对象。\n不过，外观模式不能很好的限制客户端直接使用子系统类。如果设计不当，增加新的子系统可能会导致外观类源代码的修改，这将违反开闭原则。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%81%E4%B8%80%E5%A4%96%E8%A7%82%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之十一：外观模式"},{"content":" 装饰模式（Decotator Pattern）：动态的给一个对象增加一些额外的职责，就增加对象功能来说，装饰模式比生成子类实现更加灵活。\n 装饰模式允许向一个现有的对象添加新的功能而又不改变其结构。\n结构图 装饰模式的结构图如下：\n从图中可以看出，装饰模式主要包含4个角色：\n Component（抽象构件类）：它是一个接口，声明了在具体构件类中的业务方法。具体构件类和抽象装饰类都实现了这个接口，这样一来，客户端就可以一致对待装饰前的对象和装饰后的对象。抽象构件类也可以是抽象类。 ConcreteComponent（具体构件类）：它实现了 Component 接口，定义类具体的构件。装饰器类可以给它增加额外的职能。 Decotator（抽象装饰类）：它也实现了 Component 接口，是整个模式的核心。它用来给具体构件类增加职责，但是具体的职责将在子类中实现。同时，他维护了一个对Component的引用，通过这个引用可以调用装饰前的对象的方法，通过子类扩展这个方法就可以达到装饰的目的。 ConcreteDecorator（具体装饰类）：它继承了抽象装饰类，负责给构件添加新的职责。每一个具体装饰类都定义了新的职责，可以给对象扩充职责。  模式实现 // 抽象构件接口 public interface Component { void operation(); } // 具体构件类 public class ConcreteComponent implements Component { @Override public void operation() { System.out.println(\u0026#34;调用原有的功能\u0026#34;); } } // 抽象装饰类 public abstract class Decorator implements Component{ protected Component component; // 维持一个对Component对象的引用  public Decorator(Component component){ this.component = component; } public void operation(){ component.operation(); // 调用原有业务方法  } } // 具体装饰类 public class ConcreteDecorator extends Decorator { public ConcreteDecorator(Component component) { super(component); } public void operation(){ super.operation(); // 调用原有业务方法  addBehavior(); // 调用新增方法  } // 新增方法  private void addBehavior(){ System.out.println(\u0026#34;增加新功能\u0026#34;); } } 测试的客户端代码：\n// 客户端类 public class Client { public static void main(String[] args){ Component component1 = new ConcreteComponent(); Component component2 = new ConcreteDecorator(component1); // 用component2装饰component1  component2.operation(); } } 运行结果：\n调用原有的功能 增加新功能 可以发现：由于具体构件类和装饰类都实现了相同的抽象构件接口，装饰模式能够以对客户端透明的方式 动态 的给对象添加职责。也就是说，客户端不会察觉到对象在装饰前后的不同。\n总结 装饰模式降低了系统的耦合度，可以动态的给对象增加或删除职责，使得需要装饰的具体构件类和具体装饰类可以独立的变化，非常有利于系统的扩展。\n在扩展对象功能的时候，使用组合方式的装饰模式比继承更加灵活，不会导致系统中类的迅速增加，而且可以对一个类进行多次装饰，这能得到功能更加强大的对象。\n但是，使用装饰模式的时候，系统中会产生很多小对象，大量的小对象会占用更多的系统资源。而且，装饰模式比继承更为复杂。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%81%E8%A3%85%E9%A5%B0%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之十：装饰模式"},{"content":" 组合模式（Composite Pattern）：组合多个对象形成树形结构以表示具有“整体-部分”关系的层次结构。组合模式对单个对象（叶子对象）和组合对象（容器对象）的使用具有一致性。组合模式又称“整体-部分”（Part-Whole）模式。\n 使用组合模式，客户端可以像处理简单元素一样处理复杂元素，能够降低客户端和复杂元素内部的耦合度。\n结构图 组合模式的结构图如下：\n从图中可以看出，组合模式主要包含3个角色：\n Component（抽象构件类）：它为叶子构件和容器构件声明了用来访问及管理子构件的接口，它也可以包含所有子类公共接口的声明和实现。它也可以是接口和具体的类。 Leaf（叶子构件类）：它是容器树中的叶子节点，没有子节点。因此，它实现了抽象构件类中定义的接口。可以通过特殊方式处理那些调用访问及管理子构件的行为。 Composite（容器构件类）：它是容器树中的非叶子节点，它可以包含叶子节点和容器节点。因此，它提供了一个集合用来管理子构件。它也实现了抽象构件类中定义的接口，在其业务方法中可以递归的调用子节点的业务方法。  模式实现 根据结构图，可以写出下面的简单实现代码：\n// 抽象构件类 public abstract class Component { public abstract void add(Component c); // 添加元素  public abstract void remove(Component c); // 移除元素  public abstract Component getChild(int i); // 获取子节点  public abstract void operation(); // 业务方法 } / 叶子构件类 public class Leaf extends Component { @Override public void add(Component c) { } @Override public void remove(Component c) { } @Override public Component getChild(int i) { return null; } @Override public void operation() { System.out.println(\u0026#34;调用叶子构件的业务方法\u0026#34;); } } // 容器构件类 public class Composite extends Component { private ArrayList\u0026lt;Component\u0026gt; children = new ArrayList\u0026lt;\u0026gt;(); @Override public void add(Component c) { children.add(c); } @Override public void remove(Component c) { children.remove(c); } @Override public Component getChild(int i) { return children.get(i); } @Override public void operation() { System.out.println(\u0026#34;调用容器构件的业务方法 -\u0026gt; 递归调用子构件的业务方法\u0026#34;); for(Component child: children){ child.operation(); } } } 测试用的客户端类：\n// 客户端类 public class Client { public static void main(String[] args){ Component leaf1, leaf2, composite1, composite2; leaf1 = new Leaf(); leaf2 = new Leaf(); composite1 = new Composite(); composite2 = new Composite(); composite1.add(leaf1); composite1.add(composite2); composite2.add(leaf2); composite1.operation(); } } 运行结果：\n调用容器构件的业务方法 -\u0026gt; 递归调用子构件的业务方法 调用叶子构件的业务方法 调用容器构件的业务方法 -\u0026gt; 递归调用子构件的业务方法 调用叶子构件的业务方法 可以看出: composite1 是一个容器构件，它具有两个子节点——一个叶子节点 leaf1 和一个容器节点 composite2 。 composite2 又具有一个叶子节点。调用 composite1 的业务方法会依次递归调用其子构件的业务方法。\n透明组合模式和安全组合模式 按照抽象构件类的定义形式，可以将组合模式分为透明组合模式和安全组合模式。\n透明组合模式 透明组合模式的结构图如下：\n可以看出：透明组合模式中，抽象构件类声明了所有用于管理成员对象的方法，这样一来，所有的构件类都具有相同的接口，客户端可以针对接口编程。从客户端的角度看，叶子构件和容器构件是相同的，可以同样对待。透明组合模式的缺点就是 不够安全 ，因为叶子构件和容器构件本质上是有区别的。叶子构件不可能具有子构件，也就是说：调用叶子构件中关于子构件的方法可能导致问题。\n安全组合模式 安全组合模式的结构图如下：\n可以看出：安全组合模式中，抽象构件类没有声明任何用于管理成员对象的方法，而是在Composite类中声明并实现这些方法。对于叶子构件的实例来说，就无法调用到这些方法，这是安全的。但是，安全组合是不够透明的，因为叶子构件和容器构件具有不同的方法，因此客户端不能面向抽象构件类编程。\n总结 组合模式主要用在具有 整体-部分 结构的层次结构中，可以一致性对待整体和部分，这一般是一个树形结构。使用组合模式，可以清楚的定义分层次的复杂对象，使得客户端可以忽略层次之间的差异，方便对整个结构进行控制，简化操作。在组合模式中增加新的叶子构件和容器构件非常方便，不用修改已有代码，符合开闭原则。但是，组合模式很难在增加新构件的时候对构件的类型进行限制。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B9%9D%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之九：组合模式"},{"content":" 桥接模式（Bridge Pattern）：将类的功能层次结构（抽象部分）和实现层次结构（实现部分）分离，使二者能够 独立 的变化，并在二者之间搭建桥梁，实现桥接。\n 如果系统中存在两个独立变化的维度，通过桥接模式就可以将这两个维度分离出来，使二者可以独立的扩展。\n结构图 桥接模式的结构图如下：\n从图中可以看出，桥接模式主要包含4个角色：\n Abstraction（抽象类）：抽象类用来定义接口，其中维护了一个Implementor类型的对象； RefinedAbstraction（扩充抽象类）：它扩充了由Abstraction定义的接口，实现了Abstraction中声明的方法，并可以调用Implementor的方法； Implementor（实现类接口）：它声明了基本操作的接口； ConcreteImplementor（具体实现类）：它实现了Implementor中声明的接口，在程序运行时，它将替代Implementor提供给Abstraction具体的业务操作方法；  模式实现 假设我们要开发一个绘图系统，它能够使用红、绿、蓝三种不同的颜色绘制圆形、正方形和三角形。很容易发现这里面有两个独立变化的维度，可以使用桥接模式来进行设计。可以得到下面的结构图：\n于是可以写出下面的示例代码：\n抽象颜色接口：\n// 颜色接口 public interface Color { void paint(); } 具体颜色类：\n// 红色 public class Red implements Color{ @Override public void paint() { System.out.println(\u0026#34;使用红色绘图\u0026#34;); } } // 绿色 public class Green implements Color { @Override public void paint() { System.out.println(\u0026#34;使用绿色绘图\u0026#34;); } } // 蓝色 public class Blue implements Color { @Override public void paint() { System.out.println(\u0026#34;使用蓝色绘图\u0026#34;); } } 抽象形状类：\n// 形状接口 public abstract class Shape { protected Color color; protected Shape(Color color){ this.color = color; } public abstract void draw(); } 具体形状类：\n// 圆形 public class Circle extends Shape { public Circle(Color color) { super(color); } @Override public void draw() { color.paint(); System.out.println(\u0026#34;绘制圆形\u0026#34;); } } // 正方形 public class Square extends Shape { public Square(Color color) { super(color); } @Override public void draw() { color.paint(); System.out.println(\u0026#34;绘制正方形\u0026#34;); } } // 三角形 public class Triangle extends Shape { public Triangle(Color color) { super(color); } @Override public void draw() { color.paint(); System.out.println(\u0026#34;绘制三角形\u0026#34;); } } 测试用的客户端类：\n// 客户端 public class Client { public static void main(String[] args){ Color color = new Red(); Shape shape = new Circle(color); shape.draw(); color = new Green(); shape = new Triangle(color); shape.draw(); color = new Blue(); shape = new Square(color); shape.draw(); } } 运行结果：\n使用红色绘图 绘制圆形 使用绿色绘图 绘制三角形 使用蓝色绘图 绘制正方形 总结 在进行软件设计的时候，如果系统中有两个或者多个变化的维度的时候，都可以尝试使用桥接模式来进行设计。通过在抽象层之间建立关系，可以避免层间静态继承结构（多层继承常常违反单一指职责原则）的出现，还能够松耦合抽象和实现之间的关系。分离出来的各个维度可以独立的变化，有利于单独进行扩展。不过，使用桥接模式会增加系统的理解与设计难度，需要识别出独立变化的维度，使用范围也有一定的局限性。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AB%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之八：桥接模式"},{"content":" 适配器模式（Adapter Pattern）：将一个接口转换成客户希望的另一个接口，使接口不兼容的那些类可以一起工作，又称包装器（Wrapper）。\n 适配器模式的目标就是在不修改原有适配者接口和抽象目标类接口的前提下，将一个类的接口和另一个类的接口匹配起来。根据适配器有适配者的关系的不同，适配器模式又分为 对象适配器模式 和_类适配器模式_ 。\n对象适配器模式  在对象适配器模式中，适配器与适配者之间是 关联 关系。\n 结构图 对象适配器模式的结构图如下： 从图中可以看出，对象适配器模式主要包含3个角色：\n Target（目标抽象接口）：它定义了客户端所需要的接口。它也可以是一个抽象类或者具体类； Adaptee（适配者类）：它是被适配的角色，包含了客户端希望使用的业务方法，但是客户端又不能调用，需要进行适配。 Adapter（适配器类）：它是适配器模式的核心，通过它来对Target和Adaptee进行适配。这里Adapter通过实现Target中的接口并关联一个Adaptee对象使二者产生关联。  模式实现 // 目标接口 public interface Target { // Adaptee 没有这个方法  void request(); } // 适配者类 public class Adaptee { public void specificRequest(){ System.out.println(\u0026#34;我是适配者的方法\u0026#34;); } } // 适配器类 public class Adapter implements Target { private Adaptee adaptee; // 关联一个Adaptee  public void setAdaptee(Adaptee adaptee) { this.adaptee = adaptee; } @Override public void request() { System.out.println(\u0026#34;进行适配\u0026#34;); // 转发调用  adaptee.specificRequest(); } } 测试用的客户端代码：\n// 客户端类 public class Client { public static void main(String[] args){ Target target = new Adapter(); Adaptee adaptee = new Adaptee(); ((Adapter) target).setAdaptee(adaptee); target.request(); } } 运行结果：\n进行适配 我是适配者的方法 适配器类通过关联一个适配者实例，将客户端和适配者衔接起来，然后在**request()** 方法中调用适配者的 specificRequest() 方法完成适配功能。\n类适配器模式  在类适配器模式中，适配器与适配者是 继承 或者 实现 关系。\n 结构图 模式实现 代码绝大部分和对象适配器的代码相同，不再重复，不同在于Adapter类。\n// 适配器类 public class Adapter extends Adaptee implements Target { @Override public void request() { specificRequest(); } } 由于这里使用的是继承机制，在只支持单继承结构的编程语言里会受到一定的限制。\n双向适配器模式  在双向适配器模式中，适配器类同时包含适配者和目标类的引用，适配者可以通过适配器调用目标类的方法，目标类也可以通过适配器调用适配者的方法。\n 结构图 双向适配器模式的结构图如下：\n模式实现 双向适配器的代码较为复杂，主要在于适配器同时维持了对目标类和适配者的引用：\n// 适配器类 public class Adapter implements Target, Adaptee{ // 同时维持对目标类和适配者类的引用  private Target target; private Adaptee adaptee; public void setTarget(Target target) { this.target = target; } public void setAdaptee(Adaptee adaptee) { this.adaptee = adaptee; } @Override public void specificRequest() { target.request(); } @Override public void request() { adaptee.specificRequest(); } } 总结 适配器模式将现有的接口转化为客户类所期望的接口，实现了对现有类的复用。如果没有一些现有的类，那么适配器模式是派不上用场的。\n适配器模式解除了现有类和目标类的耦合，也不用修改现有的结构。同时，适配器模式非常利于扩展，完全符合开闭原则。但是，适配器模式也有一定的局限：比如单继承结构语言的限制，适配者类不能是最终类等。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之七：适配器模式"},{"content":" 建造者模式（Builder Pattern）：将一个复杂的对象的构建与它的表示分离，使得同样的创建过程可以创建不同的表示。建造者模式又称为生成器模式。\n 基本实现方案 建造者模式常用来逐步创建复杂对象，它允许客户端通过指定复杂对象的内容和类型就可以构建它们，用户并不需要知道内部的实现细节。换句话说，复杂对象的创建过程被分解成了多个简单步骤，在创建复杂对象的时候，只需要了解复杂对象的基本属性即可，而不需要关心复杂对象的内部构造过程。用户只需要关注这个复杂对象需要哪些数据，而不用关心内部的实现细节。\n建造者模式的结构图如下：\n从图中可以看出，建造者模式主要包含4个角色：\n Builder（抽象建造者类）：它声明了创建一个产品对象所需要的接口。抽象建造者类也可以是具体类或者抽象类。它主要含有两类方法：  buildPartX(): 用来创建产品的各个组成部件； getResult(): 返回创建好的产品；   ConcreteBuilder（具体建造者类）：它实现了 Builder 接口，将被用来创建一个具体的产品； Product（产品类）：它是被Concrete创建的对象，和特定的 ConcreteBuilder 相关联； Director（指挥者类）：它隔离了客户端和产品的创建过程，负责安排产品的创建过程。它是和客户端交互的接口，将根据客户端的要求创建并返回具体的产品；  根据结构图可以写出下面的示例代码：\n// 产品类 public class Product { private String partA; private String partB; private String partC; //省略getter和setter  @Override public String toString() { return \u0026#34;Product{\u0026#34; + \u0026#34;partA=\u0026#39;\u0026#34; + partA + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, partB=\u0026#39;\u0026#34; + partB + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, partC=\u0026#39;\u0026#34; + partC + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } // 抽象建造者 public interface Builder { void buildPartA(); void buildPartB(); void buildPartC(); Product getResult(); } // 具体建造者类 public class ConcreteBuilder implements Builder { Product product = new Product(); @Override public void buildPartA() { System.out.println(\u0026#34;Build partA\u0026#34;); product.setPartA(\u0026#34;partA\u0026#34;); } @Override public void buildPartB() { System.out.println(\u0026#34;Build partB\u0026#34;); product.setPartB(\u0026#34;partB\u0026#34;); } @Override public void buildPartC() { System.out.println(\u0026#34;Build partC\u0026#34;); product.setPartC(\u0026#34;partC\u0026#34;); } @Override public Product getResult() { System.out.println(\u0026#34;Building finished!\u0026#34;); return product; } } // 指挥者类 public class Director { private Builder builder; public void setBuilder(Builder builder){ this.builder = builder; } public Product construct(){ builder.buildPartA(); builder.buildPartB(); builder.buildPartC(); return builder.getResult(); } } 测试用的客户端代码：\n// 客户端类 public class Client { public static void main(String[] args){ Director director = new Director(); Builder builder = new ConcreteBuilder(); // 注入Builder  director.setBuilder(builder); Product product = director.construct(); System.out.println(product); } } 运行结果：\nBuild partA Build partB Build partC Building finished! Product{partA=\u0026#39;partA\u0026#39;, partB=\u0026#39;partB\u0026#39;, partC=\u0026#39;partC\u0026#39;} 总结 建造者模式的核心在于：如何使用相同的构建过程一步步完成产品组件的创建，最终构建出不同的产品。使用建造者模式的目的一般有两个：第一个目的是将使用方与复杂对象的内部细节分离开来，实现解耦；第二个目的是简化复杂对象的构建过程。\n建造者模式适用于创建复杂的产品。它也能选择性的创建产品的部件以及指定产品部件的创建次序，这能够实现对产品创建过程的精确控制。每一类的建造者都相互独立，可以很方便地替换具体的建造者和增加新的建造者，这符合开闭原则。但是，建造者模式要求产品一般具有较多的共同点，如果产品之间差异很大，就不适合使用建造者模式了。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之六：建造者模式"},{"content":" 原型模式（Prototype Pattern）：使用原型实例指定创建对象，并通过克隆这些原型得到新的对象。\n 原型模式的工作原理就是：将一个原型对象传给那个要发动创建的对象，这个要发动创建的对象通过请求原型对象克隆自己来实现创建过程。\n需要注意的是：通过克隆方法所创建的对象应当都是 全新 的对象，他们在内存中拥有新的地址。通常，对克隆后的对象进行修改不会影响原型对象，每一个克隆对象都是相互独立的。通过修改克隆后的对象，可以得到一组相似的对象。\n基本实现方案 原型模式的结构图如下：\n从结构图中可以看出，原型模式主要包含3个角色：\n Prototype（抽象原型类）：它声明了克隆方法的接口，供具体原型类实现，它也可以是抽象类或这具体类； ConcretePrototype（具体原型类）：它实现了抽象原型类中声明的克隆方法，调用此方法会返回一个自己的克隆对象； Client（客户端类）：针对抽象原型类编程，让一个原型对象克隆自己从而创建一个新的原型对象。  原型模式的一个简单实现：\n原型类：\nimport java.io.*; // 实现序列化接口，便于进行深克隆 public class Book implements Serializable { private String name; private String isbn; private String publishingCompany; public Book(String name, String isbn, String publishingCompany) { this.name = name; this.isbn = isbn; this.publishingCompany = publishingCompany; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getIsbn() { return isbn; } public void setIsbn(String isbn) { this.isbn = isbn; } @Override public String toString() { return \u0026#34;Book{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, isbn=\u0026#39;\u0026#34; + isbn + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, publishingCompany=\u0026#39;\u0026#34; + publishingCompany + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } // 使用序列化技术进行深克隆  public Book deeppClone() throws IOException, ClassNotFoundException{ // 将对象写入流中  ByteArrayOutputStream bao = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bao); oos.writeObject(this); // 将对象从流中取出  ByteArrayInputStream bio = new ByteArrayInputStream(bao.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bio); return (Book) ois.readObject(); } } 测试用的客户端代码：\nimport java.io.IOException; public class Client { public static void main(String[] args) throws IOException, ClassNotFoundException { Book bookPrototype = new Book(\u0026#34;设计模式\u0026#34;, \u0026#34;978-7-303-23647-8\u0026#34;, \u0026#34;机械工业出版社\u0026#34;); Book bookCopy = bookPrototype.deeppClone(); System.out.println(\u0026#34;bookPrototype == bookCopy: \u0026#34; + (bookPrototype == bookCopy)); System.out.println(\u0026#34;bookPrototype.getClass() == bookCopy.getClass(): \u0026#34; + (bookPrototype.getClass() == bookCopy.getClass())); bookCopy.setIsbn(\u0026#34;978-6-111-22222-1\u0026#34;); System.out.println(\u0026#34;------------ bookPrototype -------------\u0026#34;); System.out.println(bookPrototype); System.out.println(\u0026#34;------------ bookCopy -------------\u0026#34;); System.out.println(bookCopy); } } 运行结果：\nbookPrototype == bookCopy: false bookPrototype.getClass() == bookCopy.getClass(): true ------------ bookPrototype ------------- Book{name=\u0026#39;设计模式\u0026#39;, isbn=\u0026#39;978-7-303-23647-8\u0026#39;, publishingCompany=\u0026#39;机械工业出版社\u0026#39;} ------------ bookCopy ------------- Book{name=\u0026#39;设计模式\u0026#39;, isbn=\u0026#39;978-6-111-22222-1\u0026#39;, publishingCompany=\u0026#39;机械工业出版社\u0026#39;} 可以发现：克隆的对象和原来的对象已经不是同一个对象了，但是它们仍然属于同一个类，仍然具有一致的行为和相似的属性。\n浅克隆VS深克隆 浅克隆和深克隆的主要区别在于是否支持 引用类型 的成员变量的复制。\n值类型VS引用类型 在Java中，数据类型分为 值类型 和 引用类型 。\n值类型也就是基本的数据类型，包括：boolean, char, byte, short, int, long, float, double；\n引用类型包括类、数组、接口等复杂类型。\n浅克隆（Shallow Clone) 在浅克隆中，如果原型对象成员变量是值类型，则复制一份给克隆对象；如果是引用类型，则将引用对象的 地址 复制一份给克隆对象。也就是说，原型对象和克隆对象的引用类型成员指向的是 同一个 内存地址， 也就是同一个引用。\n可以通过覆盖object类的 clone() 方法实现浅克隆。\n深克隆 和浅克隆不同的是：深克隆在复制成员变量的时候，不管成员变量是基本类型还是引用类型，都会复制一份给克隆对象。此外，深克隆还会将原型对象的所有引用也复制一份给克隆对象。\n可以通过序列化（Serialization）技术快速实现深克隆。\n总结 原型模式是一种用来快速创建大量相同或者相似对象的方式。特别是当创建新的对象较为复杂的时候，使用原型模式可以简化创建过程。使用了抽象层的原型模式还能很好的支持扩展。当我们需要保存对象在某一时刻的状态的时候，能够很轻松的通过深克隆机制来实现。不过，原型模式需要为每一个类都配备一个克隆方法，由于克隆方法在类的内部，所以当需要对类进行改造的时候，需要修改原有的代码，这违反了开闭原则。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之五：原型模式"},{"content":" 抽象工厂模式（Abstract Factory Pattern）：提供一个创建一系列相关或者相互依赖对象的接口，而无须指定它们具体的类。抽象工厂模式又称Kit模式。\n 产品等级结构和产品族 先回忆一下工厂方法模式：在工厂方法模式里面，每一种具体的产品都有它对应的工厂，工厂方法具有专一性，也就是说一个工厂仅提供一种产品。有时候，我们希望一个工厂可以创建多种产品。如果仍然采取一个工厂对应一种产品的方法，那么系统中类的个数就会快速增加。\n在提出抽象工厂模式之前，先引入产品等级结构和产品族的概念：\n 产品等级结构。产品等级结构也就是产品的继承结构。比如：我们现在有手机这个抽象类，它的子类有华为手机、苹果手机、小米手机……。华为手机下面又有P系列手机、Mate系列手机、畅享系列手机等等。抽象的手机和不同品牌的具体的手机就构成了一个产品等级结构。 产品族。在抽象工厂模式中，产品族指的是由同一个工厂生产的，位于不同产品等级结构的一组产品。就像上面的华为手机系列的具体机型。  产品等级结构和产品族就像是一个平面的X和Y坐标轴，通过一个产品产品等级结构和产品族就能唯一确定这个产品。\n基本实现方案 抽象工厂模式的结构图如下：\n从图中可以看出，抽象工厂模式主要包含4个角色：\n AbstractFactory（抽象工厂类）：它声明了一组用来创建一族产品的方法，每一个方法对应一种产品； ConcreteFactory（具体工厂类）：它实现了抽象工厂类中声明的方法。一个具体工厂中的不同方法负责创建位于同一产品族的不同产品等级结构的产品，所有的这些产品构成了同一个产品族。 AbstracProduct（抽象产品类）：它声明了产品所具有的业务方法； ConcreteProduct（具体产品类）：它实现了抽象产品类中声明的方法  根据结构图可以写出如下示例代码：\n抽象产品：\n// 产品族A public interface AbstractProductA { public void productAMethod(); } // 产品族B public interface AbstractProductB { public void productBMethod(); } 具体产品：\n// 具体产品A1 public class ConcreteProductA1 implements AbstractProductA { @Override public void productAMethod() { System.out.println(\u0026#34;这是产品A1, 属于产品族1\u0026#34;); } } // 具体产品A2 public class ConcreteProductA2 implements AbstractProductA { @Override public void productAMethod() { System.out.println(\u0026#34;这是产品A2, 属于产品族2\u0026#34;); } } // 具体产品B1 public class ConcreteProductB1 implements AbstractProductB { @Override public void productBMethod() { System.out.println(\u0026#34;这是产品B1, 属于产品族1\u0026#34;); } } // 具体产品B2 public class ConcreteProductB2 implements AbstractProductB { @Override public void productBMethod() { System.out.println(\u0026#34;这是产品B2, 属于产品族2\u0026#34;); } } 抽象工厂：\n// 抽象工厂 public interface AbstractFactory { // 创建产品族A  public AbstractProductA createProductA(); // 创建产品族B  public AbstractProductB createProductB(); } 具体工厂：\n// 具体工厂1 public class ConcreteFactory1 implements AbstractFactory { @Override public AbstractProductA createProductA() { System.out.println(\u0026#34;创建产品A1, 产品等级A\u0026#34;); return new ConcreteProductA1(); } @Override public AbstractProductB createProductB() { System.out.println(\u0026#34;创建产品B1, 产品等级B\u0026#34;); return new ConcreteProductB1(); } } // 具体工厂2 public class ConcreteFactory2 implements AbstractFactory { @Override public AbstractProductA createProductA() { System.out.println(\u0026#34;创建产品A2, 产品等级A\u0026#34;); return new ConcreteProductA2(); } @Override public AbstractProductB createProductB() { System.out.println(\u0026#34;创建产品B2, 产品等级B\u0026#34;); return new ConcreteProductB2(); } } 测试用的客户端代码：\n// 客户端 public class Client { public static void main(String[] args){ AbstractFactory factory; AbstractProductA productA; AbstractProductB productB; factory = new ConcreteFactory1(); productA = factory.createProductA(); productA.productAMethod(); productB = factory.createProductB(); productB.productBMethod(); factory = new ConcreteFactory2(); productA = factory.createProductA(); productA.productAMethod(); productB = factory.createProductB(); productB.productBMethod(); } } 运行结果：\n创建产品A1, 产品等级A 这是产品A1, 属于产品族1 创建产品B1, 产品等级B 这是产品B1, 属于产品族1 创建产品A2, 产品等级A 这是产品A2, 属于产品族2 创建产品B2, 产品等级B 这是产品B2, 属于产品族2 总结 和工厂方法模式相比，抽象工厂模式最大的不同在于：工厂方法模式针对的是一个产品等级结构，而抽象工厂模式针对的是多个产品等级结构（一个产品族）。在这里，属于同一个产品族的多个对象被设计成一起工作。当向系统中增加产品族的时候，不用修改已有代码，这符合开闭原则。但是当向系统中增加新的产品等级结构的时候会很麻烦，需要对原来的代码进行修改，这个修改往往是很大的，这又违反了开闭原则。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记之四：抽象工厂模式"},{"content":" 工厂方法模式（Factory Method Pattern）：定义一个用于创建对象的接口，让子类决定将哪一个类实例化，这让一个类的实例化延迟到了其子类。工厂方法模式简称工厂模式（Factory Method），也称虚拟构造器模式（Virtual Constructor Pattern），又被称为多态工厂模式（Polymorphic Factory Method）。\n 基本实现方案  将需要创建的各种不同对象的相关代码封装到不同的具体产品类中； 将具体产品类的公共代码进行抽象和提取后封装到一个抽象产品类(也可以是接口或者具体的类)中，它是所有具体产品类的父类； 创建一个抽象工厂类并提供一个创建产品的工厂方法，用于返回一个具体的产品； 创建和具体产品类对应的具体工厂类，使之成为抽象工厂类的子类，并实现抽象工厂类中定义的方法。  工厂方法模式的结构图如下：\n可以看出，工厂方法模式主要包含四个角色：\n Product（抽象产品类）：它定义了产品的接口，供所有具体的产品类实现。 ConcreteProduct（具体产品类）：它实现了产品的接口，是具体工厂类创建的对象，和具体工厂一一对应； Fatory（抽象工厂类）：它是整个模式的核心，声明了工厂方法factoryMethod()。所有的具体工厂类都必须实现它声明的方法； ConcreteFactory（具体工厂类）：它实现了抽象工厂类的接口，与特定的产品相关联，并可返回具体产品的实例；  根据结构图可以写出如下的示例代码：\n产品类：\n// 抽象产品接口 public interface Product { public void productMethod(); } // 具体产品A public class ConcreteProductA implements Product{ public ConcreteProductA(){ System.out.println(\u0026#34;创建产品A\u0026#34;); } @Override public void productMethod() { System.out.println(\u0026#34;这是产品A\u0026#34;); } } // 具体产品B public class ConcreteProductB implements Product { public ConcreteProductB(){ System.out.println(\u0026#34;创建产品B\u0026#34;); } @Override public void productMethod() { System.out.println(\u0026#34;这是产品B\u0026#34;); } } 工厂类：\n// 抽象工厂接口 public interface Factory { public Product factoryMethod(); } // 具体产品A对应的具体工厂 public class ConcreteProductAFactory implements Factory { @Override public Product factoryMethod() { return new ConcreteProductA(); } } // 具体产品B对应的具体工厂 public class ConcreteProductBFactory implements Factory { @Override public Product factoryMethod() { return new ConcreteProductB(); } } 客户端测试代码：\n// 客户端 public class Client { public static void main(String[] args){ Factory factory = null; Product product = null; // 创建产品A  factory = new ConcreteProductAFactory(); product = factory.factoryMethod(); product.productMethod(); // 创建产品B  factory = new ConcreteProductBFactory(); product = factory.factoryMethod(); product.productMethod(); } } 运行结果：\n创建产品A 这是产品A 创建产品B 这是产品B 与简单工厂模式相比，工厂方法模式最大的区别就是引入了抽象工厂这个角色。它只是简单的声明了工厂方法，具体的产品对象由其子类——具体工厂类创建，这使得客户端可以针对抽象工厂编程，具体工厂类可以延迟到运行时刻再确定。\n总结 工厂方法模式是对简单工厂模式的改进，克服了简单工厂模式的不足，关键在于基于工厂角色和产品角色的 多态 设计。抽象工厂通过指定其子类来确定具体使用工厂方法创建哪个对象，这就向客户端隐藏了具体产品的实现细节，客户端不必知道具体产品类的类名，只需知道相应的工厂即可。如果采用配置文件的方式指定具体的产品类，在更换产品的时候只需要修改配置文件，不必修改已有代码。当向系统中加入新的产品时，也无须修改已有代码，只需添加具体产品类和对应的工厂类并修改配置文件，所以工厂方法模式最终是符合 开闭原则 的，这非常有利于系统的扩展。然而，但加入新的产品的时候，系统中的类的个数也将成对增加，在一定程度上加大了系统的复杂度。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记三：工厂方法模式"},{"content":" 简单工厂模式（Simple Factory Pattern）：定义一个工厂类，工厂类根据参数返回不同的实例，被创建的实例通常具有共同的父类。由于创建实例的方法是静态方法，因此简单工厂模式又被称为静态工厂方法模式。\n 简单工厂的核心在于：根据传入的参数获取相应的对象，而无须知道创建的细节。\n基本实现方案  将需要创建的各种不同对象的相关代码封装到不同的具体产品类中； 将具体产品类的公共代码进行抽象和提取后封装到一个抽象产品类(也可以是接口或者具体的类)中，它是所有具体产品类的父类； 创建一个工厂类并提供一个创建产品的工厂方法，它能根据传入参数的不同创建不同的具体产品对象；  简单工厂模式的结构图如下：\n可以看出，简单工厂模式的结构图中包含3个角色：\n Factory（工厂类）：它是整个模式的核心，负责创建所有的产品。外界可以直接调用静态工厂方法factoryMethod()创建所需要的具体产品对象； Product（抽象产品类）：它是所有具体产品类的父类，封装了所有具体产品类的公有方法。 ConcreteProduct（具体产品类）：它是简单工厂模式创建的目标，所有被创建的对象都是某个具体产品类的一个实例。  根据结构图写出的代码如下：\n// 抽象产品 public interface Product { // 公有方法  public void sameMethod(); } // 具体产品A public class ConcreteProductA implements Product { public ConcreteProductA(){ System.out.println(\u0026#34;创建产品A\u0026#34;); } @Override public void sameMethod() { System.out.println(\u0026#34;这是产品A\u0026#34;); } } // 具体产品B public class ConcreteProductB implements Product{ public ConcreteProductB(){ System.out.println(\u0026#34;创建产品B\u0026#34;); } @Override public void sameMethod() { System.out.println(\u0026#34;这是产品B\u0026#34;); } } 工厂类根据传入参数创建对应的具体产品：\n// 工厂类 public class Factory { public static Product getProduct(String args){ Product product = null; if(args.equalsIgnoreCase(\u0026#34;A\u0026#34;)){ product = new ConcreteProductA(); // 其他操作...  } else if(args.equalsIgnoreCase(\u0026#34;B\u0026#34;)){ product = new ConcreteProductB(); // 其他操作...  } return product; } } 下面是客户端代码：\n// 客户端 public class Client { public static void main(String[] args){ Product product; product = Factory.getProduct(\u0026#34;A\u0026#34;); product.sameMethod(); product = Factory.getProduct(\u0026#34;B\u0026#34;); product.sameMethod(); } } 最终运行的结果如下：\n创建产品A 这是产品A 创建产品B 这是产品B 方案的改进 观察客户端的代码可以发现，每更换一个产品都必须更改客户端的代码，这违反了开闭原则。有没有一种方式可以实现在不修改客户端代码的情况下更换产品呢？当前有。在Java中，可以采取配置文件的方式，将factoryMethod()的参数存储在XML或者properties格式的配置文件中。通过读取配置文件获取参数，然后客户端根据从配置文件得到的参数去调用工厂类的静态方法，这就达到了目的。\n总结 使用简单工厂模式，客户端只知道传入工厂类的参数，不必关心对象是如何创建的，它也不知道所创建的产品是哪一个类。工厂类知道每一个具体产品的创建细节，它根据传入参数决定创建哪一个对象供客户端使用，实现了对象的创建和使用的分离。但是，工厂类集中了所有产品的创建逻辑，职责过重。当增加新的产品的时候，不得不修改工厂类的代码，这违反了开闭原则，同时也不利于系统的扩展和维护。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记二：简单工厂模式"},{"content":" 单例模式(Singleton Pattern)：确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例，这个类就是单例类。\n 从单例模式的定义来看，它有3个要点：\n 它只有唯一一个实例； 它必须自行创建这个实例； 它必须向整个系统提供这个实例；  基本实现方案  将构造函数设为私有，确保外界无法通过 new 创建该对象； 提供公有的静态方法，返回单例类的唯一实例，供外界访问； 创建唯一的实例，并通过公有的静态方法返回；  单例模式的结构图如下：\n根据这个图，可以很容易写出对应的代码：\n// 单例类 public class Singleton { // 私有静态成员变量  private static Singleton instance = null; // 私有构造函数  private Singleton(){} // 公有静态成员方法，返回单例  public static Singleton getInstance(){ if(instance == null) instance = new Singleton(); return instance; } } 饿汉式单例类(Eager Singleton) 饿汉式单例类的结构图如下：\n从图中可以看出，当类 加载 的时候，静态变量instance就会被初始化，此时会调用私有的构造函数创建类的唯一实例。代码如下：\n// 饿汉式单例类 public class EagerSingleton { private static final EagerSingleton instance = new EagerSingleton(); private EagerSingleton(){} public static EagerSingleton getInstance(){ return instance; } } 懒汉式单例类(Lazy Singleton) 懒汉式单例类的结构图如下：\n从图中可以看出，和饿汉式单例不同的是：懒汉式单例在类加载的时候不实例化，而是第一次调用getInstance()的时候才被实例化。这就是 延迟加载（Lazy Load）技术 ，也就是在需要的时候才加载实例。在Java中，为了避免多个县城同时调用getInstance()方法，可以使用关键字 synchronized 进行线程锁定 。代码如下（这个方法有缺陷）：\n// 有缺陷的懒汉类 public class LazySingleton1 { private static LazySingleton1 instance = null; private LazySingleton1(){} synchronized public static LazySingleton1 getInstance(){ if(instance == null) instance = new LazySingleton1(); return instance; } } 上面的代码中， synchronized 锁定了整个getInstance()方法，会影响系统的性能，一个改进的方法是：只锁定创建实例的那一行代码。修改后的getInstance()方法如下：\npublic static LazySingleton1 getInstance(){ if(instance == null){ synchronized (LazySingleton1.class){ instance = new LazySingleton1(); } } return instance; } 上面的代码看似解决了问题，然而并没有。考虑某一瞬间，如果多个线程同时调用getInstance()方法，如果此时instance == null,这些线程就都能通过if语句的判断。由于使用了synchronized，后面的线程会处于排队等待的状态。当前面的线程执行完由synchronized锁定的代码之后，实例已经被创建，而后面的线程此时并不知道实例已经被创建的事实，天真的创建了新的实例。如此一来，系统中就会出现多个单例对象，这就违背了单例模式的设计思想。一个可行的解决方案是：采用 双重检查锁定（Double-Check Locking） 。也就是在synchronized锁定的代码中再进行一次instance == null 的判断，这样后面的线程就不会通过新的判断条件，也就不会新建线程了。使用双重锁定需要使用Java中的volatile关键字，被它修饰的成员变量可以确保多个线程都能正确处理。修改后的代码如下：\n// 采用双重检查锁定的懒汉类 public class LazySingleton2 { private volatile static LazySingleton2 instance = null; private LazySingleton2(){} public static LazySingleton2 getInstance(){ if(instance == null){ // 第一重判断  synchronized (LazySingleton2.class){ if(instance == null){ // 第二重判断  instance = new LazySingleton2(); } } } return instance; } } 由于使用volatile关键字会屏蔽JVM所做的一些代码优化，因此使用DCL来实现单例模式也不够完美。一种更好的实现方法是：采用 IoDH技术 。\n采用IoDH技术实现单例模式 Initialization on Demand Holder(IoDH)技术 克服饿汉类单例和懒汉类单例的缺点，既能实现延迟加载，又能保证线程安全。在实现的时候，需要在单例类添加一个 静态内部类 ，然后在这个静态内部类中创建单例对象，再将该单例对象返回给外界使用。代码如下：\n// 使用IoDH技术实现 public class BetterSingleton { private BetterSingleton(){} private static class HolderClass{ private final static BetterSingleton instance = new BetterSingleton(); } public static BetterSingleton getInstance(){ return HolderClass.instance; } } 总结 一篇读书笔记下来，对单例模式的理解更是加深了不少，尤其是双重检查锁定机制。总的来说：单例模式适用于系统只需要一个实例对象或者只能通过一个公共访问点访问单个实例的情况。采用单例模式，能够实现对唯一实例的访问，由于只有一个实例，减少了对系统资源的占用，不失为一种节约系统资源的好方式。但是，单例模式缺乏抽象层，不利于扩展和代码的复用。还有就是，单例类通常具有很多职责，这违背了SRP。\n","href":"/notebook/reading_notes/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","title":"《设计模式的艺术》读书笔记一：单例模式"},{"content":"","href":"/tags/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","title":"设计原则"},{"content":"","href":"/categories/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1/","title":"软件设计"},{"content":"在学习软件开发的过程中，我们或多或少都听过一些经典的设计原则，比如单一原则（DRY）、简单原则（KISS）、面向对象原则（SOLID）……这些设计原则都是前辈们在经验中总结出来的。在开发大型复杂软件的过程中，开发者面临软件的可维护性、可扩展性、灵活性、可重用性等诸多方面的挑战，灵活地使用设计原则能够帮助我们更好地面对这些挑战，设计出更加优秀的软件。\n设计原则有很多，有些原则是之间是相互冲突的，而有些原则之间又是相互重复的。大家可能都有这样一个感觉——道理都懂，但是在实际运用时又会出现选择困难，不知从何下手。其实，设计原则是用来指导编程的，每个原则都有自己适用的范围，错误地使用各大原则很容易导致问题。下面是对常用设计原则的一个概括，这些原则不仅需要多看多理解，更要多实践。\nDRY 原则 Don\u0026rsquo;t Repeat Yourself。《程序员修炼之道》一书中是这么解释 DRY 的：系统的每一个功能都应该有唯一的实现，如果多次遇到同样的问题，就应该抽象出一个共同的解决方法，而不要重复开发同样的功能代码。这个原则很简单，大家在平常的编程中也经常使用。我们常听说的“不要重复造轮子”就是对 DRY 的一种认知。\nKISS 原则 Keep It Simple and Stupid。保持代码的简单，快速迭代拥抱变化。编写可读性高的代码，能够减少他人阅读代码的时间投入。\nYAGNI 原则 You Ain’t Gonna Need It。有时候，很多开发者都会写一些多余的代码，想着万一以后可能会用上。但实际上，这些多的代码可能永远都不会用上，反而会对原来的代码造成污染，增加别人阅读理解代码的难度。YAGNI 希望我们不要写将来可能需要，但现在却用不上的代码。YAGNI 原则与 KISS 原则联系紧密，能够帮助更好地实现 KISS　原则。\n迪米特法则(Law of Demeter, LoD) 一个软件实体应当尽可能少的与其它实体发生作用。如果一个系统符合迪米特法则，那么当其中的某一个模块发生改变时，就会尽量少的影响其它模块。在设计系统的时候，尽量减少对象之间的交互，如果两个对象之间不必彼此直接通信，那么这两个对象之间就不应当发生任何直接的作用。如果一个对象需要调用另一个对象的方法，可以通过一个第三者来完成这个调用。这就降低了对象之间的耦合度。\nSOLID 原则 在设计中使用这些原则，有助于提高设计模型的灵活性和可维护性，提高类的內聚度，降低类之间的耦合度。\n单一职责原则(Single Responsibility Principle, SRP) 职责 可以理解为 引起类变化的原因。 就一个类而言，应该只有一个引起它变化的原因 。如果一个类具有多个职责，那么就有多个引起它变化的原因。过多的职责耦合在一起，当其中一个职责变化时，可能影响到其它职责的正常运作。因此，在设计类的时候，要将这些职责分离，将不同的职责封装在不同的类中。确保引起该类变化的原因只有一个，从而提高类的內聚度。\n开闭原则(Open-Closed Principle, OCP) 一个软件实体应当对扩展开放，对修改关闭 。也就是说： 软件实体应该尽可能在不修改原有代码的情况下进行扩展 。为了满足开闭原则，需要对系统进行 抽象化 设计，抽象化是开闭原则的关键。可以先定义出 抽象层 ，然后通过 具体类 来进行扩展。当需要修改系统的行为时，不需要改动抽象层，只需要添加新的具体类就能实现新的业务功能，这就在不修改原有代码的基础上完成了目标。\n里氏替换原则(Liskov Substitution Principle, LSP) 子类应当可以替换父类并出现在父类能够出现的任何地方 。也就是说：在软件中将一个父类对象替换成它的子类对象，程序不会出现任何的问题，反过来则不然。里氏替换原则是实现开闭原则的重要方式之一。由于使用父类的地方都可以使用子类，因此在程序中应该尽量使用父类来对对象进行定义，而在运行的时候再确定子类类型，用子类替换父类对象。因此，可以将父类声明设计为抽象类或者接口，让子类继承父类或者实现父类接口并实现父类声明的方法。在运行的时候，子类实例替换父类实例，可以很方便的扩展系统的功能，增加新的功能可以通过增加新的子类来实现。\n依赖倒置原则(Dependence Inversion Principle, DIP) 抽象不应该依赖于细节，细节应当依赖于抽象 。高层模块不应该依赖于低层模块，两者都应当依赖于 抽象 。也就是说：要针对接口编程，而不是针对实现编程。在程序设计中，尽量使用高层的抽象类来完成功能，而不要使用具体的类来做这些。为此，一个具体类应当只实现接口或者抽象类中声明过的方法，而不要有多余的方法，否则高层的抽象类无法调用到在子类新增加的方法。\n引入抽象层之后，系统的灵活性变高。在程序中尽量使用抽象类进行编程，而将具体类写在配置文件中。如此一来，当系统需要扩展时，只需要对抽象层进行扩展并修改配置文件，无需改动原来的代码。\n依赖注入的三种方式： 构造注入 、 设值注入 、 接口注入 。\n接口隔离原则(Interface Segregation Principle, ISP) 使用多个专门的接口，而不使用一个总的接口 。也就是说：客户端不应该依赖它不需要的接口。当一个接口太大的时候，需要将其划分为一些更小的接口。如果把接口比喻成角色，那么一个接口应当只扮演一个或者一类角色。\n组合/聚合复用原则(Composite Reuse Principle, CRP) 应当尽量使用对象组合，而不是继承来达到复用的目的。继承复用会破坏封装性，因为继承会将父类的实现细节暴露给子类。当父类改变的时候，子类也必须跟着改变。组合/聚合关系可以将已有的对象纳入新的对象中，新对象可以调用已有对象的功能，而已有对象的内部对新对象是不可见的。相对于继承来说，这种方式的耦合度较低，已有对象的改变不会给新对象带来太大的影响。一般来说，如果两个类之间是 Is-A 关系，应当使用继承；如果两个类之间是 Has-A 关系，应当使用组合或者聚合。\n参考资料  刘伟. 设计模式的艺术：软件开发人员内功修炼之道. 清华大学出版社, 2013.  ","href":"/posts/design_patterns/software_design_principles/","title":"软件设计原则"},{"content":"","href":"/tags/leetcode/","title":"Leetcode"},{"content":"","href":"/categories/leetcode/","title":"Leetcode"},{"content":"这是Leetcoce上的第279个问题，解题的方法很有启发意义，以此备忘。\n题目描述 给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, \u0026hellip;）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。\n示例 1:\n输入: n = 12 输出: 3 解释: 12 = 4 + 4 + 4. 示例 2:\n输入: n = 13 输出: 2 解释: 13 = 4 + 9. 解决方案 一个错误的方法 最开始的想法是依次对每个数进行开方操作，这样能够找到比它小的最接近它的完全平方数。代码如下：\nprivate int numSquares1(int n){ int step = 0; while(n \u0026gt; 0){ n -= Math.pow((int)(Math.sqrt(n)), 2); step ++; } return step; } 这段代码使用的是贪心算法的思想，每次找比 n 小的最大的完全平方数。错误的原因是：这个贪心策略在这里 并不适用 。题目中就有一个反例：\n12 = 4 + 4 + 4 一个正确的方式 贪心策略不适用，又不想使用暴力解法。那么有没有好一点的解题方法呢？答案是有的：可以把原问题转化为一个 图论 中的问题。转化方法如下：\n 对于从 n 到 0 的每一个数字，让其成为图中的一个顶点； 如果图中的两个数字之间相差一个完全平方数，则连接它们；  这样一来，原来的问题就转化为求：n 到 0 之间的 最短路径 的问题。使用广度优先遍历的策略就可以找出答案。\njava代码如下：\n/** * 最开始是想用贪心算法来求解，但是后来发现贪心算法在这里并不适用。比如:12 = 4 + 4 + 4; * 这里可以将问题转化为一个求图中的最短路径的问题： * 从n到0,每个数字表示一个节点。如果两个数字之间相差一个完全平方数，则连接它们。 * 最终会得到一个无权图，此时原问题就转化为求这个无权图中n到0的最短路径。 * 使用BFS的思想对图进行层序遍历，从n所在层到0所在层 * @param n 给定的正整数n * @return 和为n需要的最少的完全平方数 */ private int numSquares(int n){ if(n \u0026lt;= 0){ return 0; } int step = 0; // 存放最终结果  Queue\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.offer(n); // 初始节点n入队  int size = queue.size(); // 保存当前层的节点数  while(!queue.isEmpty()){ if(size == 0){ size = queue.size(); step ++; } int number = queue.poll(); // 队首元素出队  size --; if(number == 0){ return step; // 到达终点0，结束程序  } for(int i = 1;number \u0026gt;= i * i;i ++){ // number内还存在完全平方数  queue.offer(number - i * i); } } } 不幸的是：这段代码在Leetcode上 超时 了。\n于是我改用了 Pair\u0026lt;\u0026gt; 对的形式，减少了对上面代码中size取值的判断次数，期望程序能快一点。代码如下：\nprivate int numSquares2(int n){ if(n \u0026lt;= 0){ return 0; } int step = 0; // 存放最终结果  Queue\u0026lt;Pair\u0026lt;Integer, Integer\u0026gt;\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.offer(new Pair\u0026lt;\u0026gt;(n, 0)); // 初始节点n入队  while(!queue.isEmpty()){ Pair\u0026lt;Integer, Integer\u0026gt; pair = queue.poll(); // 队首元素出队  int number = pair.getKey(); step = pair.getValue(); if(number == 0){ return step; // 到达终点0，结束程序  } for(int i = 1;number - i * i \u0026gt;= 0;i ++){ // number内还存在完全平方数  queue.offer(new Pair\u0026lt;\u0026gt;(number - i * i, step + 1)); } } return step; } 然而：还是 超时 了。\n于是我开始仔细看这一段代码，发现了引起性能问题的最大原因：对于较大的数字，每次执行for循环的时候，都会往队列中推入很多重复的路径。比如要到达数字1所在的节点，可能从2、5、10、17、26\u0026hellip;出发 。只要减少了重复的路径，程序的性能应该就会有不少的提升。为了解决重复复访问的问题，我新开了一个数组，用来判断某个节点是否已被访问，现在只将没有被访问过的节点推入队列。\njava代码如下：\nprivate int numSquares3(int n){ if(n \u0026lt;= 0){ return 0; } int step = 0; // 存放最终结果  Queue\u0026lt;Pair\u0026lt;Integer, Integer\u0026gt;\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.offer(new Pair\u0026lt;\u0026gt;(n, 0)); // 初始节点n入队  boolean[] visited = new boolean[n + 1]; // 用来保存已经访问过的节点  visited[n] = true; while(!queue.isEmpty()){ Pair\u0026lt;Integer, Integer\u0026gt; pair = queue.poll(); // 队首元素出队  int number = pair.getKey(); step = pair.getValue(); if(number == 0){ return step; // 到达终点0，结束程序  } for(int i = 1;number - i * i \u0026gt;= 0;i ++){ // number内还存在完全平方数  if(!visited[number - i * i]) { queue.offer(new Pair\u0026lt;\u0026gt;(number - i * i, step + 1)); visited[number - i * i] = true; // 更新visited[]  } } } return step; } Accepted!耶~\n终于能够看到用时极短的答案是怎么做的了。原来它们使用了 数论 的知识。我赶紧去补了一下 四平方定理 。\n四平方和定理 ：任何一个正整数都可以表示成不超过四个整数的平方之和。\n推论 ： 当n是形如:\\(4^a(8b + 7)(a \u0026gt;= 0;b \u0026gt;= 0)\\)n不能表示成3个整数的平方和\n借助这两条定理，我自己实现了一下。 java代码如下：\n// 使用四平方和定理及其推论  private int numSquares4(int n){ while(n % 4 == 0){ n /= 4; } if(n % 8 == 7){ return 4; } // 判断一个数是由一个还是两个平方数组成  int r = (int)Math.sqrt(n); if(r * r == n){ return 1; } for(int t = 1;t * t \u0026lt;= n;t ++){ int k = (int)Math.sqrt(n - t * t); if(k * k + t * t == n){ return 2; } } return 3; } ","href":"/posts/leetcode/perfect-squares/","title":"完全平方数"},{"content":"这是Leetcode上的第215题：数组中的第k个最大元素。\n问题描述 在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。\n示例 1:\n输入: [3,2,1,5,6,4] 和 k = 2 输出: 5 示例 2:\n输入: [3,2,3,1,2,4,5,5,6] 和 k = 4 输出: 4 解决方案 提供两种解法，一种是使用快速排序的解法，一种是使用划分思想但不排序的解法。\n使用排序 要找出数组中第K个最大的元素，最直观的方法就是先将原数组按照非递增顺序排序，然后返回第K个元素。我这里使用的是快速排序。\n快速排序需要先对数组进行划分，划分的代码如下：\n/** * 总是使用最右端的元素A[r]将数组A[p..r]划分为A[p..q - 1]、A[q]、A[q + 1..r] * A[p..q - 1] \u0026gt;= A[q] * A[q + 1..r] \u0026lt; A[q] * @param A 待划分数组 * @param p 下界 * @param r 上界 * @return q */ private int partition(int[] A, int p, int r){ int x = A[r]; // 保存最右端元素  int i = p - 1; // i最终指向A[p..q - 1]中的最后一个元素  for(int j = p;j \u0026lt;= r - 1;j++){ if(A[j] \u0026gt;= x){ i ++; exchange(A, i, j); // 将不小于A[q]的元素A[j]放到A[p..q - 1]中，同时一个小于A[q]的元素被放到A[q + 1..r]中  } } exchange(A, i + 1, r); // 将最右端的元素放到正确位置  return i + 1; } 接下来是寻找第K个最大的元素了：\nprivate void quickSort(int[] nums, int p, int r){ if(p \u0026lt; r){ int q = partition(nums, p, r); quickSort(nums, p, q - 1); quickSort(nums, q + 1, r); } } public int findKthLargest(int[] nums, int k){ int p = 0, r = nums.length - 1; quickSort(nums, p, r); return nums[k - 1]; } 使用划分 利用快速排序中划分的思想，假定始终选取数组A[0..n-1]中的最后一个元素（划分后它在数组中的位置为q）作为参照点，划分后的结果为：A[1..n-1]在划分后由三部分组成，从左到右依次为：\n A[0..q-1]: 均不小于A[q]; A[q] A[q+1..n-1]:均小于A[q];  由此可确定第K大的元素的位置：\n k = q + 1: 返回A[q]; k \u0026lt; q + 1: 在A[0..q-1]中继续寻找第K大的元素 k \u0026gt; q + 1： 在A[q+1..n-1]中继续寻找第(k - q + 1)大的元素  递归版本的代码如下：\nprivate int findKthLargestHelper(int[] nums, int k, int p, int r){ int q = partition(nums, p, r); if(k == q + 1){ return nums[q]; } else if(k \u0026lt; q + 1){ return findKthLargestHelper(nums, k, p, q - 1); } else{ return findKthLargestHelper(nums, k - q + 1, q + 1, r); } } 迭代版本做了一点改动，代码如下：\nprivate int findKthLargestHelperIteratively(int[] nums, int k, int p, int r){ int q = partition(nums, p, r); while(k != q + 1){ if(k \u0026lt; q + 1){ q = partition(nums, p, q - 1); } else{ q = partition(nums, q + 1, r); } } return nums[q]; } ","href":"/posts/leetcode/%E5%AF%BB%E6%89%BE%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E7%AC%ACk%E4%B8%AA%E6%9C%80%E5%A4%A7%E7%9A%84%E5%85%83%E7%B4%A0/","title":"寻找数组中的第K个最大的元素"},{"content":"问题描述 荷兰国旗问题(Dutch national flag problem)是Dijkstra提出的一个经典的编程练习。原问题大概是这样说的：\n Dijkstra used the Dutch National Flag Problem* as a structured programming exercise in program derivation and program proof. Given `N' objects coloured red, white or blue, sort them so that objects of the same colour are adjacent, with the colours in the order red, white and blue.\n Leetcode上的第75题——分类颜色考查的也正是这个问题。\n解决方案 使用任意一种排序算法 这其实是一个对数组元素进行排序的问题，因此使用任意一种排序算法均可以达到目的。\n使用计数排序 一个较为方便的排序算法是使用计数排序，统计每种颜色出现的次数，然后按照红、白、蓝(使用0,1,2代表红、白、蓝)的出现的次数排序，重写当前数组，这正是计数排序的思想。这个方法要扫描两遍数组：第一遍统计各元素出现的次数；第二遍重写当前数组。还需要开辟额外的空间来支持计数这个操作。\n 时间复杂度为：O(n); 空间复杂度为：O(k), k为元素的取值范围;  Java实现 private void countingSort(int[] nums){ int[] c = new int[3]; int n = nums.length; for(int i = 0;i \u0026lt; n;i++){ c[nums[i]] ++; } int k = 0; for(int i = 0;i \u0026lt; 3;i++){ for(int j = 0;j \u0026lt; c[i];j++){ nums[k ++] = i; } } } 使用划分 另一种解法就是利用快速排序中三路划分的思想。这样做的好处是只需要扫描一趟数组。划分过程中原来的数组由四部分组成，从左到右依次为：全0，全1，未处理，全2。初始示意图如下： 运行过程中示意图如下： 这样无需开辟新的数组。\n 时间复杂度：O(n); 空间复杂度：O(1);  Java实现 public void tripartition(int[] nums){ int n = nums.length; int zero = -1; // 从左到右指向最右的那个0  int two = n; // 从右到左指向最左的那个2  int i = 0; // 用来遍历数组  while(i \u0026lt; two){ switch (nums[i]){ case 0: exchange(nums, ++zero, i ++);break; // nums[i] = 0,将它与最右侧的那个1交换  case 1: i ++;break; // nums[i] = 1,指针前进一步  case 2: exchange(nums, -- two, i);break; // nums[i] = 2,将它与最左侧的那个2的前一个元素交换  default: break; } } } ","href":"/posts/leetcode/%E8%8D%B7%E5%85%B0%E5%9B%BD%E6%97%97%E9%97%AE%E9%A2%98/","title":"荷兰国旗问题"},{"content":"","href":"/tags/life/","title":"Life"},{"content":" 在北邮一年的交换学习已经结束。仔细想想我这一年，好像并没有什么值得一提的。在我的印象里，第一学期的大多数时间都花在了学校开设的课程上，总是在完成课程作业或者相关的东西，取得一个看起来还不错的成绩真心不容易。出于对新地方的好奇感，在周末，我总是喜欢出去转转，一个学期下来，我对北京的了解程度居然超过了已经在北京上了两年大学的室友。\n  到了第二学期，课程数量只有第一学期的一半，这就有了大量可支配的自由时间。我变得空闲下来，这个时候开始思考自己接下来到底该做什么。刚开学的3月份，北邮就有不少人开始准备考研了。对于考研，我也陷入了犹豫当中，过了一个月，我意识到再这么犹豫下去是不行的。于是我决定买一本《李永乐数学复习全书》，先做做试试看。两个月过去了，《全书》也看完了。然后我发现，我的内心是抗拒考研的，我更加倾向于毕业后直接工作。于是我丢弃了那本充满笔墨的《全书》，开始思考就业的事情。也开始复习自己的专业技能。\n  真正让我获益很大的是第二个学期的那些空闲时间，这让我有了很多的时间来思考自己到底想要什么。特别是听过北京GDG在3月底举办的灯塔IO和微软在6月举办的Insider Dev Tour之后，我对自己应该提升的能力有了更加明确的认识。他们也正是我想告诉学弟学妹们的一些建议。\n 在北京的一些经历，让我看到了很可怕的一点: 比你优秀的人比你还努力！\n我绝不是一个优秀的计算机专业的学生，但我正朝成为优秀的计算机人才努力。如何让自己变得更加的优秀呢?我觉得要诀有下:\n 夯实专业基础，提升专业能力水平； 树立终身学习的目标，学习是没有止境的； 培养一种不轻易放弃的态度；  PS：下面只是一些我个人的看法，要学会用批判的态度看待各种东西。\n专业相关 专业课程 我是大二才转来计算机学院的。在大二，我们最重要的专业课有《离散数学》、《数据结构》。我敢肯定的说:除了我之外，还有很多人在那个时候不知道《离散数学》和《数据结构》这两门课对我们计算机专业的学生来说是多么的重要。当然还有同一学期的《计算机组成原理》，这是一门偏理论和硬件的课程。我们很多人都觉得我一个计算机专业的学生多学点编程，整天学这一堆数学和与硬件相关的东西有什么用？！\n我觉得让我们产生这些想法的原因有两个：\n 老师没有在课程开始之前给我们讲清楚这门课程对我们到底有什么用 我们自己没有主动去搞清楚这些课程到底有什么用  总而言之，我们不知道这些课程到底有什么用。后来，我终于明白了学校给我们开设的课程到底有什么用了：\n 在说数学的重要性之前，我觉得应该祭出《数据结构》和《算法设计与分析》这两门课。它们是我们用编程解决复杂问题必须要考虑的东西。当我们遇到问题，在开始编写代码之前，我们需要先设计好程序的总体框架，划分程序的功能模块。然后决定不同功能模块应该采用怎样的算法以及数据结构。这时我就想起来了那个著名的公式：$$ 程序 = 算法 + 数据结构 $$而写算法不仅仅是用一些数学模型，还有逻辑推理的过程。很多时候你想到的东西到最后实现的时候都要用到数学。 再说说数学，数学是各种算法的基础。我们学校计科专业在大学主要就学《高等数学》、《离散数学》、《线性代数》和《概率论与数理统计》这四门课程。《高等数学》放哪个理工科的专业你都得学，它还是《概率论与数理统计》基础，学过《概率论与数理统计》的都知道，这里面会遇到一大堆微积分相关的知识。我认为《离散数学》是我们计算机专业学生最应该好好学习的一门数学课，它是算法的基础，是加密理论的基础。当你学习《算法设计与分析》这么课程的时候，你就会再次见到很多《离散数学》里面的知识了，只不过这次你是用它们而不是学它们。最后剩下《线性代数》和《概率论与数理统计》，关于它们的用处，你随便翻一下一本有关机器学习、数据挖掘、自然语言处理导论等这些当前比较火的人工智能技术就知道了。关于数学在计算机领域的应用，有一本书可以直观的解决你们的疑惑——吴军的《数学之美》。 还有一堆理论和硬件相关的课程。它们告诉了我们计算机是如何工作和构成的。它们或许不能直接给我们带来收益，但你绝对会在以后的工作生涯里收益于它们。它们可能是你多于同龄人的一个技能。  编程能力 编程能力是我们计算机专业学生最基本的能力。我们通过编程来解决问题，实现自己的想法。你的编程能力和你编写的代码量有着直接的关系。我以前都是懒得写程序，总觉得有些太简单，用不着直接把程序写出来。后来一遇到需要编程解决的问题的时候，往往会遇到“难产”的问题，半天写不出几行代码，不知道从哪开始。意识到这个问题之后，我就开始了积累代码量的过程。\n编程语言 编程语言层出不穷，各有各自的优势。与此对应的还有不断涌现的各种新技术。我觉得编程语言不应该是我们重点学习的对象，编程语言只是解决问题的一个工具。我们需要重点学习的是编程思想，然后精通一门面向过程的语言和一门面向对象的语言。在我们需要使用新的编程语言来解决问题的时候，找一份Tutorial,是能够很快速的入门的，然后借助于编程语言的文档和搜索引擎就可以开始工作了。\n所以我认为需要重点学习的是解决问题的思想，编程的思想，抽象思维，原理和方法。\n积累经验 趁着还在大学，有着充足的学习时间，做一些小的项目，积累一些项目经验吧，它将是我们最宝贵的财富之一。写点东西来记录是一个不错的方式。\n关于学习 除了需要学习编程以外，我觉得写作和英语也是我们应当重点培养的能力。\n学习写作 为什么要学习写作呢？我说的写作可不是让你写什么文学作品啊。我的意思是，写一些和自己专业相关的东西，记录一些自己在学习过程中遇到的问题以及是如何解决这些问题的，写一些学习笔记。好记性还是敌不过键盘侠啊。一个非常好的方式就是写博客。\n其实我大二的时候就想建一个自己的博客网站了，但当时总觉得自己太菜，写的东西太low，也不知道写什么，然后就放弃了。我真正开始写博客的时间是大三的下学期，那时候我的空闲时间很多，我又是一个喜欢折腾的人，然后就会遇到各种奇奇怪怪的问题。遇到了问题怎么办呢？当然是Google了，一般情况下，通过Google总能找到问题的解决方案，但有时候也会花掉你大量的时间。如果每次出现的问题都是不同的问题，那么这是没有什么影响的，还是得花时间去查。因为每次都是照着别人讲的方法一步步做解决问题的，基本上解决完问题过不了多久就会忘记。等到同样的问题再次出现的时候，又要重复之前的工作，去尝试那些可能的解决方案，这就很浪费时间了。那么有没有什么可以减少这种重复性劳动的方法呢，方法就是自己写博客记录下这次的解决方案，以备下次使用。\n其实我的博客是写给我自己看的，我觉得可以了就可以了，不用担心别人怎么说。\n建立自己的博客网站 我用Hugo(一个用Go语言编写静态网站生成框架)和Github Pages建立了自己的博客网站，其实就是记录一些自己学习的东西以及踩坑日记，方便自己以后回忆或者查阅。比起在电脑本地笔记来说，一个能够随时通过手机或者电脑访问的博客网站的优势不知道好到了哪里去。如果你能够让别人通过搜索引擎找到你写的东西并从其中受益，那么我觉得你应该会很高兴。\n当前搭建博客网站的框架很多，使用起来也很方便。主流的有Hexo、Hugo、Jekyll等等。关于它们，网上都有很多的教程。Hugo的编译速度是最快的，同时支持Markdown语法，但网上的中文参考资料不如其它的多，绝大多数多是英文资料。中文文档至今还没翻译完成，使用Hugo需要一定的英语能力。 选择一个漂亮的皮肤，你的博客网站就会显得很漂亮。\n有人可能会问，写博客不会花很多时间吗？写博客当然会花时间，但它能够帮你节省更多的时间，还能够帮助形成你自己的知识体系。那做笔记来说，如果你有做笔记的习惯，笔记都是要做的，那么为什么不把笔记写在一个你更加容易查阅的地方呢。\n写下你的想法并分享给别人 近年来，知识付费的潮流开始兴起。如果你拥有一个粉丝圈，或者是你能够提供别人需要的知识，你就能从中获得回报。在这之前，别人得知道你并认可你。或许写博客能够成为你自我营销的一种方式呢。\n学习英语 可以毫不夸张的说，英语是程序员除母语外的第二语言。不单单是计算机专业，各行各业都被英语影响着。Wikipedia上英语词条的数目远超中文词条，而且英文词条的质量也更高。 拿我们计算机专业来说，我们遇到的绝大多数问题都能在**StackOverFlow**上找到答案，然而这个网站是英文的。如果你的英语很差，只能靠着百度搜索一些国内的资料也是能正常工作的，汉语是我们的母语，没有交流障碍，也更加容易理解。但一到专业方面，尤其是我们计算机行业的东西，光靠中文可能就不够用了。\n计算机的相关理论几乎都是从国外发展起来的，很多专业文档都是用英语写的。如果你对比阅读过一本国外的原版英文教材和它在国内的翻译版本，你最终应该会认为：还是原版的好理解，中文翻译得太别扭了。国外的教材都很厚，不像国内的教材那么薄，那是因为国外上课讲的少，作者为了方便学生自学才写了那么多的内容，国内主要的知识还是靠老师来讲，很多课程拿着老师上课的PPT不用教材也能轻松通过，可最后学到的东西有那么多那么扎实吗？看翻译版的书的时候，你可能就不能那么准确的领会原作者的意图了，翻译是不可能做到百分之百准确的。\n当我们写程序的时候，很多时候都需要去查阅相关的技术文档。这个时候你可以使用网页翻译插件来帮助你你看懂文档，但插件翻译出来的准确度也是有限制的。最好的方法还是自己看，那样不会有太多的信息的损失，也没有那么容易产生歧义。\n如何学 如果只是为了能够看懂英文文档，那就容易多了。你只需要不断地积累自己的词汇量就可以了，不断地去眼熟新的单词，看多了自然就记住了。我学习英语词汇用的就是：Vocabulary.com + 欧路词典。它们在英语学习方面给了我巨大的帮助。如果你想系统的搞清楚某一个单词是什么意思，我推荐给你一个网站：Vocabulary.com，它用英语解释英语，能够让你直观的感受到单词的意思以及相近单词之间的区别。我一直觉得一种语言是不能完整准备的表达出另一种语言所要表达的信息的。上面那个网站是全英文版的，你可能会看不懂。所以，我再推荐给你一部词典：欧路词典，它支持扩充词库，你可以自由加载各种词库，包括各种原生英文例句。结合英文词库和中文词库，你能够更好的理解一个词的意思。当然，这只是词汇，理解一个单词最好的方法还是要看它所处的语境。就像中国汉字博大精深一样，英文单词也是博大精深。如果你想更好的学习英语，那就要自己想办法了，我已经把我用到的最好的两个软件推荐给你了。我当前的目标是，做到不借助翻译软件阅读英语，口语始终是硬伤。\n专注力 通过北邮学习的经历，我发现很多北邮人程序写得好的原因不单单是因为他们写得多，还有一个重要的原因是他们做事情的时候很专注。我觉得专注力应该也是我们必备的素质之一。当遇到难以解决的问题，不同人处理的态度是不一样的，我之前在网上看到一个缎子描述的很形象。 大概是说：985的正面硬刚，不干出来不罢休；211的先尝试，尝遍各种方法依然无果，于是放弃；民办的见状说，太难了，我选择放弃。 这或许就是不同学校之间的一个重要差距吧。不要轻易的放弃。\n提高工作效率的一个重要方法就是提高自己的专注程度，也就是减少外界干扰自己的程度或者增加自己抵抗外界干扰的程度。\n几个小建议  推荐一本书《软技能：代码之外生存指南》； 趁你还在大学，少呆寝室，多出去走走，外面的世界很精彩； 锻炼身体，少熬夜；  ","href":"/posts/recyclebin/remember_bupt/","title":"北邮回忆"},{"content":" 许多随机算法通过排列给定的输入数组来使输入随机化。这里的目标是构造数组 A 的一个随机排列。\n 方法一：排序 为数组的每一个元素 A[i] 分配一个随机的优先级 P[i] ，然后根据优先级来对数组 A 中的元素进行排序。比如：\\ A = {43, -1, 82, 61, -87, -86, -55, 28, 47, -97}\\ P = {70, 433, 302, 154, 805, 810, 740, 287, 213, 41}\\ 排序后的 A 就是：\\ A = {-97, 43, 61, 47, 28, 82, -1, -55, -87, -86}\n方法二：原地排列 假设数组 A 的长度为 n , 进行 n 次迭代，在第 i 次迭代的时候， 从 A[i..n] 中随机选取一个元素来作为 A[i] 。第 i 迭代之后， A[i] 就不再改变。\n实现 主要java代码如下：\nimport ch02.GenerateTestData; import java.util.Random; class PermutingArraysRandomly{ public void printArray(int[] A){ int n = A.length; for(int i = 0;i \u0026lt; n;i++){ System.out.printf(\u0026#34;%-6d\u0026#34;, A[i]); if((i + 1) % 10 == 0) System.out.println(); } } /** * @param a 下界 * @param b 上界 * @return [a, b]之间的一个随机整数 */ public int rand(int a, int b){ Random random = new Random(); int c = b - a; return random.nextInt(c + 1) + a; } /** * 交换A[i]和A[j]的值 * @param A * @param i * @param j */ public void exchange(int[] A, int i, int j){ int t = A[i]; A[i] = A[j]; A[j] = t; } /** * 根据优先级对数组A进行选择排序 * @param A 待排序数组 * @param P 优先级数组 * @return 根据优先级排序后的数组 */ public int[] selectSort(int[] A, int[] P){ int n = A.length; for(int i = 0;i \u0026lt; n;i++){ int minp = i; for(int j = i + 1;j \u0026lt; n;j++){ // 值小的优先级高  if(P[j] \u0026lt; P[minp]){ minp = j; } } exchange(A, i, minp); exchange(P, i, minp); } return A; } /** * 为数组的每个元素A[i]分配一个随机的优先级P[i]，然后根据优先级对数组A中的元素进行排序。 * @param A * @return 按优先级排序后的数组，即随机排列后的数组 */ public int[] permuteBySorting(int[] A){ int n = A.length; int[] P = new int[n]; // 优先级数组  for(int i = 0;i \u0026lt; n;i++){ P[i] = rand(0, n * n * n + 1); // 使P[i]尽可能唯一  } System.out.println(\u0026#34;\\nP:\u0026#34;); printArray(P); selectSort(A, P); // 根据优先级排序A  System.out.println(\u0026#34;\\nsorted:\u0026#34;); printArray(A); return A; } /** * 原地排列给定数列 * @param A * @return */ public int[] randomizeInPlace(int[] A){ int n = A.length; for(int i = 0;i \u0026lt; n;i++){ exchange(A, i, rand(i, n - 1)); } return A; } } ","href":"/notebook/reading_notes/introduction_to_algorithms/two-ways-to-produce-a-uniform-random-permutation/","title":"产生均匀随机排列的两种方法"},{"content":"","href":"/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/","title":"算法导论"},{"content":"问题描述 这是《算法导论》的习题5.1-3：\n 假设你希望以各1/2的概率输出0和1。你可以自由使用一个输出0或1的过程BIASED-RANDOM。它以概率 p 输出1，以概率 1-p 输出0，其中 0 \u0026lt; p \u0026lt; 1，但是你并不知道 p 的值。给出一个利用BIASED-RANDOM作为子程序的算法，返回一个无偏向的结果。\n 解决方案 分析 我们并不知道 p 的取值，但我们可以通过BIASED-RANDOM来重新构造一个过程，使之等概率的输出0和1。考虑连续两次调用BIASED-RANDOM得到的结果 P(ij)，将会得到如下结果： $$ P(00) = p^2 $$ $$ P(01) = (1 - p)p $$ $$ P(10) = p(1 - p) $$ $$ P(11) = (1 - p)^2 $$ 可以发现：P(01) = P(10)。利用产生 01 和 10 的概率相等这个性质就可以解决题目中的问题了。\njava代码 private int biasedRandom(){ Random random = new Random(); return random.nextInt(2); // 随机产生[0,1]内的整数  } public int rand(){ while(true){ int i = biasedRandom(), j = biasedRandom(); if(i == 1 \u0026amp;\u0026amp; j == 0){ return 1; } else if(i == 0 \u0026amp;\u0026amp; j == 1){ return 0; } else continue; } } ","href":"/notebook/reading_notes/introduction_to_algorithms/%E9%80%9A%E8%BF%8701%E6%9C%89%E5%81%8F%E6%A6%82%E7%8E%87%E7%94%9F%E6%88%90%E6%97%A0%E5%81%8F%E6%A6%82%E7%8E%87/","title":"通过偏概率0/1生成器，得到无偏概率0/1生成器"},{"content":"","href":"/tags/python/","title":"Python"},{"content":" Anaconda 是一个 python 的发行版，可以用来管理 python 的包和环境，同时它包含1000+的开源package。正如那句话一样：\\ The Most Trusted Distribution for Data Science\\ 还有一个没有包含那么多包的 python 发行版，叫做Miniconda。\n 是用Anaconda Navigator还是conda Navigator 和 conda 都是可以用来管理包和环境。在安装完 Anaconda 之后，它们就都已经存在于系统之中了。区别是：\n Navigator 是图形界面 conda 是命令行界面  可以同时使用它们来进行管理。\n还是习惯使用conda 这里有一份conda cheat sheet。花点时间看看，就能开始使用 conda 了。\n.condarc 是 conda 的配置文件，它是可选的。当我们第一次运行 conda config 命令的时候, 这个文件就会被创建，通常位于：C:\\user\\username\\.condarc。可以通过直接修改它来配置 conda ，因为 conda config 命令的结果最终会写到 .condarc 中。\n配置源(channels) Anaconda 默认的源位于国外，在国内访问的速度不够快。可以采用国内的镜像来加快访问速度。我一般使用的是清华的源，运行以下命令：\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes 通过命令：\nconda info 可以查看当前的配置信息。如果添加成功，你会看到刚才所添加的链接。不过也可以直接查看 .condarc 这个文件，文件的内容目前大致是这个样子：\nchannels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - defaults ssl_verify: true show_channel_urls: true 其中有一个字段是 defaults, 这使得所有默认的源都包含在当前的源里面，当然也可以删除它。\\ 我们也可以改变默认的源，修改**defalut_channels** 即可。\n让conda自动更新(auto_update_conda) conda 默认是自动更新的，每当用户在root环境更新或安装包的时候，就会触发。可以采用命令：\nconda config --set auto_update_conda false 来关闭自动更新。这时候 conda 就只会在用户运行手动更新的命令 conda update 的时候及进行更新了。\nAlway yes(always_yes) 每当我们执行安装包的命令的时候， conda 都会问我们Proceed ([y]/n)?。如果不想每次都回答这个问题，可以这么设置 always_yes :\nconda config --set always_yes true 目前我用到的配置部分大致就是这些。这里有一份关于使用 .condarc 的完整说明。\n","href":"/posts/python/anaconda-source-configuration/","title":"配置Anaconda源"},{"content":"矩阵乘法 矩阵相乘只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有定义。若 A 为 m x n 矩阵，B为 n x p 矩阵，则他们的乘积 C = AB 会是一个 m x p 矩阵。其乘积矩阵的元素如下面式子得出： $$C_{ij} = \\sum_{k = 1}^n A_{ik} B_{kj}$$\n解决方案 常规方法求解 根据矩阵相乘的定义，可以直接求解两个矩阵的乘积。但这种方法的时间复杂度为：$ O(n^3) $。java代码如下：\npublic int[][] matrixMultiply(int[][] A, int[][] B) { int rowc = A.length, colc = B[0].length, colab = A[0].length; int[][] C = new int[rowc][colc]; for (int i = 0; i \u0026lt; rowc; i++) { for (int j = 0; j \u0026lt; colc; j++) { for (int k = 0; k \u0026lt; colab; k++) { C[i][j] += A[i][k] * B[k][j]; } } } return C; } 分治法求解 使用分治法求解矩阵乘法实际上是利用了分块矩阵的性质。为了方便使用分治法，假定相乘的矩阵都是 n x n 的，其中 n 是2的幂。分治法将 n x n 的矩阵划分为4个 n/2 x n/2 的子矩阵，然后进行运算。假设矩阵 A 、 B 、 C 均满足上面的要求，那么它们可以进行如下表示： $$ A = \\begin{bmatrix}A_{11} \u0026amp; A_{12} \\\\ A_{21} \u0026amp; A_{22} \\\\ \\end{bmatrix}, B = \\begin{bmatrix}B_{11} \u0026amp; B_{12} \\\\ B_{21} \u0026amp; B_{22} \\\\ \\end{bmatrix}, C = \\begin{bmatrix}C_{11} \u0026amp; C_{12} \\\\ C_{21} \u0026amp; C_{22} \\\\ \\end{bmatrix} $$ 于是有： $$ A = \\begin{bmatrix}C_{11} \u0026amp; C_{12} \\\\ C_{21} \u0026amp; C_{22} \\\\ \\end{bmatrix} = \\begin{bmatrix}A_{11} \u0026amp; A_{12} \\\\ A_{21} \u0026amp; A_{22} \\\\ \\end{bmatrix} \\begin{bmatrix}B_{11} \u0026amp; B_{12} \\\\ B_{21} \u0026amp; B_{22} \\\\ \\end{bmatrix} $$ 也就是说： $$ C_{11} = A_{11}B_{11} + A_{12}B_{21} $$ $$ C_{12} = A_{11}B_{12} + A_{12}B_{22} $$ $$ C_{21} = A_{21}B_{11} + A_{22}B_{21} $$ $$ C_{22} = A_{21}B_{12} + A_{22}B_{22} $$ 《算法导论》上的伪代码如下： 书上的伪代码掩盖了一个非常重要的细节，那就是如何分解矩阵。如果真的创建出12个子矩阵，那将会花费O($ n^2 $)的时间来复制矩阵的元素。不过书上给出了一个十分关键的提示——用下标来指明一个子矩阵。如此一来，就可以避免对矩阵的复制操作，只需花费O(1)的时间。既然矩阵符合上面的要求，只需要改变矩阵的表示方式即可。添加子矩阵在原矩阵中的开始位置及子矩阵的大小就好了。\njava实现的代码如下：\n/** * @param ra A中行开始下标 * @param ca A中列开始下标 * @param n 子方阵宽度 */ private int[][] squareMatrixMultiply(int[][] A, int[][] B, int[][] C, int ra, int ca, int rb, int cb, int rc, int cc, int n) { if (n == 1) { C[rc][cc] = A[ra][ca] * B[rb][cb]; // 只有一个元素  } else if (n == 2) { // 都只有四个元素  C[rc][cc] += A[ra][ca] * B[rb][cb] + A[ra][ca + 1] * B[rb + 1][cb]; // C11 = A11 * B11 + A12 * B21  C[rc][cc + 1] += A[ra][ca] * B[rb][cb + 1] + A[ra][ca + 1] * B[rb + 1][cb + 1]; // C12 = A11 * B12 + A12 * B22  C[rc + 1][cc] += A[ra + 1][ca] * B[rb][cb] + A[ra + 1][ca + 1] * B[rb + 1][cb]; // C21 = A21 * B11 + A22 * B21  C[rc + 1][cc + 1] += A[ra + 1][ca] * B[rb][cb + 1] + A[ra + 1][ca + 1] * B[rb + 1][cb + 1]; // C22 = A21 * B12 + A22 * B22  } else { n /= 2; // 分解矩阵  squareMatrixMultiply(A, B, C, ra, ca, rb, cb, rc, cc, n); // A11 * B11  squareMatrixMultiply(A, B, C, ra, ca + n, rb + n, cb, rc, cc, n); // A12 * B21  squareMatrixMultiply(A, B, C, ra, ca, rb, cb + n, rc, cc + n, n); // A11 * B12  squareMatrixMultiply(A, B, C, ra, ca + n, rb + n, cb + n, rc, cc + n, n); // A12 * B22  squareMatrixMultiply(A, B, C, ra + n, ca, rb, cb, rc + n, cc, n); // A21 * B11  squareMatrixMultiply(A, B, C, ra + n, ca + n, rb + n, cb, rc + n, cc, n); // A22 * B21  squareMatrixMultiply(A, B, C, ra + n, ca, rb, cb + n, rc + n, cc + n, n); // A21 * B12  squareMatrixMultiply(A, B, C, ra + n, ca + n, rb + n, cb + n, rc + n, cc + n, n); // A22 * B22  } return C; } Strassen方法 Strassen方法不是很直观，它的核心思想是减少乘法的次数。分治法用了8次乘法，而Strassen方法只用了7次乘法，时间复杂度为$ O(n^{lg_7}) $。它的步骤如下：\n 先将矩阵A、B、C分解为 n/2 x n/2 的子矩阵，可以采用上面的下标计算方式，但我重新创建了子矩阵。 创建10个 n/2 x n/2 的矩阵$ S_1, S_2, \u0026hellip;, S_{10} $:  $$ S_1 = B_{12} - B_{22} $$ $$ S_2 = A_{11} + A_{12} $$ $$ S_3 = A_{21} + A_{22} $$ $$ S_4 = B_{21} - B_{11} $$ $$ S_5 = A_{11} + A_{22} $$ $$ S_6 = B_{11} + B_{22} $$ $$ S_7 = A_{12} - A_{22} $$ $$ S_8 = B_{21} + B_{22} $$ $$ S_9 = A_{11} - A_{21} $$ $$ S_{10} = B_{11} + B_{12} $$ 3. 递归的计算7次 n/2 x n/2 的矩阵乘法： $$ P_1 = A_{11}S_1 = A_{11}B_{12} - A_{11}B_{22} $$ $$ P_2 = B_{22}S_2 = A_{11}B_{22} + A_{12}B_{22} $$ $$ P_3 = B_{11}S_3 = A_{21}B_{11} + A_{22}B_{11} $$ $$ P_4 = A_{22}S_4 = A_{22}B_{21} - A_{22}B_{11} $$ $$ P_5 = S_5S_6 = A_{11}B_{11} + A_{11}B_{22} + A_{22}B_{11} + A_{22}B_{22} $$ $$ P_5 = S_5S_6 = A_{11}B_{11} + A_{11}B_{22} + A_{22}B_{11} + A_{22}B_{22} $$ $$ P_6 = S_7S_8 = A_{12}B_{21} + A_{12}B_{22} - A_{22}B_{21} - A_{22}B_{22} $$ $$ P_7 = S_9S_{10} = A_{11}B_{11} + A_{11}B_{12} - A_{21}B_{11} + A_{21}B_{12} $$ 4. 对$ P_i $ 执行加减法运算得到 C 的四个子矩阵： $$ C_{11} = P_5 + P_4 - P_2 + P_6 $$ $$ C_{12} = P_1 + P_2 $$ $$ C_{21} = P_3 + P_4 $$ $$ C_{22} = P_5 + P_1 - P_3 - P_7 $$ 由于我直接拆分了矩阵，因此这一步还需要合并四个子矩阵到 C 。\njava 代码如下：\nprivate int[][] squareMatrixMultiplyStrassen(int[][] A, int[][] B, int ra, int ca, int rb, int cb, int n) { int[][] C = new int[n][n]; if (n == 1) { C[0][0] = A[ra][ca] * B[rb][cb]; return C; } else { // step 1: 分解矩阵  n /= 2; // step 2: 创建10个矩阵  int[][] S1 = squareMatrixAddorSub(B, B, rb, cb + n, rb + n, cb + n, n, false); // S1 = B12 - B22  int[][] S2 = squareMatrixAddorSub(A, A, ra, ca, ra, ca + n, n, true); // S2 = A11 + A12  int[][] S3 = squareMatrixAddorSub(A, A, ra + n, ca, ra + n, ca + n, n, true); // S3 = A21 + A22  int[][] S4 = squareMatrixAddorSub(B, B, rb + n, cb, rb, cb, n, false); // S4 = B21 - B11  int[][] S5 = squareMatrixAddorSub(A, A, ra, ca, ra + n, ca + n, n, true); // S5 = A11 + A22  int[][] S6 = squareMatrixAddorSub(B, B, rb, cb, rb + n, cb + n, n, true); // S6 = B11 + B22  int[][] S7 = squareMatrixAddorSub(A, A, ra, ca + n, ra + n, ca + n, n, false); // S7 = A12 - A22  int[][] S8 = squareMatrixAddorSub(B, B, rb + n, cb, rb + n, cb + n, n, true); // S8 = B21 + B22  int[][] S9 = squareMatrixAddorSub(A, A, ra, ca, ra + n, ca, n, false); // S9 = A11 - A21  int[][] S10 = squareMatrixAddorSub(B, B, rb, cb, rb, cb + n, n, true); // S10 = B11 + B12  // Step 3: 计算7次乘法  int[][] P1 = squareMatrixMultiplyStrassen(A, S1, ra, ca, 0, 0, n); // P1 = A11 * S1  int[][] P2 = squareMatrixMultiplyStrassen(S2, B, 0, 0, rb + n, cb + n, n); // P2 = S2 * B22  int[][] P3 = squareMatrixMultiplyStrassen(S3, B, 0, 0, rb, cb, n); // P3 = S3 * B11  int[][] P4 = squareMatrixMultiplyStrassen(A, S4, ra + n, ca + n, 0, 0, n); // P4 = A22 * S4  int[][] P5 = squareMatrixMultiplyStrassen(S5, S6, 0, 0, 0, 0, n); // P5 = S5 * S6  int[][] P6 = squareMatrixMultiplyStrassen(S7, S8, 0, 0, 0, 0, n); // P6 = S7 * S8  int[][] P7 = squareMatrixMultiplyStrassen(S9, S10, 0, 0, 0, 0, n); // P7 = S9 * S10  // Step4: 根据P1-P7计算出C  int[][] T1 = squareMatrixAddorSub(P5, P4, 0, 0, 0, 0, n, true); // P5 + P4  int[][] T2 = squareMatrixAddorSub(P2, P6, 0, 0, 0, 0, n, false); // P2 - P6  int[][] C11 = squareMatrixAddorSub(T1, T2, 0, 0, 0, 0, n, false); // C11 = P5 + P4 - P2 + P6  int[][] C12 = squareMatrixAddorSub(P1, P2, 0, 0, 0, 0, n, true); // C12 = P1 + P2  int[][] C21 = squareMatrixAddorSub(P3, P4, 0, 0, 0, 0, n, true); // C21 = P3 + P4  int[][] T3 = squareMatrixAddorSub(P5, P1, 0, 0, 0, 0, n, true); // P5 + P1  int[][] T4 = squareMatrixAddorSub(P3, P7, 0, 0, 0, 0, n, true); // P3 + P7  int[][] C22 = squareMatrixAddorSub(T3, T4, 0, 0, 0, 0, n, false); // C22 = P5 + P1 - P3 - P7  // 合并C11、C12、C21、C22为C  for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i][j] = C11[i][j]; } } for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i][j + n] = C12[i][j]; } } for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i + n][j] = C21[i][j]; } } for (int i = 0; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; n; j++) { C[i + n][j + n] = C22[i][j]; } } } return C; } 后记 当然，并不是所有的矩阵都是方阵， 也不是所有的方阵的元素个数的都是2的幂。为了让Strassen方法适用于一般的矩阵乘法，可以通过在原矩阵的行列补零得到上面要求的矩阵。在这之前，需要判断这个矩阵是不是上面的方阵，也就是判断方阵每行的元素个数是不是2的幂次方。\n一种简单的方法如下：\nprivate boolean isSquareMatrix(int[][] A) { int rowa = A.length, cola = A[0].length; if ((rowa == cola) \u0026amp;\u0026amp; ((rowa \u0026amp; (rowa - 1)) == 0)) return true; else return false; } 比如：8 \u0026amp; 7 = (1000 \u0026amp; 0111) = 0\n如果矩阵不是我们需要的方阵，那么在将其转换成理想形式的方阵之前， 还需要找到比矩阵每行和每列的元素个数都要大并且最接近的2的幂次方数，下面是一种可行的方法：\n// 获取比a大，最接近a的2的幂次方数  private int nextP2(int a){ int t = 1; while(t \u0026lt; a) t \u0026lt;\u0026lt;= 1; return t; } 准备工作都做好了，接下来就是将矩阵转化为我们理想的方阵了，方法很简单，直接将原来的矩阵复制进新的矩阵就行了：\nprivate int[][] convertMatrixToStandardSquareMatrix(int[][] A, int n){ int rowa = A.length, cola = A[0].length; int[][] C = new int[n][n]; for(int i = 0;i \u0026lt; rowa;i++){ for(int j = 0;j \u0026lt; cola;j++){ C[i][j] = A[i][j]; } } return C; } 如此以来，Strassen方法所需要的条件都有了，但这么计算出来的结果中会出现很多很多的0。这看起来是很不舒服的，于是，可以先保存原来矩阵的行数和列数，然后去掉多余的0，得到常规方法计算矩阵乘法的结果。\n// 处理不是2的指数次幂的矩阵变成方阵后运算多出来的那一大堆0  private int[][] removeZerosInMatrix(int[][] A, int rowa, int cola){ int[][] C = new int[rowa][cola]; for(int i = 0;i \u0026lt; rowa;i++){ for(int j = 0;j \u0026lt; cola;j++){ C[i][j] = A[i][j]; } } return C; } ","href":"/notebook/reading_notes/introduction_to_algorithms/matrix-multiplication/","title":"矩阵乘法"},{"content":"问题  有一个数组A，寻找一个 A[] 的子数组 B[] ， 使得B的元素和大于A的任何一个子数组。比如A = [13, -3, 25, 20, -4, -20, -25, 18, 20, -5, 16, -5, -22, 18, -6, 8], 我们要求的 B[] 就是：[18, 20, -5, 16], 它的和大于 A[] 的任何一个子数组。\n 解决方案 暴力法求解 最简单也最直接的解题方式就是暴力法（Brute force）。用暴力法直接穷尽 A[] 的所有子数组， 然后选择其中最大的那个作为 B[] 就可以了。使用暴力法的时间复杂度为 \\( O(n^2) \\) 。 java代码如下:\npublic int[] findMaxSubarray(int left, int right, int[] data){ int i, j, maxLeft = left, maxRight = left, sum = Integer.MIN_VALUE, tempSum; for(i = left;i \u0026lt;= right;i++){ tempSum = 0; // data[i..j]的和  for(j = i;j \u0026lt;= right;j++){ tempSum += data[j]; if(tempSum \u0026gt; sum){ sum = tempSum; maxLeft = i; maxRight = j; } } } return Arrays.copyOfRange(data, maxLeft, maxRight + 1); } 虽然暴力法能够得到我们想要的结果，但是因为暴力法的解空间巨大，因此只适用于数量不大的场合。下面采取的是一种较为高效的方法——分治法。\n分治法求解 假如我们要求的是 A[left, right] 这个数组的最大子数组。分治法的思想是：divide -\u0026gt; conquer -\u0026gt; combine , 就是说：先把大问题分解成一系列的小问题，然后求解小问题，最后把小问题的解合并起来得到大问题的解。当然，有一个前提就是：大问题的解的确是有这些小问题构成的。 对于任何一个长度大于2的数组 A[left, right] ， 我们总能把它分解为两个子数组： A[left, mid] 和 A[mid + 1, right] 。假设我们要求的是 B[] = A[i, j] ,如此一来， A[i, j] 只有三种可能：\n 完全位于 A[] 的左半部分，即： left \u0026lt;= i \u0026lt;= j \u0026lt;= mid ; 完全位于 A[] 的右半部分，即： mid + 1 \u0026lt;= i \u0026lt;= j \u0026lt;= right ; 同时位于左右两部分， 即： left \u0026lt;= i \u0026lt;= j \u0026lt;= right ;  对于前两种情况，可以直接递归求解。对于第三种情况，我们就要采用其它的方式了。对于越过中点的第三种情况， A[i, j] 可以拆分为 A[i, mid] 和 A[mid + 1, j] 。 由于中点包含在结果之内，所以可以这么做：\n 对于左半部分， 从 A[mid] 开始， 向 A[left] 靠近， 寻找 A[i, mid] ; 对于右半部分， 从 A[mid + 1] 开始， 向 A[right] 靠近， 寻找 A[mid + 1, j] ;  使用分治法的时间复杂度为：O(nlogn).\njava实现的代码如下：\npublic int[] findMaxCrossingArray(int left, int mid, int right, int[] data){ int maxLeftSum = Integer.MIN_VALUE; int sum = 0; int i, j, maxLeft = mid, maxRight = mid + 1; for(i = mid;i \u0026gt;= left;i--){ // [left, mid]的maxSum  sum += data[i]; if(sum \u0026gt; maxLeftSum){ maxLeftSum = sum; maxLeft = i; } } sum = 0; int maxRightSum = Integer.MIN_VALUE; for(j = mid + 1;j \u0026lt;= right;j++){ // [mid + 1, right] 的maxSum  sum += data[j]; if(sum \u0026gt; maxRightSum){ maxRightSum = sum; maxRight = j; } } return Arrays.copyOfRange(data, maxLeft, maxRight + 1); // 得到A[i, j]  } 然后下面是递归求解原问题的代码：\npublic int[] findMaxSubarray(int left, int right, int[] data){ // 寻找最大子数组  if(left == right) return data; else{ int mid = (left + right) / 2; int[] maxLeftArray = findMaxSubarray(left, mid, data); int[] maxRightArray = findMaxSubarray(mid + 1, right, data); int[] maxCrossingArray = findMaxCrossingArray(left, mid, right, data); int sumLeft = sumArray(0, maxLeftArray.length - 1, maxLeftArray); int sumRight = sumArray(0, maxRightArray.length - 1, maxRightArray); int sumCrossing = sumArray(0, maxCrossingArray.length - 1, maxCrossingArray); if(sumLeft \u0026gt;= sumRight \u0026amp;\u0026amp; sumLeft \u0026gt;= sumCrossing) return maxLeftArray; else if(sumRight \u0026gt;= sumLeft \u0026amp;\u0026amp; sumRight \u0026gt;= sumCrossing) return maxRightArray; else return maxCrossingArray; } } 动态规划求解 《算法导论》在习题中讲到：\n Use the following ideas to develop a nonrecursive, linear-time algorithm for the maximum-subarray problem. Start at the left end of the array, and progress toward the right, keeping track of the maximum subarray seen so far. Knowing a maximum subarray of A[1..j], extend the answer to find a maximum subarray ending at index j + 1 by using the following observation: a maximum subarray of A[1..j + 1] is either a maximum subarray of A[1..j] or a subarray A[i..j + 1], for some 1 \u0026lt;= i \u0026lt;= j + 1. Determine a maximum subarray of the form A[i..j + 1] in constant time based on knowing a maximum subarray ending at index j .\n 用中文来说就是：\n 若已知A[1\u0026hellip;j]的最大子数组，基于如下性质可以得到 A[1..j + 1] 的最大子数组: A[1..j + 1] 的最大子数组要么是 A[1..j] 的最大子数组，要么是某个子数组 A[i..j + 1] (1 =\u0026lt; i \u0026lt;= j + 1) 。这个算法是线性时间O(n)。\n 因此，需要记录 A[1..j] 的最大子数组。于是我们可以这么想：如果前面的若干和小于0， 这对结果没有任何的帮助，应该丢弃，重新开始计算并更新位置标记；否则，向后延伸。 java代码如下：\npublic int[] findMaxSubarray(int left, int right, int[] data){ int maxLeft = left, maxRight = left, sum = Integer.MIN_VALUE, temp = left; int[] tempSum = new int[right - left + 1]; tempSum[0] = data[left]; for(int i = left + 1;i \u0026lt;= right;i++){ if(tempSum[i - left - 1] \u0026lt; 0){ // 前面的最大和小于0，直接丢弃, 从下一个开始考虑  tempSum[i] = data[i]; temp = i; } else{ // 向后延伸  tempSum[i - left] = tempSum[i - left - 1] + data[i]; } if(tempSum[i - left] \u0026gt; sum){ sum = tempSum[i - left]; maxRight = i; maxLeft = temp; } } return Arrays.copyOfRange(data, maxLeft, maxRight + 1); } ","href":"/notebook/reading_notes/introduction_to_algorithms/maximum-sub-array-problem/","title":"最大子数组问题"},{"content":"","href":"/tags/c%E7%AE%97%E6%B3%95/","title":"C算法"},{"content":" 键索引搜索方法中，表中的第 i 个位置保存了键为 i 对应的项，以便达到快速访问的目的。它将键作为数组的索引，并且依赖于同一范围内不同整数的键作为表的索引。这种方法不适用于更一般的键。哈希方法扩展了键索引搜索，它通过算术运算把键转换为表地址，以达到快速访问的目的。\n 使用哈希的搜索算法包括两个部分：\n 哈希函数：将键转化为表地址 冲突调节：解决映射到同一表地址之间的键的冲突  哈希函数 如果我们有一个长度为 M 的表，哈希函数要做的就是把键转换为 [0, M-1] 之间的整数。\n取模哈希函数 一种常用的方法就是选择一个素数 M 做为表长，然后： h(x) = k mod M 。其中 k 为键所对应的整数。 对于整数键，还可以采用乘法取模法：h(x) = [k * α] mod M 。 α 常设为黄金比（0.618033\u0026hellip;）。\n冲突调节 链地址法 为每个散列地址建一个链表，将散列到同一个地址的关键字放入相应的链表中。适合难以预测填入散列表的元素个数并且内存不是太充足的情况。\n线性探测法 如果能够预测填入散列表的元素个数并且内存充足，那么可以使用线性探测法。 当产生冲突时，检查表中的下一个位置，直到找到一个空的位置，然后将项放进去。 线性探测法一次探测可以辨别三种可能的结果：\n 如果表位置包含一个与搜索键匹配的项，则命中； 如果表位置为空，则搜索失败； 如果表位置包含一个与搜索键不匹配的项，则继续探测更高地址直到出现以上两种情况。  再哈希法 再哈希法的基本策略和线性探测法一致。不同的是，他不检查冲突之后的每一个位置，而是使用第二个散列函数得到用于探测序列的固定增量。\n总结  线性探测法是罪最快的（前提：内存足够大，保证哈希表是稀疏的） 再哈希法对内存的使用效率最高，但是需要额外的时间来计算第二个哈希函数 链地址法最易实现和部署  ","href":"/notebook/reading_notes/algorithms_in_c/hashing-table/","title":"哈希表"},{"content":"树是满足一定要求的顶点和边的非空集合。\n二叉树 二叉树的每个节点至多有2个子节点。 一种表示方法：\nstruct Node{type key; Node *lchild, *richild;} typedef Node *link; 这种表示方法只适合从根节点开始自顶向下的操作，而不适合自底向上的操作。不过可以在节点的定义中加入指向父节点的连接支持这种功能。 与二叉树类似的还有M叉树，它的每个节点最多只有M个节点。广义的树每个节点可以有任意多个子节点，可以用二叉树来表示它们，方法就是——“左孩子，右兄弟”。树的序列就形成了有序森林。\n 二叉树和有序森林之间存在一一的对应关系。\n 二叉树的一些数学性质  一棵二叉树有 N 个内部节点，有 N + 1 个外部节点（叶子节点）。 包含 N 个内部节点的二叉树有 2N 个链接： N - 1 个外部节点的链接和 N + 1 个内部节点的链接。 树中节点的所在的层是它的父节点的下一层（根节点位于第 0 层）。树的高度为树节点的最大层。树的路径长度为所有树节点的层总和：外部路径长度为所有外部节点的层总和，内部路径长度为所有内部节点的层总和。 这里有一个计算树路径长度的简便方法：对于所有的 k , 求 k 与 k 层节点数之积的总和。 具有 N 个内部节点的二叉树的外部路径长度比内部路径长度大 2N 。 具有 N 个内部节点的二叉树的高度的最小值为 lgN ，最大值为 N - 1 。 当树退化成只有一个叶子节点的时候，就是最坏的情况。 具有 N 个内部节点的二叉树内部路径长度最小值为 Nlg(N/4) ，最大值为 N(N - 1)/2 。  树的遍历  前序遍历 根-\u0026gt;左孩子-\u0026gt;右孩子 中序遍历 左孩子-\u0026gt;根-\u0026gt;右孩子 后序遍历 左孩子-\u0026gt;右孩子-\u0026gt;根 层次遍历 从上到下，从左到右  前序遍历（递归版）：\nvoid preorderTraverse(link h, void visit(link)){ if(h == root) return; visit(h); preorderTraverse(h -\u0026gt; lchild, visit); preorderTraverse(h -\u0026gt; rchild, visit); } 前序遍历（非递归版）：\nvoid preorderTraverse(link h, void visit(link)){ stack\u0026lt;link\u0026gt; stk(maxn); s.push(h); while(!s.empty()){ visit(s.top()); s.pop(); if(h -\u0026gt; lchild != null) s.push(h -\u0026gt; lchild); if(h -\u0026gt; rchild != null) s.push(h -\u0026gt; rchild); } } 后序遍历和中序遍历只需要交换前序遍历中访问节点的顺序即可。 层次遍历：\nvoid levelTraverse(link h, void visit(link)){ queue\u0026lt;link\u0026gt; q(maxn); q.push(h); while(!q.empty()){ visit(q.front()); q.pop(); if(h -\u0026gt; lchild != null) q.push(q -\u0026gt; lchild); if(h -\u0026gt; rchild != null) q.push(q -\u0026gt; rchild); } } 计算树含有的节点数：\nint count(link h){ if(h == null) return 0; return count(h -\u0026gt; lchild) + count(h -\u0026gt; rchild) + 1; } 计算树的高度：\nint height(link h){ if(h == null) return -1; return max(height(h -\u0026gt; lchild), height(h -\u0026gt; rchild)) + 1; } 二叉搜索树 二叉树搜索树是一棵二叉树，它要么是一棵空树，要么具有以下性质：\n 若任意节点的左子树不为空，则左子树上所有节点的值不大于它的根节点的值 若任意节点的右子树不为空，则右子树上所有节点的值不小于它的根节点的值 任意节点的左右子树都是二叉搜索树 树中没有键值相等的节点  二叉搜索树中的查找 在二叉搜索树h中查找v的过程如下：\n 若h是空树，则返回查找失败，否则： 若x为根节点对应的数据值，则查找成功，否则： 若x小于根节点对应的数据值，则查找左子树，否则： 查找右子树  Item searchP(link h, type v){ if(h == 0) return nullItem; type t = h -\u0026gt; item.getKey(); if(t == v) return h -\u0026gt; item; if(v \u0026lt; t) searchP(h -\u0026gt; lchild, v); else searchP(h -\u0026gt; rchild, v); } 在二叉搜索树中插入节点 在二叉搜索树h中插入节点v的过程如下，其中插入的节点总是叶子节点：\n 若h是空树，则将v所指的节点作为根节点插入，否则： 若v对应的数据值小于根节点对应的数据值，则在左子树中插入，否则： 在右子树中插入  void insertP(link \u0026amp;h, Item x){ if(h == 0) {h = new Node(x);return;} if(x.getKey() \u0026lt; h -\u0026gt; item.getKey()) insertP(h -\u0026gt; lchild, x); else insertP(h -\u0026gt; rchild, x); } 上面的插入只适用于插入的节点最终是叶子节点的情况，可以通过这种方式来构造一棵树来对数据进行排序。对于插入的节点不一定到达叶子节点的情况，需要考虑其它的插入方法。旋转是树的一种基本变换，它允许交换树中根及其一个孩子的角色，同时保持节点中键的次序。 涉及到3个链接和两个节点。\n 右旋（左孩子为轴，当前节点右旋）。结果就是：原来的左孩子成为了新的根，原来左孩子的左孩子依旧是新根的左孩子，旧根的右孩子依旧是旧根的右孩子。旧根成为了新根的右孩子，原来左孩子的右孩子成为了旧根（新根的右孩子）的左孩子。\n void rotateR(link \u0026amp;h){ link t = h -\u0026gt; lchild; h -\u0026gt; lchild = t -\u0026gt; rchild; t -\u0026gt; rchild = h; h = t; }  左旋和右旋相反。右孩子为轴，当前节点左旋。原来的右孩子成为了新的根，旧根成为了新根的左孩子。原来的右孩子的左孩子成为了旧根的右孩子。\n void rotateL(link \u0026amp;h){ link t = h -\u0026gt; rchild; h -\u0026gt; rchild = h -\u0026gt; lchild; t -\u0026gt; lchild = h; h = t; } 有了左旋和右旋之后，就能迅速得到在BST的根插入新节点的递归函数，再适当子树的根插入新项，然后通过旋转将它带到主树的根。\nvoid insertT(link \u0026amp;h, Item x){ if(h == 0){h = new Node(x);return;} if(x.getKey() \u0026lt; h -\u0026gt; item.getkey()){insertT(h -\u0026gt; lchild, x); rotateR(h);} else{insertT(h -\u0026gt; rchild,x); rotateL(h);} } 在二叉搜索树中选择节点 可以采用快速排序划分的思想来选择BST中第 k 小的节点。不过这需要给结点增加一个计数域，然后还需要修改其他所有的函数。\nItem selectR(link h, int k){ if(h == 0) return nullItem; int c = (h -\u0026gt; lchild == 0) ? 0 : h -\u0026gt; lchild -\u0026gt; cnt; if(c \u0026gt; k) return selectR(h -\u0026gt; lchild, k); if(c \u0026lt; k) return selectR(h -\u0026gt; rchild, k - c - 1); return h -\u0026gt; item; } 对二叉搜索树进行划分 可以将选择运算修改为划分运算，它重排树，利用左旋和右旋将第 k 小的元素放到根。\nlink partition(link \u0026amp;h, int k){ int c = (h -\u0026gt; lchild == 0) ? 0 : h -\u0026gt; lchild -\u0026gt; cnt; if(c \u0026gt; k) {partition(h -\u0026gt; lchild, k); rotateR(h);} if(c \u0026lt; k) {partition(h -\u0026gt; rchild, k - c - 1); rotateL(h);} } 在二叉搜索树中删除节点 从BST删除一个节点，首先检查该节点是否在其中一棵子树中。如果是则用递归删除节点后的结果替换子树。如果删除的节点在根部，则需要用合并两棵子树的结果替换原来的树。\nlink joinLR(link l, link r){ if(r == 0) return l; partition(r, 0); r -\u0026gt; lchild = l; return r; } void removeR(link \u0026amp;h, type v){ if(h == 0) return; type w = h -\u0026gt; item.getKey(); if(v \u0026lt; w) removeR(h -\u0026gt; lchild, v); if(v \u0026gt; w) removeR(h -\u0026gt; rchild, v); if(v == w){ link t = h; h = joinLR(h -\u0026gt; lchild, h -\u0026gt; rchild); delete t; } } 合并两棵二叉搜索树 书中的一个线性时间的递归实现：首先，利用根插入将第一课BST的根插入到第二棵BST中。这会得到两棵键小于根的子树和两棵键大于根的子树。然后递归的合并根左子树的前一对与根右子树的后一对来得到结果。\nlink joinAB(link a, link b){ if(a == 0) return b; if(b == 0) return a; insertT(b, a -\u0026gt; item); b -\u0026gt; lchild = jionAB(a -\u0026gt; lchild, b -\u0026gt; lchild); b -\u0026gt; rchild = jionAB(a -\u0026gt; rchild, b -\u0026gt; rchild); delete a; return b; } 2-3-4树 2-3-4树可以在O(logN)的时间内完成查找、插入、和删除操作。\n2-3-4树是一棵空树或者是具有以下三类节点的树：\n 2-节点 它具有一个键，以及具有较小键的左子树和具有较大键的右子树的两个链接。 3-节点 它具有两个键，以及具有较小键的左子树，较大键的右子树和介于节点键之间的中间子树的三个链接。 4-节点 它具有三个键，以及由节点键对应的区间定义的键值的树的四个链接。  节点的插入处理  如果搜索结束的节点是2-节点，将其变为3-节点。 如果搜索结束的节点是3-节点，将其变为4-节点。 如果搜索结束的节点是4-节点，将其分裂成两个2-节点，并将中间键上移到节点的父亲（父节点不是4-节点）。但如果父节点也是4-节点呢？更好的一个方法是：在沿树向下的过程中，分解任何4-节点，保证搜索路径不在4-节点终止。具体做法是：每当遇到一个2-节点（父亲）连接到4-节点（孩子），就把它转化为一个3-节点连接到连个2节点；每当遇到一个3-节点连接到4-节点，就把它转换为一个4-节点连接到两个2-节点。  红黑树 2-3-4树易于理解，但实现困难。 红黑树是2-3-4树的一种简单抽象表达方式。其基本思想是将2-3-4树表示为标准的BST(仅有2-节点)，但为每个节点添加一个额外的信息位，来为3-节点和4-节点编码。\n链接有两种不同的类型：\n 红链接 红链接将包含3-节点和4-节点的小二叉树捆绑在一起。 黑链接 黑链接将2-3-4树捆绑在一起。  红黑树有两个本质特性：\n 不用修改BST的标准搜索过程就能工作。 它们与2-3-4树直接对应。因此可以用上2-3-4树的简单插入平衡过程。  如果某个节点有2个红孩子，则它是4-节点的一部分。红黑树的插入开销很小：仅当看到4-节点的时候才采取平衡措施。分解不同4-节点的具体做法可以对照下图来说：  左一：4-节点的父亲是一个2-节点。 将中间键上移转换为一个3-节点与两个2-节点的连接（变色）。 左二：4-节点的父亲是一个3-节点，并且是它的右孩子。 变色。 左三：4-节点的父亲是一个3-节点，并且是它的左孩子。 先变色得到两个方向相同的红链接，然后右旋。 左四：4-节点的父亲是一个3-节点，并且是它的中间孩子。 先变色得到两个方向不同的红链接，然后右旋得到两个方向相同的红链接，然后再左旋。  用红黑树表示法实现2-3-4树的插入操作，首先要给修改节点定义，加入颜色位（用 1 表示红节点， 0 表示黑节点）。在沿树向下的路径中（递归调用之前），检查4-节点，并通过切换所有3-节点的颜色位来分裂它们。当到达底部时，为被插入的项新建一个红节点并返回它的指针。在沿向上的路径中（递归调用之后），检查是否需要执行一次旋转操作：如果路径上有两个相同方向的红链接，则从上方节点进行一次旋转，然后切换颜色位，以形成一个正确的4-节点；如果路径上有两个方向不同的红链接，则从下方的节点执行一次旋转，以简化为另一种情况，留作向上的下一步处理。\nint getColor(link x){if(x == 0) return 0; return x -\u0026gt; color;} void insertRB(link \u0026amp;h, Item x, int sw){ if(h == 0){h = new Node(x); return;} if(getColor(h -\u0026gt; lchild) \u0026amp;\u0026amp; getColor(h -\u0026gt; rchild)){ // 变色, 分裂4-节点为3-节点  h -\u0026gt; color = 1; h -\u0026gt; lchild -\u0026gt; color = 0; h -\u0026gt; rchild -\u0026gt; color = 0; } if(x.getKey() \u0026lt; h -\u0026gt; item.getKey()){ // 向左子树插入  insertRB(h -\u0026gt; lchild, x, 0); if(getColor(h) \u0026amp;\u0026amp; getColor(h -\u0026gt; lchild) \u0026amp;\u0026amp; sw) // 两个方向相同的红链接  rotateR(h); if(getColor(h -\u0026gt; lchild) \u0026amp;\u0026amp; getColor(h -\u0026gt; lchild -\u0026gt; lchild)){ rotateR(h); h -\u0026gt; color = 0; h -\u0026gt; rchild -\u0026gt; colot = 1; } } else{ // 向右子树插入  inserRB(h -\u0026gt; rchild, x, 1); if(getColor(h) \u0026amp;\u0026amp; getColor(h -\u0026gt; rchild) \u0026amp;\u0026amp; !sw) // 左旋  rotateL(h); if(getColor(h -\u0026gt; rchild) \u0026amp;\u0026amp; getColor(h -\u0026gt; rchild -\u0026gt; rchild)){ rotateL(h); h -\u0026gt; color = 0; h -\u0026gt; lchild -\u0026gt; color = 1; } } } 红黑树的结构性定义：\n红黑树是每个节点都带有颜色的BST，颜色为红色或黑色。除了BST的基本要求外，它还必须满足以下要求：\n 节点要么是红色，要么是黑色 根节点是黑色 所有叶子节点都是黑色 每个红节点必须有两个黑色的子节点（也就是说：不能有两个连续的红链接）。 从任一节点到其每个叶子节点的所有简单路径都包含相同数目的黑节点。  ","href":"/notebook/reading_notes/algorithms_in_c/binary-tree/","title":"树"},{"content":" 学习《C算法》中排序这一部分时做的一些笔记。主要使用C++实现了书中的大部分排序算法。\n 开始之前 为了方便增加代码的灵活性，我采取了书中作者的部分方法。主要是用 type 代替了具体的数据类型，以及定义了两个用于比较的宏。通过改变下面内容， 可以很容易的进行其它数据类型的排序。\ntypedef int type; #define less(A, B) (A \u0026lt; B) #define equal(A, B) (A == B) 下面的 exchange() 函数用来交换两个元素：\nvoid exchange(type \u0026amp;a, type \u0026amp;b){type t = a; a = b; b = t;} 还有用来测试算法正确性的驱动函数，它通过读取用户输入的数字 n , 产生 n 个10000以内的随机数作为测试数据。然后调用相应的排序算法并输出排序结果。\nint main(){ int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); type *a = new type[n]; for(int i = 0;i \u0026lt; n;i++) a[i] = 10000 * (1.0 * rand() / RAND_MAX); //selectSort(a, 0, n - 1);  //insertSort1(a, 0, n - 1);  //insertSort2(a, 0, n - 1);  //bubbleSort(a, 0, n - 1);  //shakerSort(a, 0, n - 1);  //shellSort(a, 0, n - 1);  //countSort(a, 0, n - 1);  //quickSort(a, 0, n - 1);  //tri_quickSort(a, 0, n - 1);  //mergeSort(a, 0, n - 1);  //mergeTD(a, 0, n - 1);  //oddEvenSort(a, 0, n - 1);  //heapSort(a, 0, n - 1);  //radixSort(a, 0, n - 1);  for(int i = 0;i \u0026lt; n;i++) printf(\u0026#34;%-6d\u0026#34;, a[i]); printf(\u0026#34;\\n\u0026#34;); delete []a; system(\u0026#34;pause\u0026#34;); return 0; } 选择排序 步骤  首先，找出数组中最小的元素并用首位置的元素和它交换 然后， 找出数组中次小的元素并用第二个位置的元素和它交换 重复此步骤，直到整个数组排序完成。  具体实现 对于从 left 到 right - 1 的每个 i ,用 a[i], a[i + 1], \u0026hellip;, a[right] 中的最小元素进行交换。当索引 i 从左向右遍历时，其左边的元素所处的位置就是其在数组中的最终位置。 所以，当 i 到达最右端时，整个数组排序完成。\nvoid selectSort(type a[], int left, int right){ for(int i = left;i \u0026lt; right;i++){ int minp = i; // 假定未排序序列中的第一个为最小值  for(int j = i + 1;j \u0026lt;= right;j++) if(less(a[j], a[minp])) minp = j; exchange(a[i], a[minp]); } } 插入排序 步骤 对于未排序的序列，每次取其中的一个数。然后在已排序的序列中从后向前扫描， 找到合适的位置并插入。\n说明 和选择排序一样，在排序过程中，当前索引左边的元素已经有序，但这并不是他们的最终位置，如果碰到了比它们更小的元素，它们还必须后移，为较小的元素腾出位置。\nvoid insertSort1(type a[], int left, int right){ for(int i = left + 1;i \u0026lt;= right;i++) for(int j = i;j \u0026gt; left;j--) if(less(a[j], a[j - 1])) exchange(a[j], a[j - 1]); } 改进  与选择排序不同的时，插入排序的运行时间主要取决于输入中键的初始顺序。 当我们碰到的键不大于正被插入的键时，就可以停止 less() 和 exchange() 运算，因为左边的序列时已经排序好的了。特别地，当 less(a[j - 1], a[j]) 为真时，我们可以直接跳出内层循环。 不难发现，j \u0026gt; left 的测试通常是多余的（只有在插入元素时当前看到的最小元素并且到达了数组的起始处，它才为真）。一种改进方法是：让键在 a[left] 到 a[N] 中保持有序，并在 a[0] 中放入一个标记键，它至少与数组中的最小键相同。然后通过测试是否碰到了最小键来同时测试 less(a[j - 1], a[j]) 和 j \u0026gt; left 这两个条件，让内循环更小，程序更快。 对同一个元素的连续交换效率不高，如果进行两次或者更多的交换，中间变量 t 的值并没有改变。在第二次或以后的交换中，先保存再重新载入 t 的值就是浪费时间。\n 具体做法  将数组中最小值放到第一个位置，作为标记。 在内循环中，进行单个赋值，而不是连续交换。 当正被插入的元素已经就位时，终止内循环。对于每个 i ，把大于 a[i] 的排序表 a[left], ..., a[i - 1] 的所有元素整体向右移动一个位置，再把 a[i] 放入适当的位置。这样就完成了整个排序的过程。  void insertSort2(type a[], int left, int right){ for(int i = left + 1, minp = left;i \u0026lt;= right;i++){ if(less(a[i], a[left])) minp = i; exchange(a[minp], a[left]); } for(int i = left + 2;i \u0026lt;= right;i++){ int j = i; type v = a[i]; // 先保存a[i]，确保它不会被右移的元素覆盖  //把大于a[i]的所有元素整体向右移动一个位置  for(;less(v, a[j - 1]);j--) a[j] = a[j - 1]; a[j] = v; // 放入a[i]到适当的位置  } } 冒泡排序 步骤  从左到右比较相邻的两个元素，如果第一个比第二个大，就交换它们 对每一对元素重复第1步，结束时，最大的元素在排序序列的最右端。 对除最后一个以外的所有元素重复以上步骤。 重复直到没有任何一对数字需要比较。  代码如下：\nvoid bubbleSort(type a[], int left, int right){ for(int i = left;i \u0026lt;= right;i++){ for(int j = left;j \u0026lt;= right - i - 1;j++) if(less(a[j + 1], a[j])) exchange(a[j + 1], a[j]); } } 摇摆排序 冒泡排序改良版。将单向扫描数组改成从头到尾，再从尾到头的交替方式\nvoid shakerSort(type a[], int left, int right){ for(int i = left;i \u0026lt;= right;i++){ for(int j = left;j \u0026lt;= right - i - 1;j++) if(less(a[j + 1], a[j])) exchange(a[j + 1], a[j]); for(int j = right - i - 1;j \u0026gt; i;j--) if(less(a[j], a[j - 1])) exchange(a[j], a[j - 1]); } } 希尔排序 希尔排序又叫缩小增量排序，它是插入排序扩展。\n 插入排序慢， 因为它一次只交换相邻的两个元素（步长为1）。如果最小键位于数组尾部，则将它移动到正确位置需要N步。 为了让元素能够能快的到达正确的位置，改变步长。每隔h取一个元素，可以得到一些h-有序序列。然后改变步长继续操作，最终当步长为1的时候，希尔排序变成插入排序，这就保证了排序能够完成。\n 如果不使用标记，则在插入排序中，将步长由“1”换成“h”(也就是将每个“1”换成“h”)，得到的程序对序列进行h-排序。增加一个外循环来改变增量h，就可以得到最终程序。程序中选取的增量序列为：1，4，13，40，121，364，1093，3280，9841……\nvoid shellSort(type a[], int left, int right){ int h, i, j; for(h = 1;h \u0026lt; (right - left) / 3;h = h * 3 + 1); for(;h \u0026gt; 0;h /= 3){ // 调整增量  for(i = left + h;i \u0026lt;= right;i += h){ j = i; type v = a[i]; // 保存a[i]  for(;j \u0026gt;= left + h \u0026amp;\u0026amp; less(v, a[j - h]);j -= h) a[j] = a[j - h]; // 向后移动h个元素  a[j] = v; // 把a[i]放到正确位置  } } } 键索引计数排序 键索引排序把键当作索引进行排序，而不是把键当作被比较的抽象项。比如：排序一个包含N个项的文件，项的键为0~M-1之间的整数。我们可以用每个值来对键的个数进行计数，然后在第二遍扫描中使用计算出来的数将项移到正确的位置。通俗的讲：加入你们班有30个人，统计出来有5个人的绩点比你高，那么你的绩点就排在第6位。用这个方法可以得到其它人的排名，也就排好了序。对于重复值，需要特殊处理。 不过键索引基数排序局限于待排序数据的范围。\n步骤  首先，计数每个值的键的数量 然后，小计小于或等于每个值的键数。 接着，使用这些计数作为索引分拣键，比如 cnt[i] 表示小于 i 的个数，那个 a[i] 位于 aux[ant[i]] 写回原数组  代码如下：\nvoid countSort(type a[], int left, int right){ int i, j, M = 10000; // 键必须是小于M的整数  int N = right - left + 1, cnt[M]; type *aux = new type[N]; for(j = 0;j \u0026lt; M;j++) cnt[j] = 0; // 将计数初始化为0  for(i = left;i \u0026lt;= right;i++) cnt[a[i] + 1]++; //统计小于a[i]的值的出现频率  for(j = left;j \u0026lt; M;j++) cnt[j + 1] += cnt[j]; // 得到小于等于计数器对应计数值键的数量,将频率转换为索引  for(i = left;i \u0026lt;= right;i++) aux[cnt[a[i]]++] = a[i]; // 将键分布到辅助数组中  for(i = left;i \u0026lt;= right;i++) a[i] = aux[i]; // 回写  delete []aux; } 快速排序 快速排序是一种分治方法。它的工作方式是：将待排序的序列划分为两组，然后独立排序各个部分划分的准确位置取决于输入中元素的初始顺序。 方法的关键在于划分过程，它将数组重排，使下面3个条件成立：\n 对于某一个 i 值，元素 a[i] 处于数组中的最终位置; a[i] 左边的元素都不大于 a[i] ; a[i] 右边的元素都不小于 a[i] ;  划分方法  首先，任意选定 a[right] 作为划分元素（它将处于在数组中的最终位置）。 从左向右扫描，直到发现一个大于划分元素的元素；同时从右向左扫描，直到发现一个小于划分元素的元素，然后交换这两个元素。 按这种方式继续划分。确保左指针左边的元素都小于划分元素，右指针右边的元素都大于划分元素。 当左右指针相遇或者相互经过时，交换 a[right] 和右半部分最左边的元素（由左指针指向的元素），划分结束。  int partition(type a[], int left, int right){ int i = left - 1, j = right; type v = a[right]; // a[right]为划分元素  for(;;){ for(;less(a[++i], v);); for(;less(v, a[--j]);) if(j == left) break; // 避免划分元素为序列中的最小元素的情况发生  if(j \u0026lt;= i) break; exchange(a[i], a[j]); } exchange(a[i], a[right]); //交换a[right]和a[i]  return i; } 如果数组中只有一个元素，不需要进行任何运算。否则，调用 partition() 函数处理这个数组，它将 a[i] 放到最终位置 (left \u0026lt; i \u0026lt;= right) 并且重排其它元素，让递归调用能够正确完成整个排序过程。\nvoid quickSort(type a[], int left, int right){ if(right \u0026lt;= left) return; int i = partition(a, left, right); quickSort(a, left, i - 1); quickSort(a, i + 1, right); } 改进（三元素中值划分） 三元素中值划分使用一个更有可能在中间位置出现的元素作为划分元素。它从序列中选出三个元素的样本，然后用这三个元素的中值作为划分元素。分别从数组中的左、中、右选取一个元素。接着排序这三个元素，然后用 a[right - left] 交换中间那一个，再对 a[left + 1],...,a[right - 2] 运行划分算法。\n应对大量重复键的情况\u0026mdash;三路划分  当排序序列中的重复键较多的时候，快速排序的低下性能让人难以接受。 可以把序列分为三部分，分别是：小于划分元素的部分、等于划分元素的部分、大于划分元素的部分。 修改标准划分方案如下：将在左边部分碰到的和划分元素相等的键放到序列的左端，将右边部分碰到的和划分元素相等的键放到序列右端。 然后，当指针交叉而且相等键的位置已知的时候，将所有与划分元素相等的键交换到位  一点说明 程序将数组划分为三部分：小于划分元素的部分（ a[left],..., a[j] ）, 等于划分元素的部分（ a[j + 1],..., a[i - 1] ）,大于划分元素的部分( a[i],...,a[right] )。示意图如下：\nvoid tri_quickSort(type a[], int left, int right){ if(right \u0026lt;= left) return; type v = a[right]; // 划分元素  int i = left, j = right - 1, p = left, q = right - 1; for(;;){ for(;less(a[i], v);i++); for(;less(v, a[j]);j--) if(j == left) break; // 避免划分元素为序列中的最小元素的情况发生  if(j \u0026lt;= i) break; exchange(a[i], a[j]); if(equal(a[i], v)){p++;exchange(a[i], a[p]);} if(equal(a[j], v)){q--;exchange(a[j], a[q]);} } exchange(a[i], a[right]); i--;j++; int k; for(k = left;k \u0026lt; p;k++,i--) exchange(a[k], a[i]); for(k = right - 1;k \u0026gt; q;k--, j++) exchange(a[k], a[j]); tri_quickSort(a, left, i); tri_quickSort(a, j, right); } 奇偶排序 步骤  选取所有为奇数列（下标为1,3,5\u0026hellip;)的元素与其右侧元素比较，将小的放在前面 选取所有为偶数列（下标为2,4,6\u0026hellip;)的元素与其右侧元素比较，将小的放在前面 重复1和2直到所有序列有序为止。  实现 void oddEvenSort(type a[], int left, int right){ int i; bool oddSorted = false, evenSorted = false; while(!oddSorted || !evenSorted){ oddSorted = true; evenSorted = true; for(i = left;i \u0026lt; right;i += 2){ if(less(a[i + 1], a[i])){ exchange(a[i + 1], a[i]); evenSorted = false; } } for(i = left + 1; i \u0026lt; right;i += 2){ if(less(a[i + 1], a[i])){ exchange(a[i + 1], a[i]); oddSorted = false; } } } } 归并排序 归并 数组 a 的前半部分（ a[left],...,a[m] ）有序，后半部分（ a[m + 1],...,a[right] ）有序。归并这两部分，使整个数组有序。一般需要一个辅助数组 aux ,先把结果存到 aux 中，然后把排序结果从 aux 写回到 a 中。 一种常用的归并方法：\nvoid merge0(type a[], int left, int m, int right){ int len = right - left + 1; type *aux = new type[len]; int i = left, j = m + 1, k = 0; while(i \u0026lt;= m \u0026amp;\u0026amp; j \u0026lt;= right) aux[k++] = less(a[i], a[j]) ? a[i++] : a[j++]; while(i \u0026lt;= m) aux[k++] = a[i++]; while(j \u0026lt;= right) aux[k++] = a[j++]; for(i = left, k = 0;i \u0026lt;= right;i++, k++) a[i] = aux[k]; delete []aux; } 另一种归并方法： 将前半部分复制到 aux 中，然后将后半部分（ a[m + 1],...,a[right] ）逆序复制到 aux 中,使两个部分最大元素背靠背位于 aux 中间，形成双调序列。这样，两个部分中的最大元素分别成为一个标记。\nvoid merge1(type a[], int left, int m, int right){ int len = right - left + 1; type *aux = new type[len]; int i, j , k = 0; for(i = left;i \u0026lt;= m;i++) aux[k++] = a[i]; // 复制前半部分到aux  for(j = right;j \u0026gt;= m + 1;j--) aux[k++] = a[j]; // 逆序复制后半部分到aux  i = 0;j = len - 1; for(k = left;k \u0026lt;= right;k++){ if(less(aux[i], aux[j])) a[k] = aux[i++]; // 归并，最大元素为各自的标记  else a[k] = aux[j--]; } delete []aux; } 排序 将数组分为两部分： a[left],..., a[m] 和 a[m + 1], ..., a[right] 。然后对这两个数组进行独立排序（通过递归调用），然后将这两个有序序列归并到最终的序列。\nvoid mergeSort(type a[], int left, int right){ int m = (left + right) / 2; if(right \u0026lt;= left) return; mergeSort(a, left, m); mergeSort(a, m + 1, right); merge1(a, left, m, right); // 归并 } 巴切奇偶归并排序 首先要讲两个函数： 完全混洗(shuffle):将数组 a[left],...,a[right] 分为两半，前一半进入结果中的偶数编号位置，后一半进入结果中的奇数编号位置。 逆完全混洗(unshuffle):偶数编号位置的元素进入结果的前一半，奇数编号位置的元素进入结果的后一半。 这两个函数仅对带有偶数个元素的子数组使用。\nvoid shuffle(type a[], int left, int right){ int i, j, m = (left + right) / 2, len = right - left + 1; type *aux = new type[len]; for(i = left, j = 0;i \u0026lt;= m;i++, j += 2){ aux[j] = a[i]; aux[j + 1] = a[m + 1 + i - left]; } for(i = left, j = 0;i \u0026lt;= right;i++, j++) a[i] = aux[j]; delete []aux; } void unshuffle(type a[], int left, int right){ int i, j, m = (left + right) / 2, len = right - left + 1; int ma = len / 2; type *aux = new type[len]; for(i = left, j = 0;i \u0026lt;= right;i += 2, j++){ aux[j] = a[i]; aux[ma + j] = a[i + 1]; } for(i = left, j = 0;i \u0026lt;= right;i++, j++) a[i] = aux[j]; delete []aux; } 巴切的奇偶归并网络 这个网络输入两个已排好序的序列，对这两个序列进行归并排序。首先对这两个序列进行逆混洗，然后分别归并前后部分，接着再混洗，最后进行一次(1,2)、(3,4)\u0026hellip;这些相邻元素的比较交换得到排序结果。这里的代码只适合元素个数为 2^n 的序列。\nvoid mergeTD(type a[], int left, int right){ int i, m = (left + right) / 2; quickSort(a, left, m); quickSort(a, m + 1, right); if(left + 1 == right) if(less(a[right], a[left])) exchange(a[left], a[right]); if(left + 2 \u0026gt; right) return; // 不多于两个元素  unshuffle(a, left, right); // 逆混洗  mergeTD(a, left, m); mergeTD(a, m + 1, right); shuffle(a, left, right); // 混洗  for(i = left + 1;i \u0026lt; right;i += 2) //比较交换  if(less(a[i + 1], a[i])) exchange(a[i], a[i + 1]); } 堆排序 堆 大顶堆——堆中不存在大于根键的结点。与之对应的由小顶堆。这里使用大顶堆。 如果用数组来保存堆，如果下标从 0 开始，很容易知道位置 i 处结点的父亲位于 (i - 1) / 2 , 反之位置 i 处结点的孩子位于 (2i + 1) 和**(2i + 2)** 处。 堆中的第 i 个元素大于等于第 (2i + 1) 个元素和 (2i + 2) 个元素。 有关堆的很多算法都是首先对堆做一个简单的修改，这可能违反堆的条件，然后遍历堆，同时修正堆，确保整个堆满足堆的条件。 修正堆的情况有两种，一种是在堆底部添加新节点，然后需要向上遍历调整堆；另一种是用一个新节点替换掉根节点，然后需要向下遍历调整堆。 如果是由于节点的键变得大于它的父亲而违反了堆的性质，则可以通过交换该节点和它的父亲的位置。交换后，节点大于它的两个孩子（ 一个是原来的父亲，一个是原来父亲的另一个孩子），但它仍有可能大于现在的父亲，因此需要继续调整，直到遇到一个真正比它大的父节点或者到达根的位置才结束。 如果是由于节点的键变得小于它的一个或者两个孩子而违反了堆的性质。则可以通过交换此节点和它的大孩子来进行修改。这可能导致孩子 的违规，然后就按照这种方式继续调整，直到到达不小于它的所有孩子的节点或者叶子节点才结束。\n自底向上堆化 向上遍历堆，只要 a[k/2] \u0026lt; a[k] 就交换 k 处和 k/2 处的节点的位置。继续此过程，或者直到到达根节点为止\nvoid fixUp(type a[], int left, int right){ int s = right; int f = s / 2 - 1; while(f \u0026gt;= left){ if(a[f] \u0026lt; a[s]) exchange(a[f], a[s]); s = f; f = s / 2 - 1; } } 自顶向下堆化 向下遍历堆， 交换位置 k 处的节点和它孩子中较大的那个节点（如果有需要的话），当位置 k 处的节点不小于它的孩子或者到达了底端就停止。 需要注意的是：如果 N 为偶数，且 k = N/2 时， 它只有一个孩子节点。\nvoid fixDown(type a[], int left, int right){ int f = left; int s = 2 * f + 1; //左孩子  while(s \u0026lt;= right){ // 没有到达底端  if(s + 1 \u0026lt;= right \u0026amp;\u0026amp; less(a[s], a[s + 1])) s++; // 找大孩子  if(!less(a[f], a[s])) break; // 已经满足堆的条件，跳出  exchange(a[s], a[f]); // 交换  f = s; // 继续  s = 2 * f + 1; } } 排序方法 移出堆顶元素，然后调整堆。重复直到堆中只有一个元素。\nvoid heapSort(type a[], int left, int right){ int N = right - left + 1; int i = N / 2 - 1; // 初始化i为最后一个父节点，从最后一个父节点开始调整，因为所有的叶子节点都是堆了  for(;i \u0026gt;= 0;i--) fixDown(a, i, N); // 建大顶堆  for(i = right;i \u0026gt;= left;){ exchange(a[left], a[i]); // 把根节点交换到最后  fixDown(a, left, --i); // 调整  } } 基数排序  引入：当我们在电话簿中查找某个人的电话时，我们通常只输入前几个字母，然后就能得到电话号码所在的页。为了在排序算法中取得相似效率，可以将比较键的抽象转化为另一种抽象。将这些键分解为一系列定长片段或字节。然后每次处理其中的一个片段，这种排序方法叫做基数排序。基数排序算法把键当作以R为基数的数值系统表示的数，R可取不同的值，分别处理这些数中的单个数字。 基数排序有两种：一种时从左到右按顺序检查键中的位，称为最高位基数排序。另一种采用从右到左的顺序，称为最低为基数排序。\n 以16进制为例，可以通过右移运算取得int(这里int为32 位)型数组 a[i] 的各个字节的数字:最低位(a[i] \u0026raquo; 0) \u0026amp; 0xff 、次低位(a[i] \u0026raquo; 8) \u0026amp; 0xff 以及(a[i] \u0026raquo; 16) \u0026amp; 0xff 和(a[i] \u0026raquo; 24) \u0026amp; 0xff。我们需要256(0xff - 0x00 + 1 = 256)个桶。稍微修改键索引计数的程序就得到了基数排序的程序。\nvoid radix(int b, type a[], int left, int right){ int i, j, M = 256; int N = right - left + 1, cnt[M]; type *aux = new type[N]; for(j = 0;j \u0026lt; M;j++) cnt[j] = 0; // 初始化计数为0  for(i = left;i \u0026lt;= right;i++) cnt[((a[i] \u0026gt;\u0026gt; b * 8) \u0026amp; 0xff) + 1]++; // 统计出现频率  for(j = left;j \u0026lt; M;j++) cnt[j + 1] += cnt[j]; //得到小于等于计数器对应计数值键的数量,将频率转换为索引  for(i = left;i \u0026lt;= right;i++) aux[cnt[(a[i] \u0026gt;\u0026gt; b * 8) \u0026amp; 0xff]++] = a[i]; //将键分不到辅助数组中  for(i = left;i \u0026lt;= right;i++) a[i] = aux[i]; // 回写  delete []aux; } void radixSort(type a[], int left, int right){ radix(0, a, left, right); radix(1, a, left, right); radix(2, a, left, right); radix(3, a, left, right); } ","href":"/notebook/reading_notes/algorithms_in_c/sorting/","title":"各种排序算法"},{"content":"","href":"/tags/git/","title":"Git"},{"content":"开始 文件的三种状态  已提交(committed) 数据已经安全的保存在了本地的数据库中 已修改(modified) 修改了文件，但还没保存到数据库中 已暂存(staged) 对一个已修改的文件的当前版本做了标记，使之包含在下次的提交快照中。  项目的三个工作区域  Git仓库目录 保存项目的元数据和对象数据库 工作目录 对项目的某个版本独立提取出来的内容 暂存区域 保存下次将要提交的文件列表信息，是一个文件  基本的Git工作流程  在工作目录中修改文件 暂存文件，将文件的快照放入暂存区域 提交更新，将暂存区域中的文件的快照永久保存到Git仓库目录  初次运行前  设置用户名与邮箱地址 --global 表示使用 global config file , 用于所有配置。如果是对于需要使用不同用户名和邮箱的特定项目就不需要这个选项。  $ git config --global user.name username $ git config --global user.email example@xxx.com  设置文本编辑器  $ git config --global core.editor code  查看配置信息  $ git config --list  获取帮助  有三种方式可以获取帮助，windows中默认支支持前两种\n$ git help \u0026lt;verb\u0026gt; $ git \u0026lt;verb\u0026gt; --help $ man git-\u0026lt;verb\u0026gt; 基础 获取Git仓库  在现有目录中初始化Git仓库  $ git init 如果不是在一个空文件夹下执行这个命令，那么我们可以多做一点事情：开始跟踪指定文件并提交。使用 git add 命令来实现对指定文件的跟踪，然后执行 git commit 提交。\n 克隆现有仓库  $ git clone [url] 这个命令将会克隆Git仓库服务器上几乎所有的数据。\n记录每次更新到仓库  检查文件当前状态  $ git status  跟踪新文件  $ git add (files) 跟踪后的文件会处于暂存状态。可以运行 git status 命令查看。 如果修改了已跟踪的文件，它的状态会变为已修改，要暂存这次更新，就需要运行 git add 命令 。注意： git add 是一个多功能命令，可以理解为“添加内容到下一次提交中”。\n 查看已暂存或未暂存的修改  $ git diff 注意：git diff 只显示尚未暂存的改动，可以加上 --cache 选项来查看已经暂存的改动。可以使用这个命令来分析文件的差异。\n 提交更新  $ git commit 这会启动默认的文本编辑器以便输入本次提交的说明。可以使用 -m 选项来把提交信息与命令放在同一行。如：\n$ git commit -m \u0026quot;add readme.md\u0026quot;   跳过使用暂存区域 在提交的时候，给 git commit 命令加上 -a 选项，Git就会自动把所有已跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。\n  移除文件 要从Git中移除某个文件，就必须要从已跟踪文件清单中移除（准确来说，是暂存区域），然后提交。使用的命令是 git rm ,它还会从工作目录中删除指定文件，这样就不会被跟踪了。 如果想把文件从Git仓库中删除（亦从暂存区域删除），加上 --cache 选项即可。\n  移动文件 使用命令 git mv 即可。它亦可以用来重命名文件：\n  $ git mv old_file new_file  查看提交历史  $ git log 这默认会按文件提交时间列出所有的更新，并且最新的跟新在最上面。一个常用的选项是 -p ,用来显示每次提交内容的差异。\n  撤销操作\n重新提交，下面的命令会用第二次提交取代第一次提交的结果：\n  $ git commit --amend 取消暂存的文件：\n$ git reset \u0026lt;file\u0026gt; 撤销对文件的修改：\n$ git checkout --\u0026lt;file\u0026gt; 远程仓库的使用  查看远程仓库  $ git remote  添加远程仓库  $ git remote add \u0026lt;shortname\u0026gt; \u0026lt;url\u0026gt; 这会添加一个远程仓库并指定使用间写 shortname 来引用它。\n 从远程仓库抓取  $ git fetch [remote-name]  推送到远程仓库  $ git push [remote-name] [branch-name]  查看远程仓库  $ git remote show [remote-name]  重命名远程仓库  $ git remote rename [old-name] [new-name]  删除远程仓库  $ git remote rm [remote-name] ","href":"/posts/git/git_at_first_sight/","title":"Git初探"},{"content":"","href":"/categories/tools/","title":"Tools"},{"content":"问题由来  这是一个很经典的问题了，大概就是说： 已知n个人（以编号1，2，3\u0026hellip;n分别表示）围成一个圆圈。 从第一个人开始报数，数到m的那个人出列自杀；他的下一个人又从1开始报数，数到m的那个人又出列自杀； 依此规律重复下去，找出最后剩下来的那个人。更一般的，找出自杀顺序。\n 用C++链表实现 为了以一种圆圈的形式排列人群， 我们可以构建一个循环链表，每个人和他左边的那个人之间都有一个链接。用整数 i 代表循环中的第 i 个人。从 1 开始，每次生成一个结点插入链表尾部……直到最后一个人 n 进入链表， 然后让 n 的下一个指向 1，这样就构成了一个循环链表。设置一个变量 t 记录当前所报数字。从链表头部开始（ t = 1）用迭代器 it 遍历链表，当 t == m 时，it 刚好指向要该自杀的那个人，于是输出并从链表中删除。当 it 到达链表尾部时，让 it 指向链表头，这样就形成了一个环。重复操作，直到剩下最后一个人。\n#include\u0026lt;cstdlib\u0026gt;#include\u0026lt;cstdio\u0026gt;#include\u0026lt;list\u0026gt;using namespace std; int main(){ int n, m; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n, \u0026amp;m); list\u0026lt;int\u0026gt; ring; for(int i = 1;i \u0026lt;= n;i++) ring.push_back(i); int t = 1; //计数  for(list\u0026lt;int\u0026gt;::iterator it = ring.begin();ring.size() != 1;){ if(t++ == m){ printf(\u0026#34;%-3d\u0026#34;, *it); it = ring.erase(it); // 自杀  t = 1; } else it++; // 指向下一个  if(it == ring.end()) it = ring.begin(); } printf(\u0026#34;\\nalive: %d\\n\u0026#34;, *ring.begin()); system(\u0026#34;pause\u0026#34;); return 0; } 用数组模拟链表实现 我们可以用数组的索引来实现链表，而不是用指针。方法就是：用数组元素 item[i] 存储编号为 i 的人，用数组元素 next[i] 存储 i 的下一个在数组中的位置。那么有 item[i] == i + 1。注意最开始的时候 next[n - 1] = 0 。用位置变量 pos 跟踪元素，首先让 pos = n - 1 ,这样 pos 才是指向第一个人的，通过 m - 1 次 pos = next[pos] 得到自杀的那个人的前一个人位置，然后用 next[pos] = next[next[pos]] 删除第 m 个人。这种方式还是很浪费空间的。\n#include\u0026lt;cstdio\u0026gt;#include\u0026lt;cstdlib\u0026gt;#include\u0026lt;vector\u0026gt;using namespace std; int main(){ int n, m; scanf(\u0026#34;%d%d\u0026#34;, \u0026amp;n, \u0026amp;m); vector\u0026lt;int\u0026gt; item, next; for(int i = 1;i \u0026lt;= n;i++){ item.push_back(i); next.push_back(i % n); // 存放i的下一个的数组下标  } int t, pos = n - 1; // 使pos指向链表首元素  for(int i = 0;i \u0026lt; n - 1;i++){ for(t = 0;t \u0026lt; m - 1;t++) pos = next[pos]; printf(\u0026#34;%-3d\u0026#34;, item[next[pos]]); next[pos] = next[next[pos]]; // 自杀  } printf(\u0026#34;\\nalive: %d\\n\u0026#34;, item[pos]); system(\u0026#34;pause\u0026#34;); return 0; } ","href":"/notebook/reading_notes/algorithms_in_c/josepus-problem/","title":"约瑟夫问题"},{"content":"筛法原理  给出要筛数值的范围maxn，找出maxn以内所有的素数。先用2去筛，即把2留下，把2的倍数剔除掉；再用下一个素数，也就是3筛，把3留下，把3的倍数剔除掉；接下去用下一个素数5筛，把5留下，把5的倍数剔除掉；不断重复下去\u0026hellip;\u0026hellip;。\n 算法 可以通过维护一个标记数组 **a[]**来进行判断。\n 若 i 为素数，则 a[i] = 1 ，否则 a[i] = 0。 首先，将所有的 a[i] 都设置为 1，表示已知的都是素数。然后将所有以已知素数的倍数为索引的数组元素设为 0。 如果将所有较小素数的倍数都设置为 0 之后，a[i] 仍为 1，则 i 为素数。  #include\u0026lt;cstdio\u0026gt;#include\u0026lt;cstdlib\u0026gt;const int maxn = 10000; int main(){ int i, j, t, a[maxn]; for(i = 2;i \u0026lt; maxn;i++) a[i] = 1; for(i = 2;i \u0026lt; maxn;i++){ if(a[i] == 1){ t = maxn / i; for(j = i;j \u0026lt; t;j++) a[i * j] = 0; } } for(i = 2;i \u0026lt; maxn;i++){ if(a[i] == 1) printf(\u0026#34;%4d \u0026#34;, i); } system(\u0026#34;pause\u0026#34;); return 0; } ","href":"/notebook/reading_notes/algorithms_in_c/seive-of-eratosthenes/","title":"厄拉多筛法求素数"},{"content":"记录一下最近学习的并查集。\n问题引入 假定有一个整数对序列，其中每个整数代表某种类型的一个对象，而且将 p-q 解释为“p与q连通”。关系是可传递的，如果 p-q 和 q-r ，则有 p-r 。我们的目标是编写一段程序，从集合中过滤额外连接对。当程序输入一个连接对 p-q ，若之前的连接对不能通过传递关系推导出 p-q ，则输出 p-q ， 否则忽略 p-q ，读取下一个整数对。\n过程示例:\n3-4 3-4\n4-0 4-0\n3-0\n4-1 4-1\n5-6 5-6\n0-5 0-5\n3-5\n2-9 2-9\n我们的问题是设计一个程序，它通过已有的信息，来判断新的对象是否连通。整数可能代表一个大型网络中的计算机，整数对可能代表网络中的连通情况。同样，整数也可以代表一个电网中的触电， 整数对就是电线……这些问题的规模可能都非常巨大，算法的好坏直接决定了开销的大小。\n并和查 并查集主要有以下两个操作:\n 并： 合并两个集合 查： 判断两个元素是否属于同一个集合  解决方案 我们可以通过查找（union）和并集（union）运算来解决连通性问题。每当从输入读取一个新的 p-q 对后，分别进行 p 和 q 的查找， 如果它们位于同一个集合中，就直接分析下一个 p-q 对。否则，就进行并集操作并输出。 最可能想到的是依次保存每一个 p-q 对，然后进行遍历，判断它们是否连通。可问题的规模可能很大，这时候这个方法就捉襟见肘了。我们其实并不用保存所有的 p-q 对，使用一个整数数组来保存实现find和union操作的必备信息即可。\n快速查找（quick-find）简单算法  假设A和B是朋友，B和C是朋友，A和B互不认识，但他们通过B这个共同的朋友联系在了一块儿。C还会有朋友D……,所有能通过某种朋友关系建立联系的朋友就构成了一个朋友圈。设想每个朋友圈都有一个圈主，圈主唯一的确定了这个圈子。为了简化问题，每当有一个新的人加入圈子，他就成为了这个圈子的圈主。 于是我们可以用一个数组 A[] 来进行存储。如果 i 代表一个人，那么 A[i] 就是他的圈主。现在我们来判断 p 和 q 是不是位于同一个圈子, 解决最开始提出的问题。\n 使用一个整数数组 A[] , 它具有如下性质：当且仅当 A[p] = A[q] 时，p与q连通。首先用 i 初始化 A[i] , 为了对 p 和 q 实现并集的运算，遍历数组，将所有和 A[p] 值相同项的替换为 A[q] 的值。\n#include\u0026lt;cstdio\u0026gt;const int N = 10000; int main(){ int p, q, A[N]; for(int i = 0;i \u0026lt; N;i++) A[i] = i; while(scanf(\u0026#34;%d-%d\u0026#34;, \u0026amp;p, \u0026amp;q) == 2){ if(A[p] == A[q]) continue; for(int i = 0, t = A[p];i \u0026lt; N;i++){ if(A[i] == t) A[i] = A[q]; } printf(\u0026#34;%d-%d\\n\u0026#34;, p, q); } } 可以用树来表示quick-find的过程。如下图中左半部分所示，每输入一个 p-q 对， A[q] 就成为父节点。下图的右半部分描述的是quick-union的过程， 每次只有一个值发生了改变。也就是 q 成为了 p 的父节点。 快速并集（quick-union）算法  前面的快速查找算法能够正确解决问题，当面对百万级的输入，效率就不行了。下面是quick-find的互补方法——quick-union。 可以用树来描述描述p和q是不是位于同一个集合，如果p和q有相同的根节点，那么他们位于同一个集合。如果p和q不在同一个集合中， 为了形成并集，使p和q拥有相同的根节点即可。 和quick_find的不同在于：\n  这里 A[i] 的值指向的是它的父亲在数组中的下标（比如 A[1] = 2 ， 1的父亲在数组中的下标为2）。 根节点的值指向它本身，即：若 i 为根节点，则 A[i] = i ;根节点总是存在的。 通过分别通过 i 和 j 递归查找 p 和 q 的根节点。  用下面的代码替换quick-find中的 while 循环体\nint i , j; for(i = p;i != A[i];i = A[i]); for(j = q;j != A[j];j = A[j]); if(i == j) continue; //p和q的根相同，说明p和q连通 A[i] = j; // 进行并集操作 printf(\u0026#34;%d-%d\\n\u0026#34;, p, q); 加权快速并集（weighted-quick-union）算法  quick-union是对quick-find的一种改进，但仍然有缺陷。每次进行并集操作就相当于把一棵树的树根链接到另一棵树的树根上去。 而找到根节点所化的时间和当前结点到根节点的距离有关，距离越短，for循环执行的次数就越少 如果能够直接把较小的树的根直接连接到较大的树的根上，就能缩短时间。 于是我们可以设置辅助数组 S[] 来跟踪每个结点子树中结点的数量，然后总是把较小的树的根连接到较大的树的根上去。\n #include\u0026lt;cstdio\u0026gt;const int N = 10000; int main(){ int p, q, A[N], S[N]; for(int i = 0;i \u0026lt; N;i++){A[i] = i;S[N] = 1;} while(scanf(\u0026#34;%d-%d\u0026#34;, \u0026amp;p, \u0026amp;q) == 2){ int i , j; for(i = p;i != A[i];i = A[i]); for(j = q;j != A[j];j = A[j]); if(i == j) continue; //p和q的根相同，说明p和q连通  A[i] = j; // 进行并集操作  if(S[i] \u0026lt; S[j]){A[i] = j;S[j] += S[i];} // 寻找大树  else {A[j] = i;S[i] += S[j];} printf(\u0026#34;%d-%d\\n\u0026#34;, p, q); } } 路径压缩  如果能够在并集操作中，使每个结点都能直接指向树根，那么将大大节省开销。最终的结果就是几乎把整棵树压平，接近于理想情况。 可以对上面的算法做一个简单的改进——通过某种方法让 A[i] 存储的不再是它父亲在数组中的下标，而是越过父节点更靠近根节点的结点在数组中的下标。比如父结点的父节点。\n 对代码进行一下调整：\nfor(i = p;i != A[i];i = A[i]) A[i] = A[A[i]]; for(j = q;j != A[j];j = A[j]) A[i] = A[A[i]]; ","href":"/notebook/reading_notes/algorithms_in_c/union-find/","title":"并查集"},{"content":"","href":"/tags/hugo/","title":"Hugo"},{"content":" 本文主要是想记录一下自己在win10下使用hugo和github pages搭建博客的过程，备忘。 为什么我要用hugo而不用hexo呢,最主要是因为hugo构建网站的速度非常快，直接秒好。\n 环境要求  Go语言 Git hugo  搭建环境 Go语言  下载 如何安装 添加环境变量 验证安装  win+R打开命令行窗口，输入：\ngo version 如果输出当前安装的go语言的版本号，即安装成功。\nGit  how to install Git中文教程  hugo  如何安装 添加hugo环境变量 验证安装  hugo version 现在我们需要的环境都创建好了，接下来就可以在建立自己的网站了。\n创建网站 首先在本地建立一个存放自己网站的文件夹，然后通过命令行进入刚才创建的文件夹，继续操作。\n建立新网站mysite # mysite是将要创建的网站名 hugo new site mysite 添加theme # 进入mysite cd mysite  默认情况下hugo生成的网站是没有任何theme的，所有我们需要自己去下载theme。 如果网速好的话，我们可以一次性下载所有的themes到themes文件夹下：\n git clone --depth 1 --recursive https://github.com/gohugoio/hugoThemes.git themes 也可以只下载一个theme\ncd themes # 没有的话就创建一个 使用git clone命令下载单个主题，用法如下：\ngit clone URL_TO_THEME # URL_TO_THEME是theme的地址  比如我要下载hyde这个theme:\ngit clone https://github.com/spf13/hyde 添加内容并发布网站 在网站的根目录下执行：\nhugo new posts/my-first-article.md 这样我们就成功的建立了my-first-article这篇文章，它位于网站根目录下的posts文件夹里。随便写你想写的内容，然后启动hugo服务器，下面-D表示当前内容为草稿，-t hyde表示采用hyde作为网站的theme：\nhugo setver -D -t hyde 现在在浏览器输入：http://localhost:1313/。然后你就能看到你刚才写的网站了。现在你可以继续修改my-first-article的内容，浏览器会实时更新的的改动，不过这可能并不会被你发觉，因为hugo的速度实在是太快了，哈哈哈。\n正式发布\u0026ndash;托管到github 本地网站建好了，不过现在只有你自己能够看到。如果你想让别人也看到你的网站的话，我猜你肯定不想用自己的电脑做服务器吧。这个时候你可以利用github pages来都达到你的目的。\n 如果你没有github账号的话，那么首先注册github。千万别忘了你的用户名，后面会用上。假设我的用户名是sarkar。 登录github, 建立两个repository。一个是工程仓库（假定为mysite), 用来存放hugo内容和其他的资源文件。另一个就是sarkar.github.io, 这就是用来放你的网站内容的地方了(记得把这里的和以后的sarkar都换成你自己的github用户名)。 回到网站的根目录下，也就是mysite目录，在这之前打开你准备发布的.md文件，将draft: false改为draft: false。这样就可以正式的发布内容了，不再是草稿。然后执行命令：  hugo -t hyde --baseUrl=\u0026#34;https://sarkar.github.io\u0026#34; 记得把username换成你自己的github用户名。\n 命令会在网站网站的根目录下生成一个public文件夹，里面包含了hugo生成的所有静态页面。接下来要做的就是把这下静态页面给push到github上去。\u0026gt;\n 首先需要进到pulib目录, 然后执行以下命令：\ncd public # 进入public目录 git init git remote add origin https://github.com/sarkar/sarkar.github.io.git git add -A git commit -m \u0026#34;first commit\u0026#34; git push -u origin master 不出意外的话，应该会弹出一个github的登陆窗口或者类似的东西。然后静待本地文件全部push完成。到浏览器输入sarkar.github.io应该就能访问到了刚才建立的网站了。\n","href":"/posts/getting_started/building-your-blogsite-with-github-pages-and-hugo/","title":"用hugo和github pages搭建个人博客"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/series/","title":"Series"}]