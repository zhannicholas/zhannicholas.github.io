<!doctype html><html lang=zh-cn><head><meta http-equiv=content-type content="text/html" charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon type=image/png sizes=96x96 href=https://zhannicholas.github.io/favicon/favicon-96x96.png><link rel=icon type=image/png sizes=32x32 href=https://zhannicholas.github.io/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://zhannicholas.github.io/favicon/favicon-16x16.png><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><title itemprop=name>Design Data Intensive Applications |</title><meta name=description content><meta property="og:title" content="Design Data Intensive Applications | "><meta name=twitter:title content="Design Data Intensive Applications | "><meta itemprop=name content="Design Data Intensive Applications | "><meta name=application-name content="Design Data Intensive Applications | "><meta property="og:site_name" content><meta property="og:type" content="website"><meta property="og:title" content><meta property="og:description" content><meta property="og:site_name" content><meta property="og:url" content="https://zhannicholas.github.io/pages/design-data-intensive-applications/"><meta property="og:locale" content="en"><meta property="og:image" content="/"><meta property="og:image:secure_url" content="https://zhannicholas.github.io"><meta property="og:type" content="website"><script>localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=stylesheet href="/css/style.min.70a967cc78af9ec5b1e1b5c3552172a911554a27d8e5bdf52000296bdd439d2a.css" integrity="sha256-cKlnzHivnsWx4bXDVSFyqRFVSifY5b31IAApa91DnSo="></head><body class="bg-zinc-100 dark:bg-gray-800"><div class="top-0 z-50 w-full text-gray-200 bg-gray-900 border-2 border-gray-900 md:sticky border-b-stone-200/10"><div x-data="{ open: false }" class="flex flex-col max-w-full px-4 mx-auto md:items-center md:justify-between md:flex-row md:px-6 lg:px-8"><div class="flex flex-row items-center justify-between p-4"><a href=https://zhannicholas.github.io/ class="flex text-gray-100 transition duration-1000 ease-in-out group"><img src=https://zhannicholas.github.io/images/site-logo.svg class="transition-opacity h-9 w-9 group-hover:opacity-50 group-focus:opacity-70" alt=" Logo"><div class="mt-1 text-xl font-black tracking-tight text-gray-100 uppercase transition-colors group-hover:text-gray-400/60"></div></a><button class="rounded-lg md:hidden focus:outline-none focus:shadow-outline" @click="open = !open" role=navigation aria-expanded=false aria-label=Main aria-controls=menuItems><svg fill="currentcolor" viewBox="0 0 20 20" class="w-6 h-6"><path x-show="!open" fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm6 5a1 1 0 011-1h6a1 1 0 110 2h-6a1 1 0 01-1-1z" clip-rule="evenodd"/><path x-show="open" fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414.0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button></div><nav :class="{'flex': open, 'hidden': !open}" class="flex-col flex-grow hidden pb-4 md:pb-0 md:flex md:justify-end md:flex-row"><div @click.away="open = false" class=relative x-data="{ open: false }"><button @click="open = !open" class="flex flex-row items-center w-full px-4 py-2 mt-2 text-sm font-semibold text-left bg-transparent rounded-lg md:w-auto md:inline md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-indigo-600 focus:bg-indigo-600 focus:outline-none focus:shadow-outline">
<span>The Garden</span><svg fill="currentcolor" viewBox="0 0 20 20" :class="{'rotate-180': open, 'rotate-0': !open}" class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414.0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414.0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button><div x-show=open x-transition:enter="transition ease-out duration-100" x-transition:enter-start="transform opacity-0 scale-95" x-transition:enter-end="transform opacity-100 scale-100" x-transition:leave="transition ease-in duration-75" x-transition:leave-start="transform opacity-100 scale-100" x-transition:leave-end="transform opacity-0 scale-95" class="absolute right-0 z-30 w-full mt-2 origin-top-right rounded-md shadow-lg md:w-48"><div class="px-2 py-2 text-indigo-900 bg-white rounded-md shadow"><a class="block px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg md:mt-0 hover:text-white focus:text-white hover:bg-indigo-600 focus:bg-indigo-600 focus:outline-none focus:shadow-outline" href=https://zhannicholas.github.io/posts>Posts</a>
<a class="block px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg md:mt-0 hover:text-white focus:text-white hover:bg-indigo-600 focus:bg-indigo-600 focus:outline-none focus:shadow-outline" href=https://zhannicholas.github.io/pages>Notes</a>
<a class="block px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg md:mt-0 hover:text-white focus:text-white hover:bg-indigo-600 focus:bg-indigo-700 focus:outline-none focus:shadow-outline" href=https://zhannicholas.github.io/library>Library</a></div></div></div><a class="px-4 py-2 mt-2 text-sm font-semibold rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-indigo-600 focus:bg-indigo-700 focus:outline-none focus:shadow-outline" href=https://zhannicholas.github.io/now>Now</a>
<a class="px-4 py-2 mt-2 text-sm font-semibold bg-transparent rounded-lg md:mt-0 md:ml-4 hover:text-white focus:text-white hover:bg-indigo-600 focus:bg-indigo-700 focus:outline-none focus:shadow-outline" href=https://zhannicholas.github.io/about>About</a>
<button id=theme-toggle type=button class="p-2 text-sm text-gray-500 rounded-lg md: dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-4 focus:ring-gray-200 dark:focus:ring-gray-700 md:ml-2 max-w-5 xs:hidden"><svg id="theme-toggle-dark-icon" class="hidden w-5 h-5" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden w-5 h-5" fill="currentcolor" viewBox="0 0 20 20" aria-label="Dark or Light Mode" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button></nav></div></div><div class=content><article><header class="max-w-2xl mx-auto mb-4 bg-indigo-600"><span class=py-96><h1 class="px-6 pt-6 pb-16 mx-auto dark:prose-invert text-5xl font-black text-center text-white capitalize">Design Data Intensive Applications</h1></span></header><div class="max-w-4xl mx-auto mt-8 mb-2"><div class=px-6></div></div><div class="fixed z-20 top-[3.8125rem] bottom-0 right-[max(0px,calc(50%-45rem))] w-[18rem] py-10 overflow-y-auto hidden xl:block"><nav><div class="flex items-center"><h2 class="pl-2 my-0 text-xl font-medium text-zinc-800 uppercase break-words">Table of Contents</h2></div><div class="absolute top-auto bottom-auto right-auto p-0 -left-4" role=menu><div class="relative z-50 max-w-sm m-4 overflow-hidden shadow-lg"><ul class="relative p-4 overflow-x-hidden overflow-y-auto overscroll-y-auto overscroll-x-auto"><li><a class="px-2 text-left text-zinc-700 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-1-reliable-scalable-and-maintainable-applications>Chapter 1: Reliable, Scalable, and Maintainable Applications</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#reliability>Reliability</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#scalability>Scalability</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#maintainability>Maintainability</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-2-data-models-and-query-languages>Chapter 2: Data Models and Query Languages</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#relational-model-versus-document-model>Relational Model Versus Document Model</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#query-language-for-data>Query Language for Data</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#graph-like-data-models>Graph-Like Data Models</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-3-storage-and-retrieval>Chapter 3: Storage and Retrieval</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#data-structures-that-power-your-database>Data Structures That Power Your Database</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#hash-indexes>Hash Indexes</a></li></ul></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-4-encoding-and-evolution>Chapter 4: Encoding and Evolution</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#formats-for-encoding-data>Formats for Encoding data</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#modes-of-dataflow>Modes of Dataflow</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-5-replication>Chapter 5: Replication</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#leaders-and-followers>Leaders and Followers</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#synchronous-versus-asynchronous-replication>Synchronous Versus Asynchronous Replication</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#setting-up-new-followers>Setting Up New Followers</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#handling-node-outages>Handling Node Outages</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#follower-failure-catch-up-recovery>Follower failure: Catch-up recovery</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#leader-failure-failover>Leader failure: Failover</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#implementation-of-replication-logs>Implementation of Replication Logs</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#statement-based-replication>Statement-based replication</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#write-ahead-wal-shipping>Write-ahead (WAL) shipping</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#logical-row-based-log-replication>Logical (row-based) log replication</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#trigger-based-replication>Trigger-based replication</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#problems-with-replication-lag>Problems with Replication Lag</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#reading-your-own-writes>Reading Your Own Writes</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#multi-leader-replication>Multi-Leader Replication</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#handling-write-conflicts>Handling Write Conflicts</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#leaderless-replication>Leaderless Replication</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#writing-to-the-database-when-a-node-is-down>Writing to the Database When a Node Is Down</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-6-partitioning>Chapter 6: Partitioning</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#partitioning-and-replication>Partitioning and Replication</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#partitioning-of-key-value-data>Partitioning of Key-Value Data</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#partitioning-by-key-range>Partitioning by Key Range</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#partitioning-by-hash-of-key>Partitioning by Hash of Key</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#skewed-workloads-and-relieving-hot-spots>Skewed Workloads and Relieving Hot Spots</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#partitioning-and-secondary-indexes>Partitioning and Secondary Indexes</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#partitioning-secondary-indexes-by-document>Partitioning Secondary Indexes by Document</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#rebalancing-partitions>Rebalancing Partitions</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#request-routing>Request Routing</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-7-transactions>Chapter 7: Transactions</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#the-slippery-concept-of-a-transaction>The Slippery Concept of a Transaction</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#the-meaning-of-acid>The Meaning of ACID</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#atomicity>Atomicity</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#consistency>Consistency</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#isolation>Isolation</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#durability>Durability</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#weak-isolation-levels>Weak Isolation Levels</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#read-committed>Read Committed</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#snapshot-isolation-and-repeatable-read>Snapshot Isolation and Repeatable Read</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#implementing-snapshot-isolation>Implementing snapshot isolation</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#visibility-rules-for-observing-a-consistent-snapshot>Visibility rules for observing a consistent snapshot</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#repeatable-read-and-naming-confution>Repeatable read and naming confution</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#preventing-lost-updates>Preventing Lost Updates</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#write-skew-and-phantoms>Write Skew and phantoms</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#serializability>Serializability</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#actual-serial-executation>Actual Serial Executation</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#two-phase-locking-2pl>Two-Phase Locking (2PL)</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#serializable-snapshot-isolation-ssi>Serializable Snapshot Isolation (SSI)</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#decisions-based-on-an-outdated-premise>Decisions based on an outdated premise</a></li></ul></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-8-the-trouble-with-distributed-systems>Chapter 8: The Trouble with Distributed Systems</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#faults-and-partial-failures>Faults and Partial Failures</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#unreliable-networks>Unreliable Networks</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#unreliable-clocks>Unreliable Clocks</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#process-pauses>Process Pauses</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#knowledge-truth-and-lies>Knowledge, Truth, and Lies</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-9-consistency-and-consensus>Chapter 9: Consistency and Consensus</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#consistency-guarantees>Consistency Guarantees</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#linearizability>Linearizability</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#ordering-guarantees>Ordering Guarantees</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#distributed-transactions-and-consensus>Distributed Transactions and Consensus</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#atomic-commit-and-two-phase-commit-2pc>Atomic Commit and Two-Phase Commit (2PC)</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-10-batch-processing>Chapter 10: Batch Processing</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#system-of-record-and-derived-data>System of Record and Derived Data</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#chapter-11-stream-processing>Chapter 11: Stream Processing</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#transmitting-event-streams>Transmitting Event Streams</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#message-systems>Message Systems</a><ul><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#direct-messaging-from-producers-to-consumers>Direct messaging from producers to consumers</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#message-brokers>Message brokers</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#multiple-consuemers>Multiple Consuemers</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#acknowledgements-and-redelivery>Acknowledgements and redelivery</a></li></ul></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#partitioned-logs>Partitioned Logs</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#databases-and-streams>Databases and Streams</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#change-data-capture>Change Data Capture</a></li><li><a class="px-2 text-left text-zinc-900 rounded-md focus:outline-none hover:bg-lightBlue-600 hover:text-white" x-data href=#event-sourcing>Event Sourcing</a></li></ul></li></ul></div></div></nav></div><div class="max-w-2xl px-6 pt-6 pb-16 mx-auto prose dark:prose-invert dark:text-white"><p>This is my reading notes of <a href=https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/ target=_blank>Design Data Intensive Applications</a>
.</p><p>Preface</p><ul><li><p>if data (quantity, complexity, change speed, etc) is an application&rsquo;s primary challenge, then we call the application <em><strong>data-intensive</strong></em>. As opposed to <em><strong>compute-intensive</strong></em>, where CPU cycles are the bottleneck.</p></li><li><p>When we think about data systems, not just think how they work, but also why they work that way, and what questions we need to ask.</p></li><li><p>Reference materials: <a href=https://github.com/ept/ddia-references target=_blank>ddia-references</a>
.</p></li></ul><h2 id=chapter-1-reliable-scalable-and-maintainable-applications>Chapter 1: Reliable, Scalable, and Maintainable Applications</h2><ul><li><p>For data-intensive applications, the bigger problem are usually the amount of data, the complexity of data, and the speed at which it&rsquo;s changing.</p></li><li><p>data system building blocks:</p><ul><li><p>databases: store data so that they, or another application, can find it again later</p></li><li><p>caches: remember the result of an expensive operation, to speed up reads</p></li><li><p>search indexes: allow users to search data by keyword or filter it in various ways</p></li><li><p>stream processing: send a message to another process, to be handled asynchronously</p></li><li><p>batch processing: periodically crunch a large amount of accumulated data</p></li></ul></li><li><h3 id=reliability>Reliability</h3><ul><li><p>Reliability means the system will continue to work correctly (how do we define correctness?), even when things go wrong.</p></li><li><p>The things that can go wrong are called <em>faults</em>, and systems that anticipate faults and can cope with them are called <em>fault-tolerant</em> and <em>resilient</em>.</p><ul><li><p>Fault</p><ul><li><p>A fault is not the same as a failure. A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user.</p></li><li><p>As long as you can restore a backup onto a new machine fairly quickly, the downtime in case of failure is not catastrophic in most applications.</p></li><li><p>There are several types of faults: hardware fault, software faults, human errors&mldr;</p></li><li><p>Humans are known to be unreliable, even when they have the best intentions.</p><ul><li><p>How to make our system reliable, in spite of unreliable humans?</p><ul><li><p>Design systems in a way that minimizes opportunities for error.</p></li><li><p>Decouple the places where people make the most mistakes from the places where they can cause failures.</p></li><li><p>Test thoroughly at all levels.</p></li><li><p>Allow quick and easy recovery from human errors, to minimize the impact in the case of a failure.</p></li><li><p>Set up detailed and clear monitoring.</p></li><li><p>Implement good management practices and training.</p></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><h3 id=scalability>Scalability</h3><ul><li><p>Scalability is the term we use to describe a system&rsquo;s ability to cope with increased load. A common question is what are our options to handle the additional load?</p></li><li><p>Load</p><ul><li>Load can be described with a few numbers which we call <em>load parameters</em>, such as requests per second for a web server the ratio of reads or writes to a database, etc. The best choice of parameters depends on the architecture of your system.</li></ul></li><li><p>Once we have described the load on our system, we can investigate what happens when the load increases. We can look at it in two ways:</p><ul><li><p>When we increase a load parameter and keep the system resources unchanged, how is the performance of the system affected?</p></li><li><p>When we increase a load parameter, how much do we need to increase the resources if we want to keep the performance unchanged?</p></li></ul></li><li><p>Performance</p><ul><li><p>In a batch processing system, we usually care about <em>throughput</em>.</p></li><li><p>In online systems, <em>response time</em> is more important.</p><ul><li><p>Even if we only make the same request over and over again, we&rsquo;ll get a slightly different response time on every try. Sometimes, the response time can vary a lot. Therefore, we&rsquo;d better think of response time not as a single number, but as a <strong><em>distribution</em></strong> of values that we can measure.</p><ul><li><p>It&rsquo;s common to see the <em>average</em> response time of a service reported. However, the average is not a very good metric is we want to know our &ldquo;typical&rdquo; response time because it doesn&rsquo;t tell us how many users actually experienced that delay. Usually it is better to use <strong><em>percentiles</em></strong>.</p><ul><li><p>Median is a good metric if we want to know how long users typically have to wait,. The median is also known as the <em>50th percentile</em>, or <em>p50</em>.</p></li><li><p>In order to figure out how bad the outliers are, we can look at higher percentiles: the <em>95th</em>, <em>99th</em>, and the <em>99.9th</em> percentiles are common (abbreviated <em>p95</em>, <em>p99</em>, and <em>p999</em>).</p></li><li><p>High percentiles of response times, also known as <strong><em>tail latencies</em></strong>, are important because they directly affect users&rsquo; experience of the service.</p></li></ul></li></ul></li></ul></li></ul></li><li><p>An architecture that scales well for a particular application is built around assumptions of which operations will be common and which will be rare.</p></li></ul></li><li><h3 id=maintainability>Maintainability</h3><ul><li><p>The majority of the cost of software is not in its initial development, but in its ongoing maintenance.</p></li><li><p>Maintainability is in essence about making life better for the engineering and operations teams who need to work with the system.</p></li><li><p>Three design principles for a better maintainable software system:</p><ul><li><p>Operability</p><ul><li><p>Make it easy for operations teams to keep the system running smoothly.</p></li><li><p>Good operations can often work around limitations of bad (or incomplete) software, but good software cannot run reliably with bad operations.</p></li><li><p>Good operability means making routine tasks easy, allowing the operations team to focus their efforts on high-value activities.</p></li><li><p>Good operability also means having good visibility into the system&rsquo;s health, and having effective ways of managing it.</p></li></ul></li><li><p>Simplicity</p><ul><li><p>Make it easy for new engineers to understand the system.</p></li><li><p>A software project mired in complexity is sometimes described as a <em>big ball of mud</em>.</p></li><li><p>Making a system simpler doesn&rsquo;t necessarily mean reducing its functionality. Moseley and Marks define complexity as accidental if it is not inherent in the problem that the software solves but arises only from the implementation.</p></li><li><p>One of the best tools we have for removing accidental complexity is <strong><em>abstraction</em></strong>.</p></li></ul></li><li><p>Evolvability</p><ul><li>Make it easy for engineers to make changes to the system in the future, also known as extensibility, modifiability, or plasticity.</li></ul></li><li></li></ul></li></ul></li></ul><h2 id=chapter-2-data-models-and-query-languages>Chapter 2: Data Models and Query Languages</h2><ul><li><p>Data models are perhaps the most important part of developing software, because they not only affect how the software is written, but also how we think about the problem that we&rsquo;re solving.</p></li><li><h3 id=relational-model-versus-document-model>Relational Model Versus Document Model</h3><ul><li><p>SQL</p><ul><li>SQL may be the best-known data model, it&rsquo;s based on the relational model: data is organized into <em>relations</em> (called tables in SQL), where each relation is an unordered collection of <em>tuples</em> (rows in SQL).</li></ul></li><li><p>NoSQL</p><ul><li>NoSQL, or <em>Not Only SQL</em> appeared in the 2010s.</li></ul></li><li><div class=tip>Different applications have different requirements, and the best choice of technology for one use case may well be different from the best choice for another use case.</div></li><li><p>The JSON representation of data has better <em>locality</em> than the multi-table schema. However, in document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak.</p><ul><li><div class=tip>A document is usually stored as a single continuous string. If our application needs to access the entire document, there is a performance advantage to this <em>storage locality</em>. However, the locality advantage only applies if we need large parts of the document at the same time, The database typically needs to load the entire document, even if we access only a small portion of it, which can be wasteful on large documents.
On update to a document, the entire document usually needs to be written. So, keep documents fairly small and avoid writes that increase the size of a document is a good practice.</div></li></ul></li><li><p>The main arguments in favor of the document data model are schema flexibility, better performance due to locality, and that for some applications it is closer to the data structures used by the application. The relational model counters by providing better support for joins, and many-to-one and many-to-many relationships.</p></li><li><p>Document databases are sometimes called <em>schemaless</em>, but that&rsquo;s misleading, as the code that reads the data usually assumes some kind of structure.</p></li><li><p>schema-on-read vs schema-on-write</p><ul><li><p><em>schema-on-read</em>: the structure of the data is implicit, and only interpreted when the data is read.</p></li><li><p><em>schema-on-write</em>: the traditional approach of relational databases, where the schema is explicit and the database ensure all written data conforms to it.</p></li><li><p>From a programming language perspective, schema-on-read is similar to dynamic (runtime) type checking, whereas schema-on-write is similar to static (compile-time) type checking.</p></li></ul></li><li><p>A hybrid of the relational and document models is a good route for databases to take in the future.</p></li></ul></li><li><h3 id=query-language-for-data>Query Language for Data</h3><ul><li><p>SQL is a <em>declarative</em> query language. In declarative query language, we just specify the pattern of the data we want.</p></li><li><p>An imperative language tells the computer to perform certain operations in a certain order, or how to achieve a goal.</p></li><li><p><em>MapReduce</em> is a programming model for processing large amounts of data in bulk across many machines. MapReduce is neither a declarative query language nor a fully imperative query API, but somewhere in between.</p></li></ul></li><li><h3 id=graph-like-data-models>Graph-Like Data Models</h3><ul><li><p>A graph consists of two kinds of objects: <em>vertices</em> (also known as <em>nodes</em> or <em>entities</em>) and <em>edges</em> (also known as <em>relationships</em> or <em>arcs</em>).</p></li><li><p>Graphs are not limited to homogeneous data: an equally powerful use of graphs is to provide a consistent way of storing completely different types of objects in a single datastore.</p></li><li></li></ul></li></ul><h2 id=chapter-3-storage-and-retrieval>Chapter 3: Storage and Retrieval</h2><ul><li><p>One the most fundamental level, a database needs to do two things: when you give it some data, it should store the data, when when you ask it again later, it should give the data back to you.</p></li><li><h3 id=data-structures-that-power-your-database>Data Structures That Power Your Database</h3><ul><li><p>In order to efficiently find the value for a particular key in the database, we need a data structure called <em>index</em>. The general idea behind index is to keep some additional metadata on the side, which acts as a signpost and helps you to locate the data you want. An index is an additional structure that derived from the primary data.</p></li><li><p>An important trade-off in storage systems is about the index: well-chosen indexes speed up read queries, but every index slows down writes. For this reason, database don&rsquo;t usually index everything by default, but require its user to choose indexes manually.</p></li><li><h4 id=hash-indexes>Hash Indexes</h4><ul><li><p>Hash index is for key-value data, and key-value stores are usually implemented as a hash map (hash table).</p><ul><li>A simple hash indexing strategy may look like this: keep an in-memory hash map where every key is mapped to a byte offset in the data file&mdash;the location at which the value can be found.</li></ul></li></ul></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/simple_hash_index.png alt=image.png></p><pre><code>  + Hash index is usually used with _append-only_ file (log) write strategy, which means the key-value pairs appear in the order that they were written, and values later in log take precedence over values for the same key earlier in that log.

  + The hash table index is simple, but has some limitations

    + The hash table must fit in memory, so it is not very friendly to large data.

    + Range queries are not efficient.

+ #### SSTables


  + In simple hash index, keys are not sorted. _Sorted String Table_ (or _SSTable_) require that the sequence of key-value pairs is **sorted by key**. Append new key-value pairs to the log will be unsuitable.

  + In order to find a particular key in the file, we no longer need to keep an index of all the keys in memory. In stead, we only need an in-memory index to tell us the offset for some of the keys and the index can be sparse.

  + The biggest problem is how to get our data sorted by key in the first place.


    + Some data structures, such as red-black trees and AVL trees, can help us insert keys in any order and read them back in sorted order.

    + A simple storage engine may work as follow:

      + When a write comes in, add it to an in-memory balanced tree data structure (called _memtable_).

      + Write the memtable out to disk as an SSTable file when the memtable gets bigger than some treshhold.

      + In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk SSTable file, then in the next-order one, etc.

      + From time to time, run a merging and compaction process in the background to combine files and to discard overwritten or deleted values.

    + There is a big problem in above storage engine: if the database crashes, the data in memtable is lost. To avoid this problem, we can keep a separate log on disk to which every write is immediately append, it doesn't need to be sorted, since it's only purpose is to restore memtable after a crash.

  + LSM-Tree (or Log-Structured Merge-Tree) is built on log-structured filesystems. The basic idea of LSM-trees is to keep a cascade of SSTables that are merged in the background.

+ #### B-Trees


  + B-Tree is the most widely used indexing structure. Like SSTables, B-trees keep key-value pairs sorted by key, which provides efficient key-value lookups and range queries.

  + The biggest difference between log-structured indexes and B-trees is how the database is broken down.

    + log-structured indexes break the database down into variable-size _segments_, and always write a segment sequentially.

    + B-trees break the database down into fixed-size _blocks_ or _pages_, and read or write one page at a time. This design corresponds more closely to underlying hardware, as disks are also arranged in fixed-size blocks.

  + Each page can be identified by an address or location, which allows one page to refer to another---similar to a pointer, but on disk instead of memory. These page references can be used to construct a tree of pages.
</code></pre><p><img src=https://zhannicholas.github.io/assets/ddia/looking_up_a_key_using_a_btree_index.png alt></p><pre><code>  + One page is designated as the root of the B-tree, and every lookup will start from the root page. Each page contains several keys and references to child pages, each child page is responsible for a continuous range of keys, and the keys between the references indicate where the boundaries between those ranges lie.

  + The number of references to child pages in one page of the B-tree is called the _branching factor_, which depends on the amount of space required to store the page references and the range boundaries.

  + A B-tree with _n_ keys always has a depth of O(logn).

  + The basic underlying write operation of a B-tree is to overwrite a page on disk with new data. Moreover, some operations require several different pages to be overwritten, such as a insert operation result a page split.

  + Since a database may crash at any time, write multiple pages one time is extremely dangerous. To avoid this, B-tree implementations typically will include an additional data structure on disk: a _write-ahead log_ (WAL, also known as _redo log_). This is an append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree itself.

  + 

  + 

+ #### Other Indexing Structures


  + Hash indexes, SSTables, LSM-Trees and B-Trees are key-value indexes, which are like a **_primary key_** index in the realational model. A primary key **uniquely** identifies one row in a relational table, or one document in a document database, or one vertex in a graph database.

  + _**Secondary index**_ is another type of index. It can easily be constructed from a key-value index. But the main difference is that in a secondary index, the indexed value are not necessarily unique.

  + The key in an index is the thing that queries search for, but the value can be one of two things: the actual row (document, vertex) in question, or a reference to the row stored elsewhere. In the latter case , the place where rows are stored is known as a _heap file_, and it stores data in no particular order. But heap file make update a value without changing the key more efficient.

  + Sometimes, an extra lookup into the heap has a performance penalty for reads, so it can be desirable to stored the indexed row directly within an index. This is known as a **_clustered index_**.

    + <div class=note>
    In MySQL&rsquo;s InnoDB storage engine, the primary key of a table is always a clustered index, and secondary indexes refer to the primary key.
</div>

  + A compromise between a clustered index (storing all row data within the index) and a nonclustered index (storing only references to the data within the index) is known as a _covering index_ or _index with included columns_, which stores some of a table's columns within the index alone,

    + <div class=note>
    As with any kind of duplication of data, clustered and covering indexes can speed up reads, but they require additional storage and can add overhead on writes.
</div>

  + Multi-column indexes

    + If the index only map a single key to a value, then it's not sufficient when we need to query multiple columns of a table simultaneously.

    + _Concatenated index_ is the most common type of multi-column index, it simply combines several fields into one key by appending one column to another. Such as (firstname, lastname) -&gt; firstname-lastname, in this way, prefix query is also benefited.

    + Multi-dimensional indexes are a more general way of querying several columns at one, which is particularly important for geospatial data.

  + Full-text search and fuzzy indexes

    + Both single key indexes and multi-column indexes assume that users have exact data and allow them to query for exact values of a key, or a range of values of a key with a sort order. What they don't allow us to do is search for _similar keys_, such fuzzy querying requires different techniques.
</code></pre><h2 id=chapter-4-encoding-and-evolution>Chapter 4: Encoding and Evolution</h2><ul><li><p>Applications inevitably change over time. Thus we should aim to build systems that make it easy to adapt to change.</p><ul><li><p>Data format or schema changes often result a corresponding change to application code. However, in a large application, code changes often cannot happen instantaneously:</p><ul><li><p>With server-side applications, we may want to perform a <em>rolling upgrade</em> (also known as <em>staged rollout</em>), deploying the new version to a few nodes at a time, checking whether the new version is running smoothly, and gradually working our way through all the nodes. This allows new version to be deployed without service downtime, and thus encourages more frequent releases and better evolvability.</p></li><li><p>With client-side applications, we&rsquo;re at the mercy of the user, who may not install the update for some time.</p></li></ul></li><li><p>Old and new versions of the code, and old and new data formats may potentially coexist in the system at the same time. In order for the system to continue running smoothly, we need to maintain compatibility in both directions:</p><ul><li><p><em>Backward compatibility</em>: Newer code can read data that was written by older code.</p></li><li><p><em>Forward compatibility</em>: Older code can read data that was written by newer code.</p></li><li><p>Backward compatibility is normally not hard to achieve: as author of the newer code, we know the format of data written by old code, and so we can explicitly handle it. However, forward compatibility is not very easy, because it requires older code to ignore additions made by a newer version of the code.</p></li><li><p>During rolling updates, or for various other reasons, we must assume that different nodes are running the different version of our application&rsquo;s code. Thus it is important that all data flowing around the system is encoded in a way that provides backward compatibility and forward compatibility.</p></li></ul></li></ul></li><li><h3 id=formats-for-encoding-data>Formats for Encoding data</h3><ul><li><p>Programs usually work with data in (at least) two different representations:</p><ul><li><ol><li>In memory, data is kept in objects, lists, etc. These data structures are optimized for efficient access and manipulation by the CPU.</li></ol></li></ul></li></ul></li></ul><ol start=2><li><p>When we want to write data to a file or send it over the network, the data needs to be encoded into some kind of self-contained sequence of bytes.</p><ul><li><p>The translation from the in-memory representation to a byte sequence is called <em>encoding</em> (also known as <em>serialization</em> or <em>marshalling</em>), and the reverse is called <em>decoding</em> (<em>parsing</em>, <em>deserialization</em>, <em>unmarshalling</em>).</p></li><li><p>Since data encoding and decoding is a common problem, there are many different libraries to handle this. Among these libraries, some are language-specific (such as <code>java.io.Serializable</code>), while others are cross-programming-languages (such as those in textual format like JSON, XML, CSV, and those in binary like Thrift, Protocol Buffers, Avro).</p></li><li><p>Protocol Buffers, Thrift, and Avro all use a schema to describe a binary encoding format. Schema evolution is mainly reflected on how these data formats implement backward/forward compatibility.</p></li></ul></li></ol><ul><li><h3 id=modes-of-dataflow>Modes of Dataflow</h3><ul><li><p>Compatibility is a relationship between one process that encodes the data, and another process that decodes it. There&rsquo;re many ways data can flow from one process to another. The central point is who encodes the data, and who decodes it?</p></li><li><p>Some of the common ways of data flows between processes are: via databases, via service calls, and via asynchronous message passing.</p></li><li><p>Dataflow through Databases: In the database, the process that writes to the database encodes the data, and the process that reads from the database decodes it.</p></li><li><p>RPC model tries make a request to a remote network service look the same as calling a funciton or method in your programming language, within the same process (this abstraction is called <em>location transparency</em>)</p></li><li><p>Asynchronous message-passing systems are somewhere between RPC and databases. They are similar to RPC in that a client&rsquo;s request (usually called a message) is delivered to another process with low latency. They are similar to databases in that the message is not sent via a direct network connection, but goes via an intermediary called a <em>message broker</em> (also called a <em>message queue</em> or <em>message-oriented middleware</em>), which stores the message temporarily.</p><ul><li>message-passing communicate is usually one-way: a sender normally doesn&rsquo;t expect to receive a reply to its messages. The send doesn&rsquo;t need wait for the message to be delivered, but simple sends and then forgets about it (fire-and-forget).</li></ul></li></ul></li><li></li></ul><h2 id=chapter-5-replication>Chapter 5: Replication</h2><ul><li><p>There are two common ways data is distributed across multiple nodes:</p><ul><li><p>Replication: keeping a copy of the same data on several different nodes, potentially in different locations.</p></li><li><p>Partitioning: splitting a big database into smaller subsets called <em>partitions</em> so that different partitions can be assigned to different nodes (also known as <em>sharding</em>).</p></li></ul></li><li><p>There are several reasons why we might want to replicate data:</p><ul><li><p>To keep data geographically close to our users (and thus reduce access latency)</p></li><li><p>To allow the system to continue working even if some of its parts have failed (and thus increase availability)</p></li><li><p>To scale out the number of machines that can serve read queries (and thus increase read throughput)</p></li></ul></li><li><p>All of the difficulty in replication lies in handling changes to replicated data. There are mainly three popular algorithms for replicating changes between nodes: single-leader, multi-leader, and leaderless replication.</p></li><li><h3 id=leaders-and-followers>Leaders and Followers</h3><ul><li><p>Each node that stores a copy of the database is called a <em>replica</em>. A question arises when multiple replica exists: how do we ensure that all the data ends up on all the replicas?</p></li><li><p>To avoid data unconsistency, each write to the database needs to be processed by every replica. The common solution for this is called <em>leader-based replication</em> (also known as <em>active/passive</em> or <em>master-slave replication</em>).</p></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/leader_based_replication.png alt></p><pre><code>  + The leader-based replication works as follows:

    + 1. One of the replicas is designated the _leader_ (also known as _master_ or _primary_). When clients want to write to the database, they must send their requests to the leader, which first writes the new data to its local storage.
</code></pre><ol start=2><li><p>The other replicas are known as <em>followers</em> (also know as <em>read replicas</em>, <em>slaves</em>, <em>secondaries</em>, or <em>hot standbys</em>). Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of a <em>replication log</em> or <em>change stream</em>. Each follower take the log from the leader and updates its local copy of the database accordingly.</p></li><li><p>When a client wants to read from the database, it can query either the leader of any of the followers. However, writes are only accepted on the leader.</p><ul><li><h3 id=synchronous-versus-asynchronous-replication>Synchronous Versus Asynchronous Replication</h3><ul><li><p>An important detail of a replica system is whether the replication happens <em>synchronously</em> or <em>asynchronously</em>.</p></li><li><p>A follower might be synchronous (the leader wait until the follower has confirmed that it received the write before reporting success to the user) or asynchronous (the leader send the message, but doesn&rsquo;t wait for a response from the follower).</p></li><li><p>The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the that that is consistent with the leader. The disadvantage is that if the synchronous follower doesn&rsquo;t respond, the write cannot be processed, the leader must block all writes and wait until the synchronous replica is available again.</p></li><li><p>In practice, if we enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous.</p></li><li><p>If the synchronous follower becomes unavailable or slow, one of the asynchronous followers is made synchronous. This guarantees that we have an up-to-date copy of the data on at least two nodes: the leader and one synchronous follower. This configuration is sometimes also called <em>semi-synchronous</em>.</p></li><li><p>Often, leader-based replication is configured to be completely asynchronous. In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable, even if it has been confirmed to be the client.</p></li></ul></li><li><h3 id=setting-up-new-followers>Setting Up New Followers</h3><ul><li><p>When we need to set up new followers, one thing needs to be considered is how do we ensure that the new follower has an accurate copy of the leader&rsquo;s data?</p></li><li><p>Simply copying data files from one node to another is typically not sufficient: clients are constantly writing to the database, and the data is always in flux, so a standard file copy would see different parts of the database at different points in time. Locking the database (making it not writable) is also unacceptable, because it will decrease availability. Fortunately, setting up a follower can usually be done without downtime:</p><ul><li><ol><li>Take a consistent snapshot of the leader&rsquo;s database at some point in time.</li></ol></li></ul></li></ul></li></ul></li><li><p>Copy the snapshot to the new follower node.</p></li><li><p>The follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. This requires that the snapshot is associated with an exact position in the leader&rsquo;s replication log. For PostgreSQL, this position is called <em>log sequence number</em>; for MySQL, it is called <em>binlog coordinates</em>.</p></li><li><p>When the followers has processed the backlog of data changes since the snapshot, we say it has <em>caught up</em>.</p><ul><li><h3 id=handling-node-outages>Handling Node Outages</h3><ul><li><p>How do we achieve high availability with leader-based replication?</p></li><li><h4 id=follower-failure-catch-up-recovery>Follower failure: Catch-up recovery</h4><ul><li>On its local disk, each follower keeps a log of the data changes it has received from the leader. If the follower crashes and is restarted, the follower can recover quite easily: from its log, it knows the last transaction that was processed before the fault occurred. Thus, the follower can catch up to the leader.</li></ul></li><li><h4 id=leader-failure-failover>Leader failure: Failover</h4><ul><li><p>Handling a failure of the leader is trickier: one of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader, and the other followers need to start consuming data changes from the new leader. This process is called <em>failover</em>.</p></li><li><p>When choosing a new leader, the best candidate for leadership is usually the replica with the most up-to-date data changes from the old leader (to minimize any data loss).</p></li><li><p>In certain fault scenarios, it could happen that two node both believe that they are the leader. This situation is called <em>split brain</em>, and it is dangerous: if both leaders accept writes, and there is no process for resolving conflicts, data is likely to be lost or corrupted.</p></li></ul></li></ul></li><li><h3 id=implementation-of-replication-logs>Implementation of Replication Logs</h3><ul><li><p>How does leader-based replication work under the hood?</p></li><li><h4 id=statement-based-replication>Statement-based replication</h4><ul><li><p>In the simplest case, the leader logs every write request (<em>statement</em>) that it executes and sends that statement log to its followers.</p></li><li><p>Although statement-based replication is simple, there are various ways in which this approach to replication can break down:</p><ul><li><p>Any statement that calls a nondeterministic function, such as NOW() or RAND(), is likely to generate a different value on each replica.</p></li><li><p>If statements use an autoincrementing column, or if they depend on the existing data in the database, they must be executed in exactly the same order on each replica, or else they may have a different effect.</p></li><li><p>Statements that have side effects (e.g., triggers, stored procedures, user-defined functions) may result in different side effects occurring on each replica.</p></li></ul></li></ul></li><li><h4 id=write-ahead-wal-shipping>Write-ahead (WAL) shipping</h4><ul><li><p>Both log-structured storage engine (such as SSTables and LSM-Trees) and B-tree use logs, the log is an append-only sequence of bytes containing all writes to the database. We can use the exact same log to build a replica on another node.</p></li><li><p>The main disadvantage of log mentioned above is that the log describes the data on a very low level: a WAL contains details of which bytes were changed in which disk blocks. This makes replication closely coupled to the storage engine.</p></li></ul></li><li><h4 id=logical-row-based-log-replication>Logical (row-based) log replication</h4><ul><li><p>An alternative of WAL log is to use different log formats for replication and for the storage engine, which allows the replication log to be decoupled from the storage engine internals. This kind of replication log is called a <em>logical log</em>, to distinguish it from the storage engine&rsquo;s (<em>physical</em>) data representation.</p></li><li><p>A logical log a relational database is usually a sequence of records describing writes to database tables at the granularity of a row.</p></li><li><p>A logical log format is also easier for external applications to parse. This aspect is useful if we want to send the contents of a database to an external system, this technique is called <em>change data capture</em>.</p></li></ul></li><li><h4 id=trigger-based-replication>Trigger-based replication</h4><ul><li><p>A trigger lets us register custom application code that is automatically executed when a data change (write transaction) occurs in a database system. The trigger has the opportunity to log this change into a separate table.</p></li><li><p>Trigger-based replication typically has greater overheads than other replication methods, and is more prone to bugs and limitations than the database&rsquo;s built-in replication.</p></li></ul></li></ul></li></ul></li></ol><ul><li><h3 id=problems-with-replication-lag>Problems with Replication Lag</h3><ul><li><p>Being able to tolerate node failures is just one reason for wanting replication, other reasons are scalability (processing more requests than a single machine can handle) and latency (placing replicas geographically closer to users).</p></li><li><p>In normal operation, the delay between a write happening on the leader and being reflected on a follower&mdash;the <em>replication lag</em>&mdash;may be only a fraction of a second, and not noticeable in practice. However, if the system is operating near capacity or if there is a problem in the network, the lag can easily increase to several seconds or even minutes.</p></li><li><h3 id=reading-your-own-writes>Reading Your Own Writes</h3><ul><li><p>Many applications let the user submit some data and then view that they have submitted.</p></li><li><p>With asynchronous replication, there is a problem: if the user views the data shortly after making a write, the new data may not yet have reached the replica. To the user, it looks as though the data they submitted was lost. A user makes a write, followed by a read from a stale replica. To prevent this anomaly, we need read-after-write consistency.</p></li></ul></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/a_user_makes_a_write_followed_by_a_read_from_a_stale_replica.png alt></p><pre><code>  + To prevent this anomaly, we need _read-after-write consistency_ (also known as _read-your-writes consistency_). This guarantee that if the user reloads the page, they will always see any updates they submitted themselves.

+ ### Monotonic Reads


  + When reading from asynchronous followers, it is possible for a user to see things _moving backward in time_. This can happen if a user makes several reads from different replicas. A user first reads from a fresh replica, then from a stale replica. Time appears to go backward. To prevent this anomaly, we need monotonic reads.
</code></pre><p><img src=https://zhannicholas.github.io/assets/ddia/a_user_first_read_from_a_fresh_replica_then_from_a_stale_replica.png alt></p><pre><code>  + Monotonic reads (单调读) is a guarantee moving backward in time does not happen, it means if one user makes several reads in sequence, they won't see time go backward---i.e., they won't read older data after having previously read newer data.

+ ### Consistent Prefix Reads

  + If some partitions are replicated slower than others, an observer may see the answer before they see the question. This anomaly is about causality. Preventing this kind of anomaly requires _consistent prefix reads_.
</code></pre><p><img src=https://zhannicholas.github.io/assets/ddia/some_partitions_are_replicated_slower_than_others.png alt></p><pre><code>  + Consistent prefix reads says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.

  + 
</code></pre><ul><li><h3 id=multi-leader-replication>Multi-Leader Replication</h3><ul><li><p>Leader-based replication has one major downside: there is only one leader, and all writes must go through it. If we can&rsquo;t connect to the leader for any reason, we can&rsquo;t write to the database.</p></li><li><p>An extension of the leader-based replication model is to allow more than one node to accept writes. Each node that processes a write must forward that data change to all the other nodes. This model is called <em>multi-leader</em> (also known as <em>master-master</em> or <em>active/active replication</em>). In this setup, each leader simultaneously acts as a follower to the other leaders.</p></li><li><p>There are many use cases for multi-leader replication mode, such as:</p><ul><li><p>multi-datacenter operation: each datacenter can have a leader. Within each datacenter, regular leader-follower replication is used; between datacenters, each datacenter&rsquo;s leader replicates its change to the leaders in other datacenters.</p><ul><li>Although multi-leader replication has advantages, it also has a big downside: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved.</li></ul></li><li><p>Clients with offline operation: a typical case is calendar. A person could have a calendar account, but the calendar application can be installed on several devices. Different devices need a synchronization, but the synchronization may take place after a long time because network is unavailable.</p><ul><li>From a architectural point of view, each device is a &ldquo;datacenter&rdquo;, and the network is unreliable.</li></ul></li><li><p>Collaborative editing: Real-time collaborative editing applications allow several people to edit a document simultaneously. Like offline operation, each people play a role in datacenter.</p></li></ul></li><li><h3 id=handling-write-conflicts>Handling Write Conflicts</h3><ul><li>The biggest problem with multi-leader replication is that write conflicts can occur, which means that conflict resolution is required.</li></ul></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/a_write_conflict_caused_by_two_leaders_concurrently_updating_the_same_record.png alt></p><pre><code>  + #### Synchronous versus asynchronous conflict detection

    + In a single-leader database, the second writer will either block and wait for the first write to complete, or abort the second write transaction, forcing the user to retry the write. However, in multi-leader replication, both two writes are successful, and the conflict is only detected asynchronously at some later point in time. At that time, it may be too late to ask the user to resolve the conflict.

    + In principle, we could make the conflict detection synchronous---wait for the write to be replicated to all replicas before telling the user that the write was successful. However, the main advantage (allowing each replica to accept writes independently) of multi-leader replication is lost.

  + #### Conflict avoidance

    + The simplest strategy for dealing with conflicts is to avoid them: if the application can ensure that all writes for a particular record go through the same leader, then conflicts cannot occur.

    + However, sometimes we might want to change the designated leader for a record, perhaps because one datacenter has failed, or any other reasons. In this situation, conflict avoidance won't work, we have to deal with the possibility of concurrent writes on different leaders.

  + #### Converging toward a consistent state

    + A single-leader database applies writes in a sequential order: if there are several updates to the same field, the last write determines the final value of the field. But in multi-leader configuration, there is no defined ordering of writes.

    + If each replica simply applied writes in the order that it saw the writes, the database would end up in an inconsistent state. This is not acceptable---every replication system must ensure that the data is eventually the same in all replicas. Thus, the database must resolve the conflict in a **_convergent_** way, which means that all replicas must arrive at the same final value when all changes have been replicated.

    + There are various ways of achieving convergent conflict resolution:

      + Give each write a unique ID, pick the write with the highest ID as the _winner_, and throw away the other writes. For example, if the unique ID is timestamp, the technique is known as _last write wins (LWW)_.

      + Give each replica a unique ID, and let writes that originated at a higher-numbered replica always take precedence over writes that originated at a lower-numbered replica.

      + Somehow merge the values together.

      + Record the conflict in an explicit data structure that preserves all information, and write application code that resolves the conflict at some later time.

  + #### Custom conflict resolution logic

    + As the most appropriate way of resolving a conflict may depend on the application, most multi-leader replication tools let us write conflict resolution logic using application code. That code may be executed on write or on read.

    + <div class=note>
    There has been some interesting research into automatically resolving conflicts caused by concurrent data modifications, such as Conflict-free Replicated Datatypes (CRDTs), mergeable persistent data structures, operational transformation.
</div>

+ ### Multi-Leader Replication Topologies


  + A replication topology describes the communication paths along which writes are propagated from one node to another.
</code></pre><p><img src=https://zhannicholas.github.io/assets/ddia/three_example_topologies_in_which_multi_leader_replication_can_be_setup.png alt></p><pre><code>    + In circular topology, each node receives writes from one node and forwards those writes (plus any writes of its own) to other node.

    + In star topology, one designated root node forwards writes to all of the other nodes.

    + In all-to-all topology, every leader sends its writes to every other leader.

  + To prevent infinite replication loops, each node is given a unique identifier, and in the replication log, each write is tagged with the identifiers of all the node it has passed through. When a node receives a data change that is tagged with its own identifier, that data change will be ignored.

  + A problem with circular and start topologies is that if just one node fails, it can interrupt the flow of replication messages between other nodes, causing them to be unable to communicate until the node is fixed.

  + On the other hand, all-to-all topologies can have issues too. In particular, some network links may be faster than others, with the result that some replication message may &quot;overtake&quot; others.
</code></pre><p><img src=https://zhannicholas.github.io/assets/ddia/writes_may_arrive_in_the_wrong_order_at_some_replicas.png alt></p><pre><code>    + To address this issue, imply attaching a timestamp to every write is not sufficient, because clocks may not be synchronous in all nodes. To order these events correctly, a technique called _version vector_ can be used.
</code></pre><ul><li><h3 id=leaderless-replication>Leaderless Replication</h3><ul><li><p>Both single-leader and multi-leader replications are based on the idea that a client sends a write request to one node (the leader), and the database system takes care of copying that write to the other replicas.</p></li><li><p>Some of the earliest replicated systems were leaderless, but the idea was mostly forgotten during the era of relational database. Util Amazon used it for its in-house <em>Dynamo</em> system, the architecture became popular again. Since Riak, Casssndra, and Voldmort are open source datastores with leaderless replication models inspired by Dynamo, so this kind of database is known as <em>Dynamo-style</em>.</p></li><li><h3 id=writing-to-the-database-when-a-node-is-down>Writing to the Database When a Node Is Down</h3><ul><li>In a leaderless configuration, failover doesn&rsquo;t exist. Generally, for a write request, the client will send it to all replicas in parallel, and if the client received enough (for example, two in three) <em>ok</em> responses, it consider the write to be successful.</li></ul></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/a_quorum_write_quorum_read_and_read_repair_after_a_node_outage.png alt></p><pre><code>  + Any writes that happened while the node was down are missing from that node. Thus, if a client read from that node, it may get _stale_ (outdated) values as responses.

  + To solve this problem, when a client reads from the database, it doesn't just send its request to one replica: read requests are also sent to several nodes in parallel. The client get may get different responses from different nodes. Versioned numbers are used to determine which value is newer.

  + #### Read repair and anti-entropy


    + The replication system should ensure that eventually all the data is copied to every replica. After an unavailable node comes back online, how does it catch up on the writes that it missed?

    + Two mechanisms are often used in Dynamo-sty;e datastores:

      + Read repair. When a client makes a read from several nodes in parallel, it can detect any stale responses. Once found a node has a stale value, the client will writes the newer value back to that replica. This approach works well for values that are frequently read.

      + Anti-entropy process. Some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another. Unlike the replication log in leader-based replication, this _anti-entropy process_ does not copy writes in any particular order, and there may be a significant delay before data is copied.

    + Without an anti-entropy process, values that are rarely read may be missing from some replicas and thus have reduced durability, because read repair only performed when a value is read by the application.

  + #### Quorum for reading and writing

    + If there are _n_ replicas, every write must be confirmed by _w_ nodes to be considered successful, and we must query at least _r_ nodes for each read. As long as _w + r &gt; n_, we expect to get an up-to-date value when reading, because at least one of the _r_ nodes we're reading from must be up to date. Reads and writes that obey there _r_ and _w_ values are called _quorum_ reads and writes. We can think of _r_ and _w_ as the minimum number of votes required for the read or write to be valid.

      + <div class=note>
    There may be more than <em>n</em> nodes in the cluster, but any given value is stored only on <em>n</em> nodes. This allows the database to be partitioned, supporting datasets that are larger than we can fit on one node.
</div>

+ ### Sloppy Quorums and Hinted Handoff

  + Quorums are not as fault-tolerant as they could be. A network interruption can easily cut off a client from a large number of database nodes.

  + Sloppy quorum: writes and reads still require _w_ and _r_ successful responses, but those may include nodes that are not among the designated _n_ &quot;home&quot; nodes for a value.

  + Once the network interruption is fixed, any writes that one node temporarily accepted on behalf of another node are sent to the appropriate &quot;home&quot; nodes. This is called _hinted handoff_.

  + Sloppy quorums are particularly useful for increasing write availability: as long as any _w_ nodes are available, the database can accepts writes. However, this means even when _w + r &gt; n_, you cannot be sure to read the latest value for a key, because the latest value may have been temporarily written to some nodes outside of _n_.

+ #### Detecting Concurrent Writes

  + Dynamo-style databases allow several clients to concurrently write to the same key, which means that conflicts will occur even if strict quorums are used. The problem is that events may arrive in a different order at different nodes, due to network delays and partial failures.

  + #### Last writes wins (discarding concurrent writes)

    + One approach for achieving eventual convergence is to declare that each replica need only store the most &quot;recent&quot; value and allow &quot;older&quot; values to be overwritten and discarded.

    + LWW is a good choice in which lost writes are perhaps acceptable, such as caching. However, if losing data is not acceptable, LWW is a poor choice for conflict resolution.

  + #### The &quot;happens-before&quot; relationship and concurrency

    + An operation A _happens before_ another operation B if B knows about A, or depends on A, or builds upon A in some way.

    + Whether one operation happens before another operation is the key to defining what concurrency means. In fact, we can simply say that two operations are concurrent if neither happens before the other.
</code></pre><ul><li></li></ul><h2 id=chapter-6-partitioning>Chapter 6: Partitioning</h2><ul><li><p>Replication is having multiple copies of the same data on different nodes. For very large databases, or very high query throughput, that is not sufficient: we need to break the data up into <em>partitions</em>, also known as <em>sharding</em>.</p></li><li><div class=note><em>partition</em> has many different names, but they are all the same thing. Such as: <em>shard</em> in <a href=https://zhannicholas.github.io/pages/mongodb/ target=_blank>MongoDB</a>
, Elasticsearch, and SolrCloud; <em>region</em> in HBase, <em>tablet</em> in Bigtable, <em>vnode</em> in Cassandta and Riak; <em>vBucket</em> in CouchBase.</div></li><li><p>Normally partitions are defined in such a way that each piece of data (each record, row, or document) belongs to exactly one partition.</p></li><li><p>The main reason for wanting to partition data is scalability: different partitions can be placed on different nodes in a shared-nothing cluster. Thus, a large dataset can be distributed across many nodes, so as the query load.</p></li><li><h3 id=partitioning-and-replication>Partitioning and Replication</h3><ul><li><p>Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes. This means that, even though each record belongs to exactly one partition, it may still be stored on several different nodes for fault tolerance.</p></li><li><p>A node may store more than one partition. Each partition&rsquo;s leader is assigned to one node, and its followers are assigned to other nodes. Each node may be the leader for some partitions and a followers for other partitions.</p></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/combining_replication_and_partitioning.png alt></p><ul><li><h3 id=partitioning-of-key-value-data>Partitioning of Key-Value Data</h3><ul><li><p>The goal of partitioning is to spread the data and the query load evenly across nodes. If the partitioning is unfair, so that some partitions have more data or queries than others, we call it <em>skewed</em>. The presence of skew makes partitioning much less effective. In the worst case, all the load end up on one partition. leaves all other partitions idle. A partition with disproportionately high load is called a <em>hot spot</em>.</p></li><li><p>The simplest approach to avoid hot spots would be to assign records to node randomly, but it has a big disadvantage: we have to query all nodes in parallel because we have no way of knowing which node the target data is on.</p></li><li><h3 id=partitioning-by-key-range>Partitioning by Key Range</h3><ul><li><p>Suppose we have a simple key-value data model, one way to partitioning is to make the records sorted by their key and assign a continuous range of keys (from some minimum to some maximum) to each partition.</p></li><li><p>If we know the boundaries between the ranges, we can easily determine which partition contains a given key.</p></li><li><p>The range of keys are necessarily evenly spaced, because our data may not be evenly distributed. In order to distribute the data evenly, the partition boundaries need to adapt to the data.</p></li><li><p>Unfortunately, the downside of key range partitioning is that certain access patterns can lead to hot spots.</p></li></ul></li><li><h3 id=partitioning-by-hash-of-key>Partitioning by Hash of Key</h3><ul><li><p>Because of the risk of skew and hot spots, many distributed datastores use a hash function to determine the partition for a given key. Because a good hash function takes skewed data and makes it uniformly distributed.</p></li><li><p>For partitioning purpose, the hash function need not be cryptographically strong, but be cautious the same key may have a different hash value in different processes, making it unsuitable for partitioning.</p></li><li><p>Once we have a suitable hash function for keys, we can assign each partition a range of hashes (rather a range of keys), and every key whose hash falls within a partition&rsquo;s range will be stored in that partition.</p></li><li><p>Hash function can make the partition boundaries evenly spaced or be chosen pseudorandomly (in which case the technique is sometimes known as <em>consistent hashing</em>).</p><ul><li>Consistent hashing uses randomly chosen partition boundaries to avoid the need for central control or distributed consensus. The word consistent here describes a particular approach to rebalancing.</li></ul></li><li><p>By using the hash of the key for partitioning, we get evenly distributed data. However, we lost the efficiency of range queries when partitioned by key.</p></li><li><p>Partition by key is useful for range scans, but partition by hash of key will distribute load more evenly.</p></li></ul></li><li><h3 id=skewed-workloads-and-relieving-hot-spots>Skewed Workloads and Relieving Hot Spots</h3></li><li><p>Hashing a key to determine its partition can help reduce hot spots. However, it can&rsquo;t avoid them entirely: in the extreme case where all reads and writes are for the same key, we still end up with all requests being route to the same partition.</p></li><li><p>Today, most data systems are not able to automatically compensate for such a highly skewed workload, so it&rsquo;s the responsibility of the application to reduce the skew.</p></li></ul></li><li><h3 id=partitioning-and-secondary-indexes>Partitioning and Secondary Indexes</h3><ul><li><p>Key-value data model is just a simple case, if the secondary indexes are involved, the situation will become more complicated. Secondary indexes are widely used in search engines such as Solr and Elasticsearch. The problem with secondary indexes is that they don&rsquo;t map neatly to partitions. There are two main approaches to partitioning a database with secondary indexes: document-based partitioning and term-based partitioning.</p></li><li><h3 id=partitioning-secondary-indexes-by-document>Partitioning Secondary Indexes by Document</h3><ul><li>In this indexing approach, each partition is completely separate: each partition maintains its own secondary indexes, covering only the documents in that partition.</li></ul></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/partitioning_secondary_indexes_by_document.png alt></p><pre><code>  + A document-partitioned index is also known as a _local index_ (as opposed to a _global index_), because it doesn't care what data is stored in other partitions.

  + If we want search by secondary indexes, we need to send the query to all partitions, and combine all the results we get back. This approach to query a partitioned database is sometimes known as _scatter/gather_, and it can make read queries on secondary quite expensive.

+ ### Partitioning Secondary Indexes by Term

  + Rather than each partition having it's own secondary index (a local index), we can construct a global index that covers data in all partitions.
</code></pre><p><img src=https://zhannicholas.github.io/assets/ddia/partitioning_secondary_indexes_by_term.png alt></p><pre><code>  + If we store all secondary index in one node, the node would likely become a bottleneck. Thus, a global index must also be partitioned, but it can be partitioned differently from the primary key index.

  + This kind of secondary index partition is called _term-partitioned_, because the term we're looking for determines the partition of the index. The name _term_ comes from full-text indexes (a particular kind of secondary index), where the terms are all the words that occur in a document.

  + The advantage of global (term-partitioned) index over a document-partitioned index is that it can make reads more efficient, but the disadvantage is that it makes writes slower and more complicated.
</code></pre><ul><li><h3 id=rebalancing-partitions>Rebalancing Partitions</h3><ul><li><p>The processing of moving load from one node in the cluster to another is called <em>rebalancing</em>.</p></li><li><p>No matter which partitioning scheme is used, rebalancing is usually expected to meet some minimum requirements:</p><ul><li><p>After rebalancing, the load should be shared fairly between the nodes in the cluster.</p></li><li><p>While rebalancing is happening, the database should continue accepting reads and writes.</p></li><li><p>No more data than necessary should be moved between nodes.</p></li></ul></li><li><p>Hash mod N is not very good for rebalancing. Because if the number of nodes N changes, most of the keys will need to be moved from one node to another.</p></li><li><p>A fairly simple solution to overcome the drawback of mod N approach is use fixed number of partitions: create many more partitions than there are nodes, and assign several partitions to a node. If a node is added to the cluster, the new node can steal a few partitions from every existing node until partitions are fairly distributed once again. If a node is removed from the cluster, the same happens in reverse.</p></li><li><p>Some key range-partitioned database can create partitions dynamically: when a partition grows to exceed a threshold, it is split into two smaller partitions. Conversely, if lots of data is deletes, a partition can be merged into an adjacent partition. In a nutshell, dynamic partitioning is about the number of partitions adapting to the total data volume.</p></li><li><p>With dynamic partitioning, the number of partitions is proportional to the size of the dataset. On the other hand, with a fixed number of partitions, the size of each partition is proportional to the size of the dataset.</p></li></ul></li><li><h3 id=request-routing>Request Routing</h3><ul><li><p>As partitions are rebalanced, the assignment of partitions to nodes changes. When a client wants to make a request, how does it know which node to connect to? This is an instance of a more general problem: <em>service discovery</em>.</p></li><li><p>On a high level, there are a few different approaches to this problem:</p><ul><li><p>Allow clients contact any node. If that node coincidentally owns the partition to which the request applies, handle the request directly; otherwise, forward the request to the appropriate node, receive the reply and pass the reply along to the client.</p></li><li><p>Sends all request from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly.</p><ul><li>Many distributed data system rely on a separate coordination service such as ZooKeeper to keep track of cluster metadata. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date.</li></ul></li><li><p>Require the clients be aware of the partitioning and the assignment of the partitions to nodes.</p></li></ul></li><li></li></ul></li></ul><h2 id=chapter-7-transactions>Chapter 7: Transactions</h2><ul><li><p>A transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (<em>commit</em>) or it fails (<em>abort</em>, <em>rollback</em>).</p></li><li><p>Transactions are not a law of nature; they were created with a purpose, namely to <em>simplify the programming model</em> for applications accessing a database.</p></li><li><h3 id=the-slippery-concept-of-a-transaction>The Slippery Concept of a Transaction</h3><ul><li><p>Like every other technical design choice, transactions have advantages and limitations.</p></li><li><h3 id=the-meaning-of-acid>The Meaning of ACID</h3><ul><li><p>The safety guarantees provides by transactions are often described by the well-known acronym ACID, which stands for Atomicity, Consistency, Isolation, and Durability.</p><ul><li><div class=warning>In practice, one database&rsquo;s implementation of ACID doesn&rsquo;t equal another&rsquo;s implementation.</div></li></ul></li><li><p>Systems that don&rsquo;t meet the ACID criteria are sometimes called BASE, which stands for Basically Available, Soft state, and Eventual consistency.</p></li><li><h4 id=atomicity>Atomicity</h4><ul><li><p>In general, atomic refers to something that cannot be broken down into smaller parts. ACID atomicity describes what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed. If the writes are grouped together into a atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted and the database must discard or undo any writes it has made so far in that transaction.</p></li><li><p>The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity. Perhaps <em>abortability</em> is a better term than <em>atomicity</em>.</p></li></ul></li><li><h4 id=consistency>Consistency</h4><ul><li><p>The idea of ACID consistency is that you have certain statements about your data (<em>invariants</em>) that must always be true. But the statements are depend on the application&rsquo;s notion of invariants, and it&rsquo;s the application&rsquo;s responsibility to define its transactions correctly so that they preserve consistency.</p></li><li><p>In general, the application defines what data is valid or invalid, the database only stores it.</p></li><li><p>Atomicity, isolation, and durability are properties of the database, whereas consistency (in the ACID sense) is a property of the application.</p></li></ul></li><li><h4 id=isolation>Isolation</h4><ul><li><p>Isolation in the sense of ACID means concurrently executing transactions are isolated from each other: they cannot step on each other&rsquo;s toes.</p></li><li><p>This is achieved by ensuring that any interim state changes made during one transaction are invisible to other transactions.</p></li></ul></li><li><h4 id=durability>Durability</h4><ul><li><p>The purpose of a database is to provide a safe place where data can be stored without fearing of losing it. Durability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.</p></li><li><p>In order to provide a durability guarantee, a database must wait until these writes or replications are complete before reporting a transaction as successfully committed.</p></li></ul></li></ul></li></ul></li><li><h3 id=weak-isolation-levels>Weak Isolation Levels</h3><ul><li><p>Databases try to hide concurrency issues from application developers by providing transaction isolation.</p></li><li><p>Serializable isolation means that the database guarantees that transactions have the same effect as if they ran serially. However, serializable isolation has a performance cost, and many databases don&rsquo;t want to pay that price. Instead, most database systems use weaker level of isolation, which protect against some concurrency issues, but not all.</p></li><li><h3 id=read-committed>Read Committed</h3><ul><li><p>The most basic level of transaction isolation is <em>read committed</em>. It makes two guarantees:</p><ul><li><ol><li>No dirty reads. When reading from the database, you only see data that has been committed.</li></ol></li></ul></li></ul></li></ul></li></ul><ol start=2><li><p>No dirty writes. When writing to the database, you will only overwrite data that has been committed.</p><ul><li><p>Most commonly, databases prevent dirty writes by using row-level locks: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object. It must be hold that lock until the transaction is committed or aborted.</p></li><li><p>Lock can also be used to prevent dirty reads, but its bad for performance. Thus, most database prevent dirty reads by a different approach: for every object that is written, then database remembers both the old committed value and the new value set by transaction that is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.</p></li><li><h3 id=snapshot-isolation-and-repeatable-read>Snapshot Isolation and Repeatable Read</h3><ul><li><p>The idea of <em>snapshot isolation</em> is that each transaction reads from a consistent snapshot of the database. In other words, the transaction sees all the data that was committed in the database at the start of the transaction. Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.</p></li><li><h4 id=implementing-snapshot-isolation>Implementing snapshot isolation</h4><ul><li><p>Implementations of snapshot isolation typically use write locks to prevent dirty writes, however, reads do not require any locks. From a performance view, a key principle of snapshot isolation is that <em>readers never block writers, and writers never block readers</em>.</p></li><li><p>To implement snapshot isolation, database must potentially keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it remains several versions of an object side by side, this technique is known as <em>multi-version concurrency control (MVCC)</em>.</p></li><li><p>When a transaction is started, it is given a unique, always-increasing transaction ID (txid). Whenever a transaction writes anything to the database, the data it writes is tagged with the transaction ID of the writer.</p></li></ul></li><li><h4 id=visibility-rules-for-observing-a-consistent-snapshot>Visibility rules for observing a consistent snapshot</h4><ul><li><p>When a transaction reads from the database, transaction IDs are used to decide which objects it can see and which are invisible. By carefully defining visibility rules, the database can present a consistent snapshot of the database to the application. This works as follows:</p><ul><li><ol><li>At the start of the transaction, the database makes a list of all the other transactions that are in progress (not yet committed or aborted) at that time. Any writes that those transactions have made are ignored, even if the transactions subsequently commit.</li></ol></li></ul></li></ul></li></ul></li></ul></li><li><p>Any writes made by aborted transactions are ignored.</p></li><li><p>Any writes made by transactions with a later transaction ID are ignored, regardless of whether those transactions have committed.</p></li><li><p>All other writes are visible to the application&rsquo;s queries.</p><pre><code> + Put another, way, an object is visible if both of the following conditions are true:

   + At the time when the reader's transaction started, the transaction that created the object had already committed.

   + The object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed at the time when the reader's transaction started.
</code></pre><ul><li><h4 id=repeatable-read-and-naming-confution>Repeatable read and naming confution</h4><ul><li>Snapshot isolation is a useful isolation level, especially for read-only transactions. However, many databases that implement it call it by different names. In Oracle, it is called <em>serializable</em>, and in PostgreSQL and MySQL, it is called <em>repeatable read</em>.</li></ul></li><li><h3 id=preventing-lost-updates>Preventing Lost Updates</h3><ul><li><p>The lost updates problem can occur if an application reads some value from the database, modifies it, and writes back the modified value (a <em>read-modify-write cycle</em>). If two transactions do this concurrently, one of the modifications will be lost, because the second write does not include the first modification.</p></li><li><p>A variety of solutions have been developed to deal with lost updates problem:</p><ul><li><p>Atomic write operations.</p></li><li><p>Explicit locking. (<code>SELECT ... FOR UPDATE</code>).</p></li><li><p>Automatically detecting lost updates and forcing the read-modify-write cycles to happen sequentially.</p></li><li><p>Compare-and-set.</p></li><li><p>Resolve conflicts when replications exist.</p></li></ul></li></ul></li><li><h3 id=write-skew-and-phantoms>Write Skew and phantoms</h3><ul><li><p>Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects).</p></li><li><p>Write skew is neither a dirty write or a lost update, because the two transactions are updating two different objects. But we can think of write skew as a generalization of the lost update problem (when different transactions update the same object).</p></li><li><p>Most of the writes that are skewed happen in a similar pattern:</p><ul><li><ol><li>A SELECT query checks whether some requirement is satisfied by searching for rows that match some search condition.</li></ol></li></ul></li></ul></li></ul></li><li><p>Depending on the result of the first query, the application code decides how to continue (perhaps to go ahead with the operation, or report an error to the user and abort).</p></li><li><p>If the application decides to go ahead, it makes a write (INSERT, UPDATE, DELETE) to the database and commits the transaction.</p><pre><code>   + The effect of this write changes the precondition of the decision of step 2. This effect, where a write in one transaction changes the result of a search query in another, is called a _phantom_.
</code></pre></li></ol><ul><li><h3 id=serializability>Serializability</h3><ul><li><p>Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, <em>serially</em>, without any concurrency.</p></li><li><p>Most databases that provide serializability today use one of three techniques:</p><ul><li><ol><li>Literally executing transaction in a serial order.</li></ol></li></ul></li></ul></li></ul><ol start=2><li><p>Two-phase locking, which for several decades was the only viable option.</p></li><li><p>Optimistic concurrency control techniques such as serializable snapshot isolation.</p><ul><li><h3 id=actual-serial-executation>Actual Serial Executation</h3><ul><li><p>The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute one transaction at time, in serial order, on a single thread.</p></li><li><p>A system designed for single-threaded execution can sometimes perform better than a system that supports concurrency, because it can avoid the coordination overhead or locking. However, its throughput is limited to that of a single CPU core.</p></li></ul></li><li><h3 id=two-phase-locking-2pl>Two-Phase Locking (2PL)</h3><ul><li><p>In two-phase locking, several transactions are allowed to concurrently read the same object as long as nobody is writing to it. But as soon as anyone wants to write an object, exclusive access is required:</p><ul><li><p>If transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts before it can continue.</p></li><li><p>If transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts before it can continue.</p></li></ul></li><li><p>In a nutshell, in 2PL, writers don&rsquo;t just block other writers, but also block readers and vice versa.</p></li><li><p>The blocking of readers and writers is implemented by having a lock on each object in the database. The lock can either be in shared mode or in exclusive mode. The lock is used as follows:</p><ul><li><p>If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, these transactions must wait.</p></li><li><p>If a transaction wants to write an object, it must first acquire the lock in exclusive mode. No other transaction may hold the lock at the same time, so if there is any existing lock on the object, the transaction must wait.</p></li><li><p>If a transaction first reads the then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.</p></li><li><p>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name &ldquo;two-phase&rdquo; comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.</p></li></ul></li><li><p>The big downside of two-phase locking is performance: transaction throughput and response time of queries are significantly worse under two-phase locking than under weak isolation. This is partly due to the overhead of acquiring and releasing all those locks, but more importantly due to reduced concurrency.</p></li></ul></li><li><h3 id=serializable-snapshot-isolation-ssi>Serializable Snapshot Isolation (SSI)</h3><ul><li><p>Serializable snapshot isolation (SSI) provides full serializability, but has only a small performance penalty compared to snapshot isolation.</p></li><li><p>As the name suggests, SSI is based on snapshot isolation&mdash;that is, all reads within a transaction are made from a consistent snapshot of the database.</p></li><li><h4 id=decisions-based-on-an-outdated-premise>Decisions based on an outdated premise</h4><ul><li><p>Under snapshot isolation, the transaction is taking an action based on a <em>premise</em> (a fact was true at the beginning of the transaction). Later, when the transaction wants to commit, the original data may have changed&mdash;the premise may no longer be true.</p></li><li><p>In order to provide serializable isolation, the database must detect situations in which a transaction may have acted on an outdated premise and abort the transaction in that case. To know if a query result might have changed, the database needs to consider two cases:</p><ul><li><p>Detecting reads of a stale MVCC object version (uncommitted write occurred before the read).</p><ul><li>The database needs to track when a transaction ignores another transaction&rsquo;s writes due to MVCC visibility rules. When the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so the transaction must be aborted.</li></ul></li><li><p>Detecting writes that affect prior reads (the write occurs after the read).</p><ul><li>When a transaction writes to the database it must look in the indexes for any other transactions that have recently read the affected data.</li></ul></li></ul></li></ul></li><li><p>Compare to 2PL, the big advantage of serializable snapshot isolation is that one transaction doesn&rsquo;t need to block waiting for locks held by another transaction. This makes query latency much more predictable and less variable.</p></li><li><p>Compare to serial execution, SSI is not limited to the throughput of a single CPU core.</p></li></ul></li></ul></li></ol><ul><li></li></ul><h2 id=chapter-8-the-trouble-with-distributed-systems>Chapter 8: The Trouble with Distributed Systems</h2><ul><li><p>An assumption: anything that can go wrong will go wrong.</p></li><li><h3 id=faults-and-partial-failures>Faults and Partial Failures</h3><ul><li><p>In a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even though other parts of the system are working fine. This is know as <em><strong>partial failure</strong></em>.</p></li><li><p>Partial failures are underterministic.</p></li><li><p>Nondeterminism and possibility of partial failure is what makes distributed systems hard to work with.</p></li><li><p>If we want to make distributed systems work, we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software. In other words, we need to build a reliable system from unreliable components,</p></li></ul></li><li><h3 id=unreliable-networks>Unreliable Networks</h3></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/unreliable_networks.png alt></p><pre><code>+ If you send a request and don't get a response, it's not possible to distinguish whether the request was lost, the remote node is down, or the response was lost. The usual way of handling this issue is a **_timeout_**: after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don't know whether the remote node got your request or not. There's no &quot;correct&quot; value for timeouts---they need to be determined experimentally.

+ When one part of the network is cut off from the rest due to a network fault, that is sometimes called a _network partition_ or _netsplit_.

+ Rapid feedback about a remote node being down is useful, but you cannot count on it. Even if TCP acknowledges that a packet was delivered, the application may have crashed before handling it.

+ If a timeout is the only sure way of detecting a fault, then how long should the timeout be? There is unfortunately no simple answer. A long timeout means a long wait until a node is declared dead. A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead when in fact it has only suffered a temporary slowdown.

+ Prematurely declaring a node dead is problematic: if the node is actually alive and in the middle of performing some action, and another node takes over, the action may end up being performed twice.

+ When a node is declared dead, its responsibilities need to be transferred to other nodes, which places additional load on other nodes and network. If the system is already struggling with high load, declaring nodes dead prematurely can make the problem worse. Transferring a node's load to other overloaded nodes can cause a cascading failure.

+ Why do datacenter networks and the internet use packet switching? The answer is that they are optimized for _bursty traffic_. TCP dynamically adapts the rate of data transfer to the available network capacity.

+ More generally, you can think of variable delays as a consequence of dynamic resource partitioning.
</code></pre><ul><li><h3 id=unreliable-clocks>Unreliable Clocks</h3><ul><li><p>In a distributed system, time is a tricky business, because communication is not instantaneous.</p></li><li><p>Modern computers have at least two different kinds of clocks: a <em>time-of-day clock</em> and a <em>monotonic clock</em>.</p><ul><li><p>A <em>time-of-day clock</em> returns the current date and time according to some calendar (also known as wall-clock time), they are usually synchronized with NTP (Network Time Protocol), which means that at timestamp from one machine (ideally) means the same as a timestamp on another machine.</p></li><li><p>A <em>monotonic clock</em> is suitable for measuring a duration (time interval), such as a timeout or a service&rsquo;s response time. The name comes from the fact that they are guaranteed to always move forward (whereas a time-of-day clock may jump back in time). In particular, it makes no sense to compare monotonic clock values from two different computers, because they don&rsquo;t mean the same thing.</p></li></ul></li><li><p>Logical clocks and physical clocks</p><ul><li><p><em>logical clocks</em> are based on incrementing counters rather than an oscillating quartz crystal, which is a safer alternative for ordering events than timestamp.</p></li><li><p><em>physical clocks</em> are those who measure actual elapsed time, such as time-of-day and monotonic clocks.</p></li></ul></li><li><h3 id=process-pauses>Process Pauses</h3><ul><li><p>A process (or thread) might be paused for a long time.</p><ul><li><p>JVM has a garbage collector that occasionally needs to stop all running threads.</p></li><li><p>In virtualized environments, a VM can be suspend and resumed.</p></li><li><p>&mldr;</p></li></ul></li><li><p>There are so many situations in which a running thread is preempted at any point and resumed at some later time, without the tread even noticing.</p></li><li><p>The negative effects of process pauses can be mitigated without resorting to expensive real-time scheduling guarantees.</p><ul><li><p>An emerging idea is to treat GC pauses like brief planned outage of a node, an to let other nodes handle requests from client while one node is collecting its garbage.</p></li><li><p>A variant of this idea is to use the garbage collector only if short-lived objects (which are fast to collect) and to restart processes periodically, before they accumulate enough long-lived objects to require a full GC of long-lived objects.</p></li></ul></li></ul></li></ul></li><li><h3 id=knowledge-truth-and-lies>Knowledge, Truth, and Lies</h3><ul><li><p>A node in the network cannot know anything for sure&mdash;it can only make guesses based on the messages it receives (or doesn&rsquo;t receive) via the network. A node can only find out what state another node is in by exchanging messages with it.</p></li><li><p>If a node continues acting as the chosen one, even though the majority of nodes have declared it dead, it could cause problems in a system that is not carefully designed. 幂等</p></li><li><p>Proving an algorithm correct does not mean its implementation on a real system will necessarily always behave correctly.</p></li></ul></li></ul><h2 id=chapter-9-consistency-and-consensus>Chapter 9: Consistency and Consensus</h2><ul><li><p>The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees.</p></li><li><p>One of the most important abstractions for distributed system is <strong><em>consensus</em></strong>: that is, getting all of the nodes to agree on something.</p></li><li><h3 id=consistency-guarantees>Consistency Guarantees</h3><ul><li>Eventual consistency: if you stop writing to the database and wait for some unspecified length of time, then eventually all read requests will return the same value. In other words, the inconsistency is temporary, and eventually resolves itself. A better name for eventual consistency may be <em>convergence</em>, as we expect all replicas to eventually converge to the same value.</li></ul></li><li><h3 id=linearizability>Linearizability</h3><ul><li><p>The basic idea of linearizability (also known as atomic consistency, strong consistency, immediate consistency or external consistency) is to make the system appear as if there were only one copy of the data, and all operations on it are atomic.</p></li><li><p>In a linearizable system, the value read is the most recent, update-to-date value, and doesn&rsquo;t come from a stale cache or replica. In other words, linearizablity is a <em>recency guarantee</em>.</p></li></ul></li><li><h3 id=ordering-guarantees>Ordering Guarantees</h3><ul><li><p>Ordering helps preserve <strong><em>causality</em></strong>. Causality imposes an ordering on events: cause comes before effect. If a system obeys the ordering imposed by causality, we say that it is <em>causally consistent</em>.</p></li><li><p>Although causality is an important theoretical concept, actually keeping track of all causal dependencies can become impracticable. There is a better way: we can use <em>sequence numbers</em> or <em>timestamps</em> to order events.</p></li><li><p><strong><em>Lamport timestamp</em></strong> provices a total ordering consistent with causality. In Lamport timestamp, each node has a unique identifier, and each node keeps a counter of the number of operations it has processed. The Lamport timestamp is then simply a pair of <em>(counter, node ID)</em>. Two node may sometimes have the same counter value, but by including the node ID in the timestamp, each timestamp is made unique.</p><ul><li>The key idea about Lamport timestamps, which makes them consistent with causality, is the following: every node and every client keeps track of the <em>maximum</em> counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.</li></ul></li><li><p>A big problem about total ordering is that the total order of operations only emerges after you have collected all of the operations.</p></li><li><p>Total order broadcast is asynchronous: messages are guaranteed to be delivered reliably in a fixed order, but there is no guarantee about when a message will be delivered. Total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes.</p></li></ul></li><li><h3 id=distributed-transactions-and-consensus>Distributed Transactions and Consensus</h3><ul><li><p>Consensus is one of the most important and fundamental problems in distributed computing. Its goal is to get several nodes to agree on something.</p></li><li><p>In a distributed system, we must assume that node may crash, so reliable consensus is impossible.</p></li><li><h3 id=atomic-commit-and-two-phase-commit-2pc>Atomic Commit and Two-Phase Commit (2PC)</h3><ul><li>Two phase commit is an algorithm for achieving atomic transaction commit across multiple nodes&mdash;i.e., to unsure that either all nodes commit or all nodes abort.</li></ul></li></ul></li></ul><p><img src=https://zhannicholas.github.io/assets/ddia/successful_two_phase_commit.png alt></p><p><img src=./assets/ddia/coordinator_crashes_in_two_phase_commit.png alt></p><pre><code>+ ### Fault-Tolerant Consensus

  + The consensus problem is normally formalized as follows: one or more nodes may propose values, and the consensus algorithm decides on one of those values. In this formalism, a consensus algorithm must satisfy the following properties:

    + Uniform agreement: no two nodes decide differently.

    + Integrity: no node decides twice.

    + Validity: if a node decides value v, then v was proposed by some node.

    + Termination: every node that does not crash eventually decides some value.

+ ### Membership and Coordination Services

  + Projects like ZooKeeper and etcd are often described as &quot;distributed key-value stores&quot; or &quot;coordination and configuration services&quot;. ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory. That small amount of data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm.

  + 
</code></pre><h2 id=chapter-10-batch-processing>Chapter 10: Batch Processing</h2><ul><li><h3 id=system-of-record-and-derived-data>System of Record and Derived Data</h3><ul><li><p>On a high level, systems that store and process data can be grouped into two broad categories:</p><ul><li><p><strong>Systems of record</strong>. A system of record, also known as <em>source of truth</em>, holds the authoritative version of your data. Each fact is represented exactly once (the representation is typically <em>normalized</em>).</p></li><li><p><strong>Derived data systems</strong>. Data in a derived system is the result of taking some existing data from another system and transforming or processing it in some way. If you lose derived data, you can recreate it from the original source. Technically speaking, derived data is <em>redundant</em>, in the sense that it duplicates existing information. However, it is often essential for getting good performance on read queries. Derived data is commonly <em>denormalized</em></p></li></ul></li></ul></li><li><p>A batch processing system takes a large amount of input data, run a job to process it, and produces some output data. Jobs often take a while (from a few minutes to several days), so there normally isn&rsquo;t a user waiting for the job to finish. The primary performance measure of a batch job is usually <em>throughput</em>.</p></li><li><p>In batch processing world, the inputs and outputs of a job are files (perhaps on a distributed filesystem). When the input is a file (a sequence of bytes), the first processing step is usually to parse it into a sequence of records.</p></li><li><p>A nice property of the batch processing systems is that they provide a strong reliability guarantee: failed tasks are automatically retried, and partial output from failed tasks are automatically discarded.</p></li></ul><h2 id=chapter-11-stream-processing>Chapter 11: Stream Processing</h2><ul><li><p>In batch processing, input is bounded. However, in stream processing, the input is unbounded&mdash;that is, the inputs of our job are never-ending streams of data. In this case, a job is never complete, because at any time there may still be more work coming in.</p></li><li><p>In general, a &ldquo;stream&rdquo; refers to data that is incrementally made available over time.</p></li><li><p>In batch processing, a file is written once and then potentially read by multiple jobs. Analogously, in streaming terminology, an event is generated once by a producer (also known as a publisher or sender), and then potentially processed by multiple consumer (subscribers or recipients). In a filesystem (used in batch processing), a filename identifies a set of related records; in a streaming system, related events are usually grouped together into a topic or stream.</p></li><li><h3 id=transmitting-event-streams>Transmitting Event Streams</h3><ul><li><h3 id=message-systems>Message Systems</h3><ul><li><p>A common approach for notifying consumers about new event is to use a messaging system: a producer sends a message containing the event, which is then pushed to consumers.</p></li><li><p>Within publish/subscribe model, different systems take a wide range of approaches, and there is no one right answer for all purposes. To differentiate the systems, it is particularly helpful to ask the following two questions:</p><ul><li><p>What happens if the producers send messages faster than the consumers can process them?</p><ul><li><p>Broadly speaking, there are three options:</p><ul><li><ol><li>drop messages</li></ol></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><ol start=2><li><p>buffer messages in a queue</p></li><li><p>apply backpressure (also known as flow control; i.e., blocking the producer from sending more messages)</p><pre><code> + What happens if nodes crash or temporarily go offline---are any message lost?

   + Whether message lost is acceptable depends very much on the application
</code></pre><ul><li><h4 id=direct-messaging-from-producers-to-consumers>Direct messaging from producers to consumers</h4><ul><li><p>A number of messaging systems use direct network communication between producers and consumers without going via intermediary nodes, such as UDP multicast, brokerless messaging libraries, webhooks, etc.</p></li><li><p>Direct messaging systems generally require the application code to be aware of the possibility of message loss. If a consumer is offline, it may miss messages that were sent while it is unreachable.</p></li></ul></li><li><h4 id=message-brokers>Message brokers</h4><ul><li><p>A widely used alternative of direct messaging systems is to send messages via a <em>message broker</em> (also known as a <em>message queue</em>), which is essentially a kind of database that is optimized for handling message streams.</p></li><li><p>By centralizing the data in the broker, these systems can more easily tolerate clients that come and go, and the question of durability is moved to the broker instead.</p></li></ul></li><li><h4 id=multiple-consuemers>Multiple Consuemers</h4><ul><li><p>When multiple consumers read messages in the same topic, two main patterns if messaging are used:</p><ul><li><p><strong>Load balancing</strong>: each message is delivered to one of the consumers, so the consumers can share the work of processing the messages in the topic.</p></li><li><p><strong>Fan-out</strong>: each message is delivered to all of the consumers.</p></li></ul></li></ul></li><li><h4 id=acknowledgements-and-redelivery>Acknowledgements and redelivery</h4><ul><li><p>Consumers can crash at any time. In order to ensure that the message is not lost, message brokers use acknowledgements: a client must explicitly tell the broker when it has finished processing a message so that the broker can remove it from the queue.</p></li><li><p>If the broker didn&rsquo;t receive an acknowledgement, it assumes that the message was not processed, and therefore it delivers the message again to another consumer.</p><ul><li>It could happen that the massage actually was fully processed, but the acknowledgement was lost in the network. Handling this case requires an atomic commit protocol.</li></ul></li><li><p>Message redelivery may have a bad effect on the ordering of messages. Message reordering is not a problem if messages are completely independent (each consumer has their own queue) of each other, but it can be important if there are <em>causal dependencies</em> between messages.</p></li></ul></li><li><h3 id=partitioned-logs>Partitioned Logs</h3><ul><li><p>Even message brokers that durably write messages to disk quickly delete them again after they have been delivered to consumers. However, in database and filesystems, everything that is written to a database or file is normally expected to be permanently recorded.</p></li><li><p>The idea behind <em>log-based message brokers</em> is combining the durable storage approach of database with low-latency notification facilities of messaging.</p></li><li><p>In order to scale to higher throughput than a single disk can offer, the log can be partitioned. A topic can then be defined as a group of partitions that all carry messages of the same type. Within each partition, the broker assigns a monotonically increasing sequence number, or offset, to every message. Such a sequence number makes sense because a partition is read-only, so the messages within a partition are totally ordered. There is no guarantee across different partitions.</p></li></ul></li></ul></li></ol><p><img src=https://zhannicholas.github.io/assets/ddia/producers_send_messages_by_appending_them_to_a_topic_partition_file_and_consumers_read_these_files_sequentially.png alt></p><pre><code>  + Since partitioned logs typically preserve message ordering only within a single partition, all messages that need to be ordered consistently need to be routed to the same partition.

  + Consuming a partition sequentially makes it easy to tell which messages have been processed: all messages with an offset less than a consumer's current offset have already been processed, and all messages with a greater offset have not yet been seen. Thus, the broker does not need to track acknowledgements for every single message---it only needs to periodically record the consumer offsets.

  + If a consumer node fails, another node in the consumer group is assigned the failed consumer's partitions, and it starts consuming messages at the last recorded offset.

  + If you only ever append to the log, you will eventually run out of disk space. To reclaim disk space, the log is actually divided into segments, and from time to time old segments are deleted or moved to archive storage.
</code></pre><ul><li><h3 id=databases-and-streams>Databases and Streams</h3><ul><li><h3 id=change-data-capture>Change Data Capture</h3><ul><li><p><em>Change data capture (CDC)</em> is the process of observing all data changes written to a database and extracting them in a form in which they can be replicated to other systems.</p></li><li><p>CDC is a mechanism for ensuring that all changes made to the system of record are also reflected in the derived data systems so that the derived systems have a accurate copy of the data.</p></li><li><p>Essentially, change data capture makes one database the leader (to one from which the changes are captured), and turns the others into followers.</p></li></ul></li><li><h3 id=event-sourcing>Event Sourcing</h3><ul><li>Event sourcing involves storing all changes to the application state as a log of change events.</li></ul></li><li></li></ul></li></ul><pre><code>+ 
</code></pre></div><hr><aside><h4>No linked references</h4></aside><br><aside class=related></aside></article></div><div x-cloak x-data="{ atTop: false }"><button id=scrollToTop name="scroll to top button" aria-label="scroll to top button" class="fixed z-50 w-10 h-10 font-bold text-center text-white transition-all duration-1000 ease-in-out transform bg-violet-600 rounded-full cursor-pointer animate-bounce bottom-24 right-8 focus:outline-none" :class="{ 'opacity-0 translate-y-24': !atTop, 'opacity-1 translate-y-2' :  atTop}" @click="window.scrollTo({ top: 0, behavior: 'smooth'})" @scroll.window="atTop = (window.pageYOffset > 200)"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 11l3-3m0 0 3 3m-3-3v8m0-13a9 9 0 110 18 9 9 0 010-18z"/></svg></button></div><script src=https://zhannicholas.github.io/js/alpine.js defer></script>
<script src=https://zhannicholas.github.io/js/darkmode.js defer></script><footer class=bg-gray-900><div class="max-w-md px-4 py-12 mx-auto overflow-hidden sm:max-w-3xl sm:px-6 lg:max-w-7xl lg:px-8"><nav class="flex flex-wrap justify-center -mx-5 -my-2" aria-label=Footer><div class="px-5 py-2"><a href=https://zhannicholas.github.io/about class="text-base text-gray-400 hover:text-gray-300">About</a></div><div class="px-5 py-2"><a href=https://zhannicholas.github.io/posts class="text-base text-gray-400 hover:text-gray-300">Posts</a></div><div class="px-5 py-2"><a href=https://zhannicholas.github.io/notes class="text-base text-gray-400 hover:text-gray-300">Notes</a></div><div class="px-5 py-2"><a href=https://zhannicholas.github.io/library class="text-base text-gray-400 hover:text-gray-300">Library</a></div><div class="px-5 py-2"><a href=https://zhannicholas.github.io/contact/ class="text-base text-gray-400 hover:text-gray-300">Contact</a></div></nav><div class="flex justify-center mt-2 space-x-6"><a href=https://github.com/zhannicholas class="text-gray-400 hover:text-gray-300"><span class=sr-only>GitHub</span><svg class="w-6 h-6" fill="currentcolor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483.0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951.0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65.0.0.84-.27 2.75 1.026A9.564 9.564.0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688.0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855.0 1.338-.012 2.419-.012 2.747.0.268.18.58.688.482A10.019 10.019.0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"/></svg></a>
<a href=mailto:zhan_nicholas@outlook.com class="text-gray-400 hover:text-gray-300"><span class=sr-only>Email</span><svg class="w-6 h-6" fill="currentcolor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M21 7.38246601V5H3V7.38199365l9.0000224 4.50046115L21 7.38246601zm0 2.23606798-9.0000224 4.50001121L3 9.61810635V19H21V9.61853399zM3 3H21c1.1045695.0 2 .8954305 2 2V19c0 1.1045695-.8954305 2-2 2H3c-1.1045695.0-2-.8954305-2-2V5c0-1.1045695.8954305-2 2-2z" clip-rule="evenodd"/></svg></a>
<a href=https://t.me/zhannicholas class="text-gray-400 hover:text-gray-300"><span class=sr-only>Telegram</span><svg class="w-6 h-6" fill="currentcolor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.78754 14.0196C5.83131 14.0344 5.87549 14.0448 5.91963 14.0512 5.96777 14.1644 6.02996 14.3107 6.10252 14.4818c.17707.4176.41566.9825.66194 1.5717C7.2667 17.2552 7.77332 18.4939 7.88521 18.8485 8.02372 19.2868 8.17013 19.5848 8.32996 19.7883 8.4126 19.8935 8.50819 19.9853 8.62003 20.0549 8.67633 20.0899 8.7358 20.1186 8.79788 20.14 8.80062 20.141 8.80335 20.1419 8.80608 20.1428 9.1261 20.2636 9.41786 20.2133 9.60053 20.1518 9.69827 20.1188 9.77735 20.0791 9.8334 20.0469 9.86198 20.0304 9.88612 20.0151 9.90538 20.0021L9.90992 19.9991l2.82618-1.7625 3.2646 2.5028C16.0488 20.7763 16.1014 20.8073 16.157 20.8316 16.5492 21.0027 16.929 21.0624 17.2862 21.0136 17.6429 20.9649 17.926 20.8151 18.1368 20.6464 18.3432 20.4813 18.4832 20.2963 18.5703 20.1589 18.6148 20.0887 18.6482 20.0266 18.6718 19.9791 18.6836 19.9552 18.6931 19.9346 18.7005 19.9181L18.7099 19.8963 18.7135 19.8877 18.715 19.8841 18.7156 19.8824 18.7163 19.8808C18.7334 19.8379 18.7466 19.7935 18.7556 19.7482L21.7358 4.72274C21.7453 4.67469 21.7501 4.62581 21.7501 4.57682 21.7501 4.13681 21.5843 3.71841 21.1945 3.46452 20.8613 3.24752 20.4901 3.23818 20.2556 3.25598 20.0025 3.27519 19.7688 3.33766 19.612 3.38757 19.5304 3.41355 19.4619 3.43861 19.4126 3.45773 19.3878 3.46734 19.3675 3.47559 19.3523 3.48188L19.341 3.48666 2.62725 10.0432 2.62509 10.044C2.61444 10.0479 2.60076 10.053 2.58451 10.0593 2.55215 10.0719 2.50878 10.0896 2.45813 10.1126 2.35935 10.1574 2.22077 10.2273 2.07856 10.3247c-.22719.1556-.74968.5817-.6617 1.285.07019.5611.45457.9057.68876 1.0714C2.23421 12.7721 2.35638 12.8371 2.44535 12.8795 2.48662 12.8991 2.57232 12.9339 2.6095 12.9491L2.61889 12.9529l3.16865 1.0667zM19.9259 4.86786 19.9236 4.86888C19.9152 4.8725 19.9069 4.87596 19.8984 4.87928L3.1644 11.4438C3.15566 11.4472 3.14686 11.4505 3.138 11.4536L3.12869 11.4571C3.11798 11.4613 3.09996 11.4686 3.07734 11.4788 3.06451 11.4846 3.05112 11.491 3.03747 11.4978 3.05622 11.5084 3.07417 11.5175 3.09012 11.5251 3.10543 11.5324 3.11711 11.5374 3.1235 11.54l3.14263 1.058C6.32365 12.6174 6.37727 12.643 6.42649 12.674L16.8033 6.59948 16.813 6.59374C16.8205 6.58927 16.8305 6.58353 16.8424 6.5768 16.866 6.56345 16.8984 6.54568 16.937 6.52603 17.009 6.48938 17.1243 6.43497 17.2541 6.39485 17.3444 6.36692 17.6109 6.28823 17.899 6.38064 18.0768 6.43767 18.2609 6.56028 18.3807 6.76798 18.4401 6.87117 18.4718 6.97483 18.4872 7.06972 18.528 7.2192 18.5215 7.36681 18.4896 7.49424 18.4208 7.76875 18.228 7.98287 18.0525 8.14665 17.9021 8.28706 15.9567 10.1629 14.0376 12.0147 13.0805 12.9381 12.1333 13.8525 11.4252 14.5359l-.465.449 5.8719 4.5018C16.9668 19.5349 17.0464 19.5325 17.0832 19.5274 17.1271 19.5214 17.163 19.5045 17.1997 19.4752 17.2407 19.4424 17.2766 19.398 17.3034 19.3557L17.3045 19.354 20.195 4.78102C20.1521 4.79133 20.1087 4.80361 20.0669 4.81691 20.0196 4.83198 19.9805 4.84634 19.9547 4.85637 19.9418 4.86134 19.9326 4.86511 19.9276 4.86719L19.9259 4.86786zM11.4646 17.2618 10.2931 16.3636 10.0093 18.1693 11.4646 17.2618zM9.21846 14.5814l1.16494-1.1247c.7081-.6835 1.6555-1.5979 2.6127-2.5215l.9725-.9382-6.52007 3.8168L7.48351 13.8963c.1777.4191.41736.9864.664940000000001 1.5788.185129999999999.4429.37872.9093.55504 1.3411l.28304-1.8004C9.01381 14.8422 9.09861 14.692 9.21846 14.5814z" clip-rule="evenodd"/></svg></a></div><p class="mt-2 text-base text-center text-gray-400">2018-2023 &copy; Nicholas Zhan. Licensed under <a rel=license href=http://creativecommons.org/licenses/by-nc/4.0/>CC BY-NC 4.0</a>.</p></div></footer></body></html>